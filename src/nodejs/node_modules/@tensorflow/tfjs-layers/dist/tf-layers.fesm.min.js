/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
import{backend as t,util as e,serialization as n,tidy as s,sqrt as i,sum as r,mul as a,clipByValue as o,div as l,add as u,relu as h,slice as c,slice4d as p,slice3d as d,slice2d as f,slice1d as g,tensor1d as m,cast as y,gather as b,reshape as w,tile as k,randomNormal as v,elu as S,abs as x,dropout as N,concat as I,fused as A,transpose as C,concat4d as z,concat3d as T,concat2d as $,concat1d as E,zeros as F,ones as D,scalar as L,randomUniform as _,truncatedNormal as R,eye as M,linalg as O,variable as B,dispose as W,nextFrame as P,keep as U,log as j,sub as V,mean as K,neg as q,log1p as G,exp as H,softmax as J,fill as Z,maximum as Y,floor as X,oneHot as Q,max as tt,softplus as et,onesLike as nt,greater as st,equal as it,argMax as rt,logicalAnd as at,where as ot,squeeze as lt,train as ut,memory as ht,clone as ct,Tensor as pt,Optimizer as dt,io as ft,selu as gt,minimum as mt,sigmoid as yt,tanh as bt,logSoftmax as wt,leakyRelu as kt,prelu as vt,conv2dTranspose as St,conv3dTranspose as xt,image as Nt,conv1d as It,conv3d as At,separableConv2d as Ct,depthwiseConv2d as zt,expandDims as Tt,reverse as $t,unstack as Et,stack as Ft,split as Dt,conv2d as Lt,any as _t,notEqual as Rt,zerosLike as Mt,all as Ot,matMul as Bt,greaterEqual as Wt,moments as Pt,batchNorm2d as Ut,batchNorm3d as jt,batchNorm4d as Vt,pad as Kt,maxPool as qt,avgPool as Gt,maxPool3d as Ht,avgPool3d as Jt}from"@tensorflow/tfjs-core";function Zt(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function Yt(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function Xt(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||ie(t)&&!n)for(let s=0;s<t.length;++s)Xt(t[s],e,n);else e.push(t);return e}function Qt(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function te(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function ee(t){return t%1==0}function ne(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function se(t,e){const n=e.length;return Yt((t=null==t?e.map(((t,e)=>e)):[].concat(t)).every((t=>t>=-n&&t<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${t}`)),Yt(t.every((t=>ee(t))),(()=>`All values in axis param must be integers but got axis ${t}`)),t.map((t=>t<0?n+t:t))}function ie(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function re(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error(`Unknown dtype ${t}`)}function ae(t){return"string"==typeof t||t instanceof String}function oe(t){return Array.isArray(t)?oe(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":ae(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function le(t){return!!(t&&t.constructor&&t.call&&t.apply)}function ue(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function he(t,e,n,s=!1){const i=new Array;if(1===e.length){const r=e[0]*(s?2:1);for(let e=0;e<r;e++)i[e]=n[t+e]}else{const r=e[0],a=e.slice(1),o=a.reduce(((t,e)=>t*e))*(s?2:1);for(let e=0;e<r;e++)i[e]=he(t+e*o,a,n,s)}return i}function ce(t,e,n=!1){if(0===t.length)return e[0];const s=t.reduce(((t,e)=>t*e))*(n?2:1);if(0===s)return[];if(s!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}${n?" for a complex tensor":""}.`);return he(0,t,e,n)}function pe(t,e){const n=de(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function de(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error(`Unknown data type ${e}`)}function fe(t){return t&&t.then&&"function"==typeof t.then}class ge{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=me,this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(fe(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=this.getQueryParams(this.global.location.search);if("tfjsflags"in t){t.tfjsflags.split(",").forEach((t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)}))}}}function me(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("=")))),e}function ye(){return we}let be,we=null;function ke(){if(null==be){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}be=t}return be}function ve(t,e){const n=function(){const t=ke();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const Se=ve("kernelRegistry",(()=>new Map)),xe=ve("gradRegistry",(()=>new Map));function Ne(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return Se.get(n)}function Ie(t){return xe.get(t)}function Ae(t){const e=Se.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function Ce(t){const{kernelName:e}=t;xe.has(e)&&ye().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),xe.set(e,t)}function ze(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=Xt(t)),ye().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error(`Unknown data type ${e}`)}function Te(){return ye().platform.now()}function $e(t,e="utf-8"){return e=e||"utf-8",ye().platform.decode(t,e)}class Ee{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new De)}profileKernel(t,e,n){let s;const i=()=>{s=n()};let r;const a=Te();if(this.backendTimer.timerAvailable())r=this.backendTimer.time(i);else{i();for(const t of s)t.dataSync();r=Promise.resolve({kernelMs:Te()-a})}if(ye().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then((e=>{Fe(e,n.dtype,t)}))}return{kernelName:t,outputs:s,inputs:e,timeMs:r.then((t=>t.kernelMs)),extraInfo:r.then((t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():""))}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach((t=>{Promise.all([t.data(),s,r]).then((n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])}))}))}}function Fe(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class De{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?ne(`${s}ms`,9):s.error,o=ne(t,25),l=e.rank,u=e.size,h=ne(e.shape.toString(),14);let c="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;c+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${h}\t%c${u}\t%c${c}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function Le(t,e,n,s){const i=ue(e),r=function(t,e,n,s){const i=Qt(e),r=s[s.length-1],a=new Array(r).fill(0),o=e.length,l="complex64"===n?Oe(t):t;if(o>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)a[t]=Math.max(a[t],_e(l[e+t],0,n).length)}return a}(t,e,n,i),a=e.length,o=Me(t,e,n,i,r),l=["Tensor"];return s&&(l.push(`  dtype: ${n}`),l.push(`  rank: ${a}`),l.push(`  shape: [${e}]`),l.push("  values:")),l.push(o.map((t=>"    "+t)).join("\n")),l.join("\n")}function _e(t,e,n){let s;return s=Array.isArray(t)?`${parseFloat(t[0].toFixed(7))} + ${parseFloat(t[1].toFixed(7))}j`:ae(t)?`'${t}'`:"bool"===n?Re(t):parseFloat(t.toFixed(7)).toString(),ne(s,e)}function Re(t){return 0===t?"false":"true"}function Me(t,e,n,s,i,r=!0){const a="complex64"===n?2:1,o=e[0],l=e.length;if(0===l){if("complex64"===n){return[_e(Oe(t)[0],0,n)]}return"bool"===n?[Re(t[0])]:[t[0].toString()]}if(1===l){if(o>20){const e=3*a;let s=Array.from(t.slice(0,e)),r=Array.from(t.slice((o-3)*a,o*a));return"complex64"===n&&(s=Oe(s),r=Oe(r)),["["+s.map(((t,e)=>_e(t,i[e],n))).join(", ")+", ..., "+r.map(((t,e)=>_e(t,i[o-3+e],n))).join(", ")+"]"]}return["["+("complex64"===n?Oe(t):Array.from(t)).map(((t,e)=>_e(t,i[e],n))).join(", ")+"]"]}const u=e.slice(1),h=s.slice(1),c=s[0]*a,p=[];if(o>20){for(let e=0;e<3;e++){const s=e*c,r=s+c;p.push(...Me(t.slice(s,r),u,n,h,i,!1))}p.push("...");for(let e=o-3;e<o;e++){const s=e*c,r=s+c;p.push(...Me(t.slice(s,r),u,n,h,i,e===o-1))}}else for(let e=0;e<o;e++){const s=e*c,r=s+c;p.push(...Me(t.slice(s,r),u,n,h,i,e===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let t=1;t<p.length-1;t++)p[t]=" "+p[t]+d;let f=",\n";for(let t=2;t<l;t++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(r?"":f),p}function Oe(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let Be=null;class We{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=Qt(t),this.strides=ue(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return null.buffer(this.shape,this.dtype,t)}bufferSync(){return null.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return ce(this.shape,t,"complex64"===this.dtype)}arraySync(){return ce(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const t=Be().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map((t=>$e(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=Be().readSync(this.dataId);if("string"===this.dtype)try{return t.map((t=>$e(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await Be().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(Be().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return null.print(this,t)}clone(){return this.throwIfDisposed(),null.clone(this)}toString(t=!1){return Le(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),null.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),Be().makeVariable(this,t,e,n)}}Object.defineProperty(We,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),ve("Tensor",(()=>We));class Pe extends We{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!te(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);Be().disposeTensor(this),this.dataId=t.dataId,Be().incRef(this,null)}dispose(){Be().disposeVariable(this),this.isDisposedInternal=!0}}var Ue,je,Ve,Ke,qe;Object.defineProperty(Pe,Symbol.hasInstance,{value:t=>t instanceof We&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(Ue||(Ue={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(je||(je={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(Ve||(Ve={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(Ke||(Ke={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(qe||(qe={}));const Ge={float32:Ke,int32:je,bool:Ve,complex64:qe};function He(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Ge[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function Je(t){const e=[];return Ze(t,e,new Set),e}function Ze(t,e,n){if(null==t)return;if(t instanceof We)return void e.push(t);if(s=t,!Array.isArray(s)&&"object"!=typeof s)return;var s;const i=t;for(const t in i){const s=i[t];n.has(s)||(n.add(s),Ze(s,e,n))}}function Ye(t){return null!=t.kernelName}class Xe{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((t=>t.name))))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class Qe{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new Xe}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(`${t} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new Ee(this.backendInstance),!0}setupRegisteredKernels(){Ae(this.backendName).forEach((t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(t){Ae(t).forEach((e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])}))}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const n=e.factory();if(!n||n instanceof class{refCount(t){return Zt("refCount")}incRef(t){return Zt("incRef")}timerAvailable(){return!0}time(t){return Zt("time")}read(t){return Zt("read")}readSync(t){return Zt("readSync")}numDataIds(){return Zt("numDataIds")}disposeData(t,e){return Zt("disposeData")}write(t,e,n){return Zt("write")}move(t,e,n,s,i){return Zt("move")}memory(){return Zt("memory")}floatPrecision(){return Zt("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return Zt("dispose")}}||"function"!=typeof n.then)return this.registry[t]=n,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,s=n.then((n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0))).catch((n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1)));return this.pendingBackendInit=s,{success:s,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(`${t} backend not found in registry`);this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority))}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e),r=s.refCount(e);s.disposeData(e,!0),n.backend=t,t.move(e,i,n.shape,n.dtype,r),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return Qe.nextTensorId++}nextVariableId(){return Qe.nextVariableId++}clone(t){const e=tn.runKernel("Identity",{x:t}),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],(t=>({x:()=>{const e={x:t},n={dtype:"float32"};return tn.runKernel("Cast",e,n)}})),[],{}),e}runKernel(t,e,n){null==this.backendName&&this.backend;if(!(null!=Ne(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach((t=>{i+="complex64"===t.dtype?3:1}));const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const s=this.isTapeOn(),i=this.state.numBytes,r=this.state.numTensors;let a,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const l=Ye(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(Ye(t)){const{kernelName:e,inputs:i,attrs:r}=t;null==this.backendName&&this.backend;const l=Ne(e,this.backendName);Yt(null!=l,(()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`)),a=()=>{const t=this.backend.numDataIds();o=l.kernelFunc({inputs:i,attrs:r,backend:this.backend});const a=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,a);const u=a.map((t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)}));if(s){const t=this.getTensorsForGradient(e,i,u);n=this.saveTensorsForBackwardMode(t)}return u}}else{const{forwardFunc:e}=t,i=t=>{s&&(n=t.map((t=>this.keep(this.clone(t)))))};a=()=>{const t=this.backend.numDataIds();o=this.tidy((()=>e(this.backend,i)));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(l,t,n),n}}const{inputs:u,attrs:h}=t,c=Ye(t)?null:t.backwardsFunc;let p;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(p=this.profiler.profileKernel(l,u,(()=>a())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(p),e=p.outputs):e=a()})),s&&this.addTapeNode(l,u,e,c,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:l,bytesAdded:this.state.numBytes-i,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-r,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(u).map((t=>null!=u[t]?u[t].shape:null)),outputShapes:e.map((t=>t.shape)),kernelTimeMs:p.timeMs,extraInfo:p.extraInfo}),Array.isArray(o)?e:e[0]}saveTensorsForBackwardMode(t){return t.map((t=>this.keep(this.clone(t))))}getTensorsForGradient(t,e,n){const s=Ie(t);if(null!=s){const t=s.inputsToSave||[],i=s.outputsToSave||[];let r;s.saveAllInputs?(Yt(Array.isArray(e),(()=>"saveAllInputs is true, expected inputs to be an array.")),r=Object.keys(e).map((t=>e[t]))):r=t.map((t=>e[t]));const a=n.filter(((t,e)=>i[e]));return r.concat(a)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&ae(t[0])&&(i=t.map((t=>function(t,e="utf-8"){return e=e||"utf-8",ye().platform.encode(t,e)}(t))));const r=s.write(i,e,n),a=new We(e,n,r,this.nextTensorId());if(this.trackTensor(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach((t=>e+=t.length)),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new We(e,n=n||"float32",t,this.nextTensorId());return this.trackTensor(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new Pe(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}trackTensor(t,e){this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*re(t.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(t.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n})),t instanceof Pe||this.track(t)}incRef(t,e){this.trackTensor(t,e),this.backend.incRef(t.dataId)}removeDataId(t,e){this.state.tensorInfo.has(t)&&this.state.tensorInfo.get(t).backend===e&&(this.state.tensorInfo.delete(t),this.state.numDataBuffers--)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;const e=this.state.tensorInfo.get(t.dataId);if(this.state.numTensors--,"string"===t.dtype&&(this.state.numStringTensors--,this.state.numBytes-=e.bytes),"complex64"!==t.dtype&&"string"!==t.dtype){const e=t.size*re(t.dtype);this.state.numBytes-=e}e.backend.disposeData(t.dataId)&&this.removeDataId(t.dataId,e.backend)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((t=>t.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=Ie(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map(((t,e)=>{if(null==t){const t=n[e],s=de(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t})),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=Je(t),n=new Set(e.map((t=>t.id)));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach((t=>{t.kept||t.scopeId!==s.id||this.track(t)}))}gradients(t,e,n,s=!1){if(Yt(e.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const i=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",t)));Yt(i instanceof We,(()=>"The result y returned by f() must be a tensor."));const r=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach((t=>s[t.id]=!0)),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,i);if(!s&&0===r.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const t={};t[i.id]=null==n?function(t){const e=pe(Qt(t),"float32");return tn.makeTensor(e,t,"float32")}(i.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach((e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)})),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const o=r.gradient(a);for(const e in r.inputs){if(!(e in o))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(o)}.`);const i=n((()=>o[e]()));if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!te(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,r,(t=>this.tidy(t)),en);const s=e.map((e=>t[e.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((t=>{for(const e of t.saved)e.dispose()})),this.state.activeTape=null),{value:i,grads:s}}))}customGrad(t){return Yt(le(t),(()=>"The f passed in customGrad(f) must be a function.")),(...e)=>{let n;Yt(e.every((t=>t instanceof We)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const s={};e.forEach(((t,e)=>{s[e]=t}));return this.runKernelFunc({forwardFunc:(s,i)=>(n=t(...e,i),Yt(n.value instanceof We,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),Yt(le(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(t,s)=>{const i=n.gradFunc(t,s),r=Array.isArray(i)?i:[i];Yt(r.length===e.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),Yt(r.every((t=>t instanceof We)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const a={};return r.forEach(((t,e)=>{a[e]=()=>t})),a},inputs:s})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=Te(),n=await this.backend.time(t);return n.wallMs=Te()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new Xe;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}Qe.nextTensorId=0,Qe.nextVariableId=0;const tn=function(){const t=ke();if(null==t._tfengine){const e=new ge(t);t._tfengine=new Qe(e)}var e;return e=t._tfengine.ENV,we=e,Be=()=>t._tfengine,t._tfengine}();function en(t,e){const n={a:t,b:e};return tn.runKernel("Add",n)}function nn(t,e){let n=t;if(ie(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||ie(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&ye().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&sn(t,s,[]),s}function sn(t,e,n){if(n=n||[],!Array.isArray(t)&&!ie(t))return void Yt(0===e.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${e[0]} elements`));Yt(e.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${t.length} elements`)),Yt(t.length===e[0],(()=>`Element arr[${n.join("][")}] should have ${e[0]} elements, but has ${t.length} elements`));const s=e.slice(1);for(let e=0;e<t.length;++e)sn(t[e],s,n.concat(e))}function rn(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function an(t,e,n,s="numeric"){if(t instanceof We)return rn(s,t.dtype,e,n),t;let i=oe(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),rn(s,i,e,n),null==t||!ie(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const r=nn(t,i);ie(t)||Array.isArray(t)||(t=[t]);const a="string"!==i?ze(t,i):Xt(t,[],!0);return tn.makeTensor(a,r,i)}function on(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map(((t,i)=>an(t,`${e}[${i}]`,n,s)))}function ln(t){const e=Object.keys(t);if(1!==e.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${e.length} keys.`);let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{tn.startScope(n);try{const e=s(...t);return fe(e)&&console.error("Cannot return a Promise inside of tidy."),tn.endScope(e),e}catch(t){throw tn.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const un=ln({cast_:function(t,e){const n=an(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error(`Failed to cast to unknown dtype ${e}`);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return tn.runKernel("Cast",s,i)}});const hn=ln({mul_:function(t,e){let n=an(t,"a","mul"),s=an(e,"b","mul");[n,s]=He(n,s);const i={a:n,b:s};return tn.runKernel("Multiply",i)}});const cn=ln({step_:function(t,e=0){const n={x:an(t,"x","step")},s={alpha:e};return tn.runKernel("Step",n,s)}}),pn={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,cn(un(n,"float32"),-1))}}};const dn=ln({floorDiv_:function(t,e){let n=an(t,"a","floorDiv"),s=an(e,"b","floorDiv");[n,s]=He(n,s);const i={a:n,b:s};return tn.runKernel("FloorDiv",i)}});const fn=ln({div_:function(t,e){let n=an(t,"a","div"),s=an(e,"b","div");if([n,s]=He(n,s),"int32"===n.dtype&&"int32"===s.dtype)return dn(n,s);const i={a:n,b:s};return tn.runKernel("RealDiv",i,{})}});const gn=ln({neg_:function(t){const e={x:an(t,"x","neg")};return tn.runKernel("Neg",e)}});function mn(t,e,n,s){if(null==s&&(s=oe(t)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!ie(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach((e=>{Yt(Number.isInteger(e)&&e>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`))}))}(e);const t=Qt(e),s=Qt(n);Yt(t===s,(()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${s}`));for(let t=0;t<n.length;++t){const s=n[t],i=t!==n.length-1||s!==Qt(e.slice(t));Yt(n[t]===e[t]||!i,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `))}}return ie(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==s?ze(t,s):Xt(t,[],!0),tn.makeTensor(t,e,s)}function yn(t,e){if((ie(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&ie(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return mn(t,[],[],e)}const bn=ln({sqrt_:function(t){const e={x:an(t,"x","sqrt")};return tn.runKernel("Sqrt",e)}});const wn=ln({square_:function(t){const e=an(t,"x","square");return tn.runKernel("Square",{x:e},{})}});const kn=ln({sub_:function(t,e){let n=an(t,"a","sub"),s=an(e,"b","sub");[n,s]=He(n,s);const i={a:n,b:s};return tn.runKernel("Sub",i)}}),vn={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=wn(un(n,"float32")),s=bn(kn(yn(1),e));return gn(fn(t,s))}}}},Sn={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=bn(kn(wn(un(n,"float32")),1));return fn(t,e)}}}};function xn(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function Nn(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const In=ln({reshape_:function(t,e){const n={x:an(t,"x","reshape","string_or_numeric")},s={shape:e};return tn.runKernel("Reshape",n,s)}});const An=ln({sum_:function(t,e=null,n=!1){let s=an(t,"x","sum");"bool"===s.dtype&&(s=un(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return tn.runKernel("Sum",i,r)}}),Cn={kernelName:"Add",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{let e=t;const s=xn(n.shape,i);return s.length>0&&(e=An(e,s)),In(e,n.shape)},b:()=>{let e=t;const n=xn(s.shape,i);return n.length>0&&(e=An(e,n)),In(e,s.shape)}}}},zn={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}};const Tn=ln({zerosLike_:function(t){const e={x:an(t,"x","zerosLike")};return tn.runKernel("ZerosLike",e)}}),$n={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Tn(n)}}},En={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Tn(n)}}},Fn={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,bn(kn(yn(1),wn(un(n,"float32")))))}}};const Dn=ln({add_:function(t,e){let n=an(t,"a","add"),s=an(e,"b","add");[n,s]=He(n,s);const i={a:n,b:s};return tn.runKernel("Add",i)}}),Ln={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=bn(Dn(yn(1),wn(un(n,"float32"))));return fn(t,e)}}}},_n={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{const e=Dn(wn(n),wn(s));let r=hn(t,fn(s,e));const a=xn(n.shape,i);return a.length>0&&(r=An(r,a)),In(r,n.shape)},b:()=>{const e=Dn(wn(n),wn(s));let r=gn(hn(t,fn(n,e)));const a=xn(s.shape,i);return a.length>0&&(r=An(r,a)),In(r,s.shape)}}}},Rn={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,Dn(wn(un(n,"float32")),1))}}},Mn={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,kn(yn(1),wn(un(n,"float32"))))}}};const On=ln({avgPool3dGrad_:function(t,e,n,s,i,r){const a=an(t,"dy","avgPool3dGrad"),o=an(e,"input","avgPool3dGrad");let l=a,u=o,h=!1;4===o.rank&&(h=!0,l=In(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),u=In(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),Yt(5===l.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${l.rank}.`)),Yt(5===u.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${u.rank}.`)),null!=r&&Yt(ee(i),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const c={dy:l,input:u},p={filterSize:n,strides:s,pad:i,dimRoundingMode:r},d=tn.runKernel("AvgPool3DGrad",c,p);return h?In(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}}),Bn={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>On(t,s,i,r,a,o)}}};const Wn=ln({avgPoolGrad_:function(t,e,n,s,i){const r=an(t,"dy","avgPoolGrad"),a=an(e,"input","avgPoolGrad");Yt(a.rank===r.rank,(()=>`Rank of input (${a.rank}) does not match rank of dy (${r.rank})`));let o=a,l=r,u=!1;3===a.rank&&(u=!0,o=In(a,[1,a.shape[0],a.shape[1],a.shape[2]]),l=In(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Yt(4===l.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${l.rank}.`)),Yt(4===o.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${o.rank}.`));const h={dy:l,input:o},c={filterSize:n,strides:s,pad:i},p=tn.runKernel("AvgPoolGrad",h,c);return u?In(p,[p.shape[1],p.shape[2],p.shape[3]]):p}}),Pn={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>Wn(t,s,i,r,a)}}};const Un=ln({matMul_:function(t,e,n=!1,s=!1){let i=an(t,"a","matMul"),r=an(e,"b","matMul");[i,r]=He(i,r);const a={a:i,b:r},o={transposeA:n,transposeB:s};return tn.runKernel("BatchMatMul",a,o)}}),jn={kernelName:"BatchMatMul",inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>Un(t,i,!1,!1),b:()=>Un(t,s,!0,!1)}:r&&!a?{a:()=>Un(i,t,!1,!0),b:()=>Un(s,t,!1,!1)}:{a:()=>Un(i,t,!0,!0),b:()=>Un(t,s,!0,!0)}:{a:()=>Un(t,i,!1,!0),b:()=>Un(s,t,!0,!1)}}};const Vn=ln({spaceToBatchND_:function(t,e,n){const s=an(t,"x","spaceToBatchND");Yt(s.rank>=1+e.length,(()=>`input rank ${s.rank} should be > than [blockShape] ${e.length}`)),Yt(n.length===e.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`)),Yt(s.shape.reduce(((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t),!0),(()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`));const i={x:s},r={blockShape:e,paddings:n};return tn.runKernel("SpaceToBatchND",i,r)}}),Kn={kernelName:"BatchToSpaceND",gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>Vn(t,s,i)}}},qn={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>An(t,o,!0)}}},Gn={kernelName:"Cast",gradFunc:t=>({x:()=>t.clone()})},Hn={kernelName:"Ceil",gradFunc:t=>({x:()=>Tn(t)})};const Jn=ln({greaterEqual_:function(t,e){let n=an(t,"a","greaterEqual","string_or_numeric"),s=an(e,"b","greaterEqual","string_or_numeric");[n,s]=He(n,s),Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("GreaterEqual",i)}});const Zn=ln({lessEqual_:function(t,e){let n=an(t,"a","lessEqual","string_or_numeric"),s=an(e,"b","lessEqual","string_or_numeric");[n,s]=He(n,s),Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("LessEqual",i)}});const Yn=ln({logicalAnd_:function(t,e){const n=an(t,"a","logicalAnd","bool"),s=an(e,"b","logicalAnd","bool");Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("LogicalAnd",i)}});const Xn=ln({clone_:function(t){const e={x:an(t,"x","clone","string_or_numeric")};return tn.runKernel("Identity",e)}});const Qn=ln({broadcastTo_:function(t,e){let n=an(t,"broadcastTo","x");const s=n.shape;if(e.some((t=>!(t>0)||t%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=In(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map(((t,e)=>t>1?e:-1)).filter((t=>t>=0)).length)return Xn(n);const a={x:n},o={reps:r};return tn.runKernel("Tile",a,o)}});const ts=ln({where_:function(t,e,n){const s=an(e,"a","where"),i=an(n,"b","where"),r=an(t,"condition","where","bool"),a=Nn(Nn(r.shape,s.shape),i.shape),o={condition:Qn(r,a),t:Qn(s,a),e:Qn(i,a)};return tn.runKernel("Select",o)}}),es={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>ts(Yn(Jn(s,i),Zn(s,r)),t,Tn(t))}}},ns={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:pn.gradFunc};const ss=ln({split_:function(t,e,n=0){const s={x:an(t,"x","split")},i={numOrSizeSplits:e,axis:n};return tn.runKernel("SplitV",s,i)}}),is={kernelName:"Concat",saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,r=se(i,e[0].shape)[0],a=s.map((t=>t[r]));return ss(t,a,r).map((t=>()=>t))}};const rs=ln({conv2DBackpropFilter_:function(t,e,n,s,i,r="NHWC",a){let o=t;3===t.rank&&(o=In(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=In(e,[1,e.shape[0],e.shape[1],e.shape[2]])),Yt(4===o.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${o.shape}.`)),Yt(4===l.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${l.shape}.`)),Yt(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const u="NHWC"===r?o.shape[3]:o.shape[1],h="NHWC"===r?l.shape[3]:l.shape[1];Yt(u===n[2],(()=>`Error in conv2dDerFilter: depth of input ${u}) must match input depth in filter (${n[2]}.`)),Yt(h===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`)),null!=a&&Yt(ee(i),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const c={x:o,dy:l},p={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,filterShape:n};return tn.runKernel("Conv2DBackpropFilter",c,p)}});const as=ln({conv2DBackpropInput_:function(t,e,n,s,i,r="NHWC",a){Yt(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let o=t,l=e,u=!1;3===e.rank&&(u=!0,l=In(e,[1,e.shape[0],e.shape[1],e.shape[2]]),o=[1,t[0],t[1],t[2]]),Yt(4===o.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${o.length}.`)),Yt(4===l.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${l.rank}`)),Yt(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const h="NHWC"===r?o[3]:o[1],c="NHWC"===r?l.shape[3]:l.shape[1];Yt(h===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${h}) must match input depth for filter ${n.shape[2]}.`)),Yt(c===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${c}) must match output depth for filter ${n.shape[3]}.`)),null!=a&&Yt(ee(i),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const p={dy:l,filter:n},d={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,inputShape:o},f=tn.runKernel("Conv2DBackpropInput",p,d);return u?In(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});function os(t){const[e,n,s]=function(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}(t);return 1===e&&1===n&&1===s}function ls(t,e){return os(t)||os(e)}const us={kernelName:"Conv2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:r,strides:a,pad:o,dataFormat:l}=n;return Yt(os(r),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${r}'`)),{x:()=>as(s.shape,t,i,a,o,l),filter:()=>rs(s,t,i.shape,a,o,l)}}};const hs=ln({conv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=an(t,"x","conv2d"),l=an(e,"filter","conv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=In(o,[1,o.shape[0],o.shape[1],o.shape[2]])),Yt(4===u.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${u.rank}.`)),Yt(4===l.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${l.rank}.`)),null!=a&&Yt(ee(s),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const c="NHWC"===i?u.shape[3]:u.shape[1];Yt(c===l.shape[2],(()=>`Error in conv2d: depth of input (${c}) must match input depth for filter ${l.shape[2]}.`)),Yt(ls(n,r),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${r}'`));const p={x:u,filter:l},d={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},f=tn.runKernel("Conv2D",p,d);return h?In(f,[f.shape[1],f.shape[2],f.shape[3]]):f}}),cs={kernelName:"Conv2DBackpropInput",inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>hs(t,i,r,a,o,1,l),filter:()=>rs(t,s,i.shape,r,a,o,l)}}};const ps=ln({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=In(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let a=e;4===a.rank&&(a=In(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),Yt(5===r.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${r.shape}.`)),Yt(5===a.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${a.shape}.`)),Yt(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),Yt(r.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${r.shape[4]}) must match input depth in filter (${n[3]}.`)),Yt(a.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${a.shape[4]}) must match output depth for filter (${n[4]}).`));const o={x:r,dy:a},l={strides:s,pad:i,filterShape:n};return tn.runKernel("Conv3DBackpropFilterV2",o,l)}});const ds=ln({conv3DBackpropInput_:function(t,e,n,s,i){Yt(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let r=t,a=e,o=!1;4===e.rank&&(o=!0,a=In(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),r=[1,t[0],t[1],t[2],t[3]]);const l=r[4],u=a.shape[4];Yt(5===r.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${r.length}.`)),Yt(5===a.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${a.rank}`)),Yt(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),Yt(l===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${l}) must match input depth for filter ${n.shape[3]}.`)),Yt(u===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${u}) must match output depth for filter ${n.shape[4]}.`));const h={dy:a,filter:n},c={pad:i,strides:s,inputShape:r},p=tn.runKernel("Conv3DBackpropInputV2",h,c);return o?In(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}}),fs={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r}=n;Yt(os(s),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`));const[a,o]=e;return{x:()=>ds(a.shape,t,o,i,r),filter:()=>ps(a,t,o.shape,i,r)}}};const gs=ln({sin_:function(t){const e={x:an(t,"x","sin")};return tn.runKernel("Sin",e)}}),ms={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(gn(gs(un(n,"float32"))),t)}}};const ys=ln({sinh_:function(t){const e={x:an(t,"x","sinh")};return tn.runKernel("Sinh",e)}}),bs={kernelName:"Cosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(ys(un(n,"float32")),t)}}};function ws(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map((t=>1)),e)}function ks(t){return t.map(((t,e)=>[e,t])).sort(((t,e)=>t[1]-e[1])).map((t=>t[0]))}const vs=ln({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:an(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return tn.runKernel("Cumsum",i,r)}});const Ss=ln({transpose_:function(t,e){const n=an(t,"x","transpose");if(null==e&&(e=n.shape.map(((t,e)=>e)).reverse()),Yt(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`)),e.forEach((t=>{Yt(t>=0&&t<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${e}`))})),n.rank<=1)return n.clone();const s={x:n},i={perm:e};return tn.runKernel("Transpose",s,i)}}),xs={kernelName:"Cumsum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach((t=>n.push(t))),n}([i],s.rank);let n=vs(t,i,r,!a);return null!=e&&(n=Ss(n,e)),n}}}};const Ns=ln({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=In(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=In(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return tn.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const Is=ln({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=In(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=tn.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?In(c,[c.shape[1],c.shape[2],c.shape[3]]):c}}),As={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r,dimRoundingMode:a}=n,o=null==s?[1,1]:s;Yt(os(o),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`));const[l,u]=e;return Yt(4===l.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${l.rank}.`)),Yt(4===u.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${u.rank}.`)),Yt(l.shape[3]===u.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${l.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`)),Yt(ls(i,o),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${i} and dilations '${o}'.`)),null!=a&&Yt(ee(r),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`)),{x:()=>Is(l.shape,t,u,i,r,s,a),filter:()=>Ns(l,t,u.shape,i,r,s,a)}}},Cs={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>tn.runKernel("Dilation2DBackpropInput",r,n),filter:()=>tn.runKernel("Dilation2DBackpropFilter",a,n)}}},zs={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>tn.runKernel("EluGrad",s)}}};const Ts=ln({exp_:function(t){const e={x:an(t,"x","exp")};return tn.runKernel("Exp",e)}}),$s={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=hn(Ts(gn(wn(n))),2/Math.sqrt(Math.PI));return{x:()=>hn(t,s)}}},Es={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,n)}}},Fs={kernelName:"ExpandDims",inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>In(t,n.shape)}}},Ds={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,Ts(n))}}},Ls={kernelName:"Floor",gradFunc:t=>({x:()=>Tn(t)})},_s={kernelName:"FloorDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{const e=fn(t,un(s,"float32")),r=xn(n.shape,i);return r.length>0?In(An(e,r),n.shape):e},b:()=>{let e=hn(t,un(n,"float32"));const r=xn(s.shape,i);r.length>0&&(e=In(An(e,r),s.shape));const a=wn(s);return gn(fn(e,un(a,"float32")))}}}};const Rs=ln({rsqrt_:function(t){const e={x:an(t,"x","rsqrt")};return tn.runKernel("Rsqrt",e)}});const Ms=ln({tile_:function(t,e){const n=an(t,"x","tile","string_or_numeric");Yt(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`));const s={x:n},i={reps:e};return tn.runKernel("Tile",s,i)}}),Os={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?yn(1):o,u=xn(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=kn(i,r),p=hn(t,l),d=Rs(Dn(a,yn(s))),f=hn(hn(hn(d,d),d),yn(-.5));return{x:()=>1===r.rank?In(hn(hn(t,Ms(In(d,[1,1,1,r.shape[0]]),h)),l),i.shape):In(hn(hn(t,d),l),i.shape),mean:()=>{let t=hn(hn(d,yn(-1)),p);return 1===r.rank&&(t=An(t,u)),In(t,r.shape)},variance:()=>{let t=hn(hn(f,c),p);return 1===r.rank&&(t=An(t,u)),In(t,r.shape)},scale:()=>{const e=hn(c,d);let n=hn(t,e);return 1===r.rank&&(n=An(n,u)),In(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=An(e,u)),In(e,r.shape)}}}};const Bs=ln({unsortedSegmentSum_:function(t,e,n){const s=an(t,"x","unsortedSegmentSum"),i=an(e,"segmentIds","unsortedSegmentSum","int32");Yt(ee(n),(()=>"numSegments must be of dtype int"));const r={x:s,segmentIds:i},a={numSegments:n};return tn.runKernel("UnsortedSegmentSum",r,a)}}),Ws={kernelName:"GatherV2",inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=se(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=Ps(0,l),p=Ps(l+1,l+1+h),d=Us([o,[n],u]),f=In(t,d),g=In(i,[n]),m=Us([[l],c,p]),y=Ss(f,m);let b=Bs(y,g,s.shape[a]);const w=ks(m);return b=Ss(b,w),b},indices:()=>i}}};function Ps(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Us(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const js={kernelName:"GreaterEqual",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Tn(n),b:()=>Tn(s)}}},Vs={kernelName:"Identity",gradFunc:t=>({x:()=>un(t,"float32")})},Ks={kernelName:"IsFinite",gradFunc:t=>({x:()=>Tn(t)})},qs={kernelName:"IsInf",gradFunc:t=>({x:()=>Tn(t)})},Gs={kernelName:"IsNan",gradFunc:t=>({x:()=>Tn(t)})};const Hs=ln({greater_:function(t,e){let n=an(t,"a","greater","string_or_numeric"),s=an(e,"b","greater","string_or_numeric");[n,s]=He(n,s),Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("Greater",i)}}),Js={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=Hs(s,0);return{x:()=>ts(r,t,hn(t,i))}}},Zs={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,Dn(n,1))}}},Ys={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,un(n,"float32"))}}},Xs={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=Ts(s);return kn(t,hn(An(t,i,!0),e))}}}};const Qs=ln({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return tn.runKernel("LRNGrad",o,l)}}),ti={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>Qs(s,i,t,r,a,o,l)}}};const ei=ln({equal_:function(t,e){let n=an(t,"a","equal","string_or_numeric"),s=an(e,"b","equal","string_or_numeric");[n,s]=He(n,s),Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("Equal",i)}});function ni(t,e,n,s){return e.rank<n.rank&&(e=In(e,ws(e.shape,s))),t.rank<n.rank&&(t=In(t,ws(t.shape,s))),{x:()=>hn(t,un(ei(n,e),t.dtype))}}const si={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=ni(t,e[1],r,se(i,r.shape));return{x:()=>a.x()}}};const ii=ln({less_:function(t,e){let n=an(t,"a","less","string_or_numeric"),s=an(e,"b","less","string_or_numeric");[n,s]=He(n,s),Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("Less",i)}}),ri={kernelName:"Maximum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>hn(t,un(Jn(n,s),"float32")),b:()=>hn(t,un(ii(n,s),"float32"))}}};const ai=ln({maxPool3dGrad_:function(t,e,n,s,i,r,a){const o=an(t,"dy","maxPool3dGrad"),l=an(e,"input","maxPool3dGrad"),u=an(n,"output","maxPool3dGrad");let h=o,c=l,p=u,d=!1;4===l.rank&&(d=!0,h=In(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=In(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=In(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),Yt(5===h.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),Yt(5===c.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),Yt(5===p.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${p.rank}.`)),null!=a&&Yt(ee(r),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const f={dy:h,input:c,output:p},g={filterSize:s,strides:i,pad:r,dimRoundingMode:a},m=tn.runKernel("MaxPool3DGrad",f,g);return d?In(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),oi={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>ai(t,s,i,r,a,o,l)}}};const li=ln({maxPoolGrad_:function(t,e,n,s,i,r,a){const o=an(t,"dy","maxPoolGrad"),l=an(e,"input","maxPoolGrad"),u=an(n,"output","maxPoolGrad");Yt(l.rank===o.rank,(()=>`Rank of input (${l.rank}) does not match rank of dy (${o.rank})`)),Yt(4===o.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${o.rank}.`)),Yt(4===l.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${l.rank}.`)),null!=a&&Yt(ee(r),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const h={dy:o,input:l,output:u},c={filterSize:s,strides:i,pad:r,dimRoundingMode:a};return tn.runKernel("MaxPoolGrad",h,c)}}),ui={kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>li(t,s,i,r,a,o)}}};const hi=ln({complex_:function(t,e){const n=an(t,"real","complex"),s=an(e,"imag","complex");!function(t,e,n=""){Yt(te(t,e),(()=>n+` Shapes ${t} and ${e} must match`))}(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const i={real:n,imag:s};return tn.runKernel("Complex",i)}});function ci(t,e="float32"){if("complex64"===e){const e=ci(t,"float32"),n=ci(t,"float32");return hi(e,n)}const n=de(Qt(t),e);return tn.makeTensor(n,t,e)}function pi(t,e="float32"){if("complex64"===e){const e=pi(t,"float32"),n=ci(t,"float32");return hi(e,n)}const n=pe(Qt(t),e);return tn.makeTensor(n,t,e)}const di={kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=se(i,s.shape),a=Qt(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map((e=>t[e]))]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach((t=>{e[t]=1}));const n=In(t,e);return fn(hn(n,pi(s.shape,"float32")),a)}}}},fi={kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=ni(t,a,r,se(i,r.shape));return{x:()=>o.x()}}},gi={kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>hn(t,un(Zn(n,s),"float32")),b:()=>hn(t,un(Hs(n,s),"float32"))}}};const mi=ln({slice_:function(t,e,n){const s=an(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return tn.runKernel("Slice",i,r)}}),yi={kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>mi(t,r,s.shape)}}};const bi=ln({floor_:function(t){const e={x:an(t,"x","floor")};return tn.runKernel("Floor",e)}}),wi={kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{const e=xn(n.shape,i);return e.length>0?In(An(t,e),n.shape):t},b:()=>{const e=hn(t,gn(bi(fn(n,s)))),r=xn(s.shape,i);return r.length>0?In(An(e,r),s.shape):e}}}},ki={kernelName:"Multiply",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{const e=hn(t,un(s,"float32")),r=xn(n.shape,i);return r.length>0?In(An(e,r),n.shape):e},b:()=>{const e=hn(t,un(n,"float32")),r=xn(s.shape,i);return r.length>0?In(An(e,r),s.shape):e}}}},vi={kernelName:"Neg",gradFunc:t=>({x:()=>gn(t)})},Si={kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>ci(n.shape,"float32")}}},xi={kernelName:"OnesLike",gradFunc:t=>({x:()=>Tn(t)})};const Ni=ln({unstack_:function(t,e=0){const n=an(t,"x","unstack","string_or_numeric");Yt(e>=-n.shape.length&&e<n.shape.length,(()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`));const s={value:n},i={axis:e};return tn.runKernel("Unpack",s,i)}}),Ii={kernelName:"Pack",saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return Ni(t,s).map((t=>()=>t))}},Ai={kernelName:"PadV2",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>mi(t,r,s.shape)}}};const Ci=ln({log_:function(t){const e={x:an(t,"x","log")};return tn.runKernel("Log",e)}});const zi=ln({pow_:function(t,e){let n=an(t,"base","pow"),s=an(e,"exp","pow");[n,s]=He(n,s);const i={a:n,b:s};return tn.runKernel("Pow",i)}}),Ti={kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=Nn(r.shape,a.shape);return{a:()=>{const e=un(a,"float32");let n=hn(t,hn(e,zi(r,kn(e,yn(1)))));const s=xn(r.shape,o);return s.length>0&&(n=An(n,s)),In(n,r.shape)},b:()=>{const e=Hs(r,0),n=ts(e,Ci(r),Tn(r));let s=hn(t,hn(i,n));const l=xn(a.shape,o);return l.length>0&&(s=An(s,l)),In(s,a.shape)}}}},$i={kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=Hs(n,0);return{x:()=>ts(i,t,hn(t,s)),alpha:()=>{let e=ts(i,Tn(t),hn(t,n));const r=xn(s.shape,t.shape);return r.length>0&&(e=An(e,r)),In(e,s.shape)}}}},Ei={kernelName:"RealDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{const e=fn(t,un(s,"float32")),r=xn(n.shape,i);return r.length>0?In(An(e,r),n.shape):e},b:()=>{let e=hn(t,un(n,"float32"));const r=xn(s.shape,i);r.length>0&&(e=In(An(e,r),s.shape));const a=wn(s);return gn(fn(e,un(a,"float32")))}}}},Fi={kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,gn(wn(n)))}}},Di={kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=hn(Zn(n,6),cn(n));return{x:()=>hn(t,un(s,"float32"))}}},Li={kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,un(cn(n),"float32"))}}},_i={kernelName:"Reshape",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,n.shape)}}},Ri={kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>tn.runKernel("ResizeBilinearGrad",i,n)}}},Mi={kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>tn.runKernel("ResizeNearestNeighborGrad",i,n)}}};const Oi=ln({reverse_:function(t,e){const n={x:an(t,"x","reverse")},s={dims:e};return tn.runKernel("Reverse",n,s)}}),Bi={kernelName:"Reverse",gradFunc:(t,e,n)=>{const{dims:s}=n,i=se(s,t.shape);return{x:()=>Oi(t,i)}}},Wi={kernelName:"Round",gradFunc:t=>({x:()=>Tn(t)})},Pi={kernelName:"Rsqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>gn(fn(t,hn(zi(n,1.5),2)))}}};const Ui=ln({logicalNot_:function(t){const e={x:an(t,"x","logicalNot","bool")};return tn.runKernel("LogicalNot",e)}}),ji={kernelName:"Select",inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>un(Tn(n),"float32"),t:()=>hn(t,un(n,t.dtype)),e:()=>hn(t,un(Ui(n),t.dtype))}}},Vi={kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Hs(n,yn(0)),s=yn(1.7580993408473768),i=yn(1.0507009873554805),r=hn(t,i),a=hn(hn(t,s),Ts(un(n,"float32")));return ts(e,r,a)}}}},Ki={kernelName:"Sigmoid",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,hn(n,kn(yn(1),n)))}}},qi={kernelName:"Sign",gradFunc:t=>({x:()=>Tn(t)})};const Gi=ln({cos_:function(t){const e={x:an(t,"x","cos")};return tn.runKernel("Cos",e)}}),Hi={kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(Gi(un(n,"float32")),t)}}};const Ji=ln({cosh_:function(t){const e={x:an(t,"x","cosh")};return tn.runKernel("Cosh",e)}}),Zi={kernelName:"Sinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(Ji(un(n,"float32")),t)}}};const Yi=ln({pad_:function(t,e,n=0){const s=an(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return tn.runKernel("PadV2",r,i)}});const Xi={kernelName:"Slice",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:r}=n,a=s.shape,[o,l]=function(t,e,n){let s;const i=t.shape.length;let r;return s="number"==typeof e?[e,...new Array(i-1).fill(0)]:e.length<i?e.concat(new Array(i-e.length).fill(0)):e.slice(),s.forEach((t=>{Yt(-1!==t,(()=>"slice() does not support negative begin indexing."))})),r=null==n?new Array(i).fill(-1):"number"==typeof n?[n,...new Array(i-1).fill(-1)]:n.length<i?n.concat(new Array(i-n.length).fill(-1)):n,r=r.map(((e,n)=>e>=0?e:(Yt(-1===e,(()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`)),t.shape[n]-s[n]))),[s,r]}(s,i,r),u=[];for(let e=0;e<t.rank;e++)u.push([o[e],a[e]-o[e]-l[e]]);return{x:()=>Yi(t,u)}}},Qi={kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=hn(t,s);return{logits:()=>kn(r,hn(An(r,[i],true),s))}}};const tr=ln({sigmoid_:function(t){const e={x:an(t,"x","sigmoid")};return tn.runKernel("Sigmoid",e)}}),er={kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,tr(n))}}};const nr=ln({batchToSpaceND_:function(t,e,n){const s=an(t,"x","batchToSpaceND"),i=e.reduce(((t,e)=>t*e));Yt(s.rank>=1+e.length,(()=>`input rank is ${s.rank} but should be > than blockShape.length ${e.length}`)),Yt(n.length===e.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`)),Yt(s.shape[0]%i==0,(()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${i}`));const r={x:s},a={blockShape:e,crops:n};return tn.runKernel("BatchToSpaceND",r,a)}}),sr={kernelName:"SpaceToBatchND",gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>nr(t,s,i)}}};const ir=ln({concat_:function(t,e=0){Yt(t.length>=1,(()=>"Pass at least one tensor to concat"));const n=on(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)})),1===n.length)return Xn(n[0]);const s=n,i={axis:e};return tn.runKernel("Concat",s,i)}}),rr={kernelName:"SplitV",gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>ir(t,s)}}},ar={kernelName:"Sqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,hn(bn(un(n,"float32")),2))}}},or={kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(t,hn(un(n,"float32"),2))}}},lr={kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=yn(2);return{a:()=>hn(t,hn(i,kn(n,s))),b:()=>hn(t,hn(i,kn(s,n)))}}},ur={kernelName:"Step",gradFunc:t=>({x:()=>Tn(t)})},hr={kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Nn(n.shape,s.shape);return{a:()=>{let e=t;const s=xn(n.shape,i);return s.length>0&&(e=An(e,s)),In(e,n.shape)},b:()=>{let e=t;const n=xn(s.shape,i);return n.length>0&&(e=An(e,n)),In(gn(e),s.shape)}}}},cr={kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;se(r,s.shape).forEach((t=>{i[t]=1}));const a=In(t,i),o=hn(a,pi(s.shape,"float32"));return{x:()=>o}}},pr={kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fn(t,wn(Gi(n)))}}},dr={kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>hn(kn(yn(1),wn(n)),t)}}},fr={kernelName:"Tile",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=Tn(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=Dn(e,mi(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=Dn(e,mi(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=Dn(e,mi(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=Dn(e,mi(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},gr={kernelName:"Transpose",gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=ks(i);return{x:()=>Ss(t,r)}}};const mr=ln({stack_:function(t,e=0){const n=on(t,"tensors","stack","string_or_numeric");Yt(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&Yt(e<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const s=n,i={axis:e};return tn.runKernel("Pack",s,i)}}),yr={kernelName:"Unpack",gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>mr(t,i)}}};const br=ln({expandDims_:function(t,e=0){const n=an(t,"x","expandDims","string_or_numeric");Yt(e<=n.rank,(()=>"Axis must be <= rank of the tensor"));const s={input:n},i={dim:e};return tn.runKernel("ExpandDims",s,i)}});const wr=ln({gather_:function(t,e,n=0,s=0){const i={x:an(t,"x","gather"),indices:an(e,"indices","gather","int32")},r={axis:n,batchDims:s};return tn.runKernel("GatherV2",i,r)}});const kr=ln({maximum_:function(t,e){let n=an(t,"a","maximum"),s=an(e,"b","maximum");[n,s]=He(n,s),"bool"===n.dtype&&(n=un(n,"int32"),s=un(s,"int32")),Nn(n.shape,s.shape);const i={a:n,b:s};return tn.runKernel("Maximum",i)}});const vr=[pn,vn,Sn,Cn,zn,$n,En,Fn,Ln,_n,Rn,Mn,Bn,Pn,jn,Kn,qn,Gn,Hn,es,ns,is,cs,us,fs,ms,bs,xs,As,Cs,Ei,zs,$s,Es,Fs,Ds,_s,Ls,Os,Ws,js,Vs,Ks,qs,Gs,Js,Zs,Ys,Xs,ti,si,si,ri,oi,ui,di,fi,gi,yi,wi,ki,vi,Si,xi,Ii,Ai,Ai,Ti,$i,Fi,Di,Li,_i,Ri,Mi,Bi,Wi,Pi,ji,Vi,Ki,qi,Hi,Zi,Xi,Qi,er,sr,sr,rr,rr,ar,lr,or,ur,hr,cr,pr,dr,fr,gr,yr,{kernelName:"UnsortedSegmentSum",inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=kr(e,Tn(e)),s=wr(t,n);let i=Jn(e,yn(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=br(i,t+1);i=Yn(i,pi(s.shape,"bool"));const a=Tn(s);return ts(i,s,a)}(t,n)}}},{kernelName:"ZerosLike",gradFunc:t=>({x:()=>Tn(t)})}];for(const t of vr)Ce(t);let Sr;function xr(){return null==Sr&&(Sr=t().epsilon()),Sr}class Nr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Nr.prototype)}}class Ir extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Ir.prototype)}}class Ar extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Ar.prototype)}}class Cr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Cr.prototype)}}class zr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,zr.prototype)}}function Tr(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function $r(t,e){if(!t)throw new zr(e)}function Er(t,e){let n=0;for(const s of t)s===e&&n++;return n}function Fr(t){return 1===t.length?t[0]:t}function Dr(t){return Array.isArray(t)?t:[t]}function Lr(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function _r(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let Rr={};function Mr(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function Or(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>Or(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?Or(e):t[n]=e.value)}}}function Br(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in Rr)r=Rr[i];else if(r=e[i],null==r)throw new Ar(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new Ar(`${s}: Improper config format: ${JSON.stringify(r)}.\n'className' and 'config' must set.`);const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in Rr?[o,l]=Rr.className:a in e&&([o,l]=e[a]),null==o)throw new Ar(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(Rr))t[e]=Rr[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},Rr);for(const t of Object.keys(n))Rr[t]=n[t];Or(r.config);const s=l(o,r.config,n,i);return Rr=Object.assign({},e),s}{const t=Object.assign({},Rr);for(const t of Object.keys(n))Rr[t]=n[t];const e=new o(r.config);return Rr=Object.assign({},t),e}}}function Wr(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function Pr(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function Ur(t){if(null==t)throw new Ar(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function jr(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new Ar(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function Vr(t,e,n=0,s=1/0){return $r(n>=0),$r(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function Kr(t,n){Array.isArray(t)?(e.assert(t.length>0,(()=>`${n} is unexpectedly an empty array.`)),t.forEach(((t,e)=>Kr(t,`element ${e+1} of ${n}`)))):e.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${n} to be a positive integer, but got ${qr(t)}.`))}function qr(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>qr(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function Gr(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function Hr(t,e){return s((()=>i(r(a(t,t),e,!0))))}class Jr extends n.Serializable{getConfig(){return{}}}class Zr extends Jr{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>{const e=Hr(t,this.axis),n=o(e,0,this.maxValue);return a(t,l(n,u(xr(),e)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}Zr.className="MaxNorm",n.registerClass(Zr);class Yr extends Jr{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>l(t,u(xr(),Hr(t,this.axis)))))}getConfig(){return{axis:this.axis}}}Yr.className="UnitNorm",n.registerClass(Yr);class Xr extends Jr{apply(t){return h(t)}}Xr.className="NonNeg",n.registerClass(Xr);class Qr extends Jr{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>{const e=Hr(t,this.axis),n=u(a(this.rate,o(e,this.minValue,this.maxValue)),a(1-this.rate,e));return a(t,l(n,u(xr(),e)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Qr.className="MinMaxNorm",n.registerClass(Qr);const ta={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function ea(t){return Mr(t)}function na(t,e={}){return Br(t,n.SerializationMap.getMap().classNameMap,e,"constraint")}function sa(t){if(null==t)return null;if("string"==typeof t){return na({className:t in ta?ta[t]:t,config:{}})}return t instanceof Jr?t:na(t)}var ia=Object.freeze({__proto__:null,maxNorm:function(t){return new Zr(t)},unitNorm:function(t){return new Yr(t)},nonNeg:function(){return new Xr},minMaxNorm:function(t){return new Qr(t)}});const ra=["channelsFirst","channelsLast"],aa=["nearest","bilinear"],oa=["valid","same","causal"],la=["max","avg"],ua=["sum","mul","concat","ave"],ha=new Map;function ca(t){jr(ra,"DataFormat",t)}function pa(t){jr(oa,"PaddingMode",t)}function da(t){jr(la,"PoolMode",t)}const fa=[];function ga(t,e){fa.push(t);try{const t=e();return fa.pop(),t}catch(t){throw fa.pop(),t}}function ma(t){if(!wa(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===fa.length?"":fa.join("/")+"/")+t}function ya(t){if(!wa(t))throw new Error("Not a valid tensor name: '"+t+"'");ha.has(t)||ha.set(t,0);const e=ha.get(t);if(ha.set(t,ha.get(t)+1),e>0){const n=`${t}_${e}`;return ha.set(n,1),n}return t}const ba=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function wa(t){return!!t.match(ba)}function ka(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function va(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s<e&&(e=s)}return e}function Sa(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s>e&&(e=s)}return e}function xa(t,e){if(e<t)throw new Ar(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Na(t,e){return y(t,e)}function Ia(t,e=-1){const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),w(t,n)}function Aa(t,e,n){return s((()=>{switch(t.rank){case 1:return g(t,e,n);case 2:return f(t,[e,0],[n,t.shape[1]]);case 3:return d(t,[e,0,0],[n,t.shape[1],t.shape[2]]);case 4:return p(t,[e,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3]]);case 5:return c(t,[e,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return c(t,[e,0,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new Ar(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function Ca(t,e,n){return s((()=>{switch(t.rank){case 1:return g(t,e,n);case 2:return f(t,[0,e],[t.shape[0],n]);case 3:return d(t,[0,0,e],[t.shape[0],t.shape[1],n]);case 4:return p(t,[0,0,0,e],[t.shape[0],t.shape[1],t.shape[2],n]);default:throw new Ar(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function za(t,e,n,i){return s((()=>{switch(t.rank){case 1:return g(t,e,n);case 2:switch(i){case 1:return Aa(t,e,n);case 2:return Ca(t,e,n);default:throw new Ar(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return Aa(t,e,n);case 2:return d(t,[0,e,0],[t.shape[0],n,t.shape[2]]);case 3:return Ca(t,e,n);default:throw new Ar(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return Aa(t,e,n);case 2:return p(t,[0,e,0,0],[t.shape[0],n,t.shape[2],t.shape[3]]);case 3:return p(t,[0,0,e,0],[t.shape[0],t.shape[1],n,t.shape[3]]);case 4:return Ca(t,e,n);default:throw new Ar(`The axis is not within the rank of the tensor ${i}`)}default:throw new Ar(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function Ta(t,e=-1){let n;return e<0&&(n=t[0].rank,e=0!==n?n:0),e===t[0].rank&&(e=-1),I(t,e)}function $a(t,e){switch(t.rank){case 1:return E([t,e]);case 2:return $([t,e],0);case 3:return T([t,e],0);case 4:return z([t,e],0);default:throw new Ar(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function Ea(t,e){if(Array.isArray(e)||(e=[e]),t.rank!==e.length)throw new Ar(`The length of input n (${e.length}) does not match the number of dimensions in input x (${t.rank})`);return k(t,e)}function Fa(t,e=0,n=1,s,i){return v(t,e,n,s,i)}function Da(t,e,n,s){if(t.rank<2||e.rank<2)throw new Cr(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${e.shape}`);if(e.rank>=3){if(t.shape.slice(-1)[0]!==e.shape.slice(-2)[0])throw new Cr(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${e.shape}`)}if(2===t.rank&&2===e.rank){const i=!1,r=!1;return A.matMul({a:t,b:e,transposeA:i,transposeB:r,bias:s?Ra(t.rank,s,"channelsLast"):null,activation:n})}{const i=t.shape.slice(),r=i.pop();t=w(t,[-1,r]);const a=e.shape.slice(),o=a.pop(),l=a.pop(),u=[...a,o],h=Array.from({length:e.rank},((t,n)=>0===n?e.rank-2:n<=e.rank-2?n-1:n));e=w(C(e,h),[l,-1]);const c=[...i,...u],p=!1,d=!1;return w(A.matMul({a:t,b:e,transposeA:p,transposeB:d,bias:s?Ra(t.rank,s,"channelsLast"):null,activation:n}),c)}}function La(t,e,n){return s((()=>(e=Array.isArray(e)?m(e,"int32"):y(e,"int32"),b(t,e,n))))}function _a(t){return a(t,t)}function Ra(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new Ar(`Unexpected bias dimensions: ${e.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===n)return 1===s.length?w(e,[1,s[0],1,1,1]):w(e,[1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?w(e,[1,1,1,1,s[0]]):w(e,[1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?w(e,[1,s[0],1,1]):w(e,[1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?w(e,[1,1,1,s[0]]):w(e,[1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?w(e,[1,s[0],1]):w(e,[1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?w(e,[1,1,s[0]]):w(e,[1].concat(s))}else if(t<3)return e;throw new Ar(`Unsupported input rank by biasAdd: ${e.rank}`)}function Ma(t,e,n){return s((()=>(null==n&&(n="channelsLast"),ca(n),u(t,Ra(t.rank,e,n)))))}function Oa(t,e,n,i){return s((()=>N(t,e,n,i)))}function Ba(t,e,n=!1){return n?t():e()}const Wa=["fanIn","fanOut","fanAvg"],Pa=["normal","uniform","truncatedNormal"];class Ua extends n.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class ja extends Ua{apply(t,e){return F(t,e)}}ja.className="Zeros",n.registerClass(ja);class Va extends Ua{apply(t,e){return D(t,e)}}Va.className="Ones",n.registerClass(Va);class Ka extends Ua{constructor(t){if(super(),"object"!=typeof t)throw new Ar(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new Ar(`config must have value set but got ${t}`);this.value=t.value}apply(t,e){return s((()=>a(L(this.value),D(t,e))))}getConfig(){return{value:this.value}}}Ka.className="Constant",n.registerClass(Ka);class qa extends Ua{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return _(t,this.minval,this.maxval,e)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}qa.className="RandomUniform",n.registerClass(qa);class Ga extends Ua{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new Cr(`randomNormal does not support dType ${e}.`);return Fa(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Ga.className="RandomNormal",n.registerClass(Ga);class Ha extends Ua{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new Cr(`truncatedNormal does not support dType ${e}.`);return R(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Ha.className="TruncatedNormal",n.registerClass(Ha);class Ja extends Ua{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return s((()=>{if(2!==t.length||t[0]!==t[1])throw new Ar("Identity matrix initializer can only be used for 2D square matrices.");return a(this.gain,M(t[0]))}))}getConfig(){return{gain:this.gain}}}Ja.className="Identity",n.registerClass(Ja);class Za extends Ua{constructor(t){if(super(),t.scale<0)throw new Ar(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,jr(Wa,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){jr(Pa,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const n=function(t,e="channelsLast"){let n,s;if(ca(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=ka(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=ka(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=ka(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),s=n[0],i=n[1];let r=this.scale;if("fanIn"===this.mode?r/=Math.max(1,s):"fanOut"===this.mode?r/=Math.max(1,i):r/=Math.max(1,(s+i)/2),"normal"===this.distribution){const n=Math.sqrt(r);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new Cr(`${this.getClassName()} does not support dType ${e}.`);return R(t,0,n,e,this.seed)}{const n=Math.sqrt(3*r);return _(t,-n,n,e)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}Za.className="VarianceScaling",n.registerClass(Za);class Ya extends Za{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Za.className}}Ya.className="GlorotUniform",n.registerClass(Ya);class Xa extends Za{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Za.className}}Xa.className="GlorotNormal",n.registerClass(Xa);class Qa extends Za{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Za.className}}Qa.className="HeNormal",n.registerClass(Qa);class to extends Za{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Za.className}}to.className="HeUniform",n.registerClass(to);class eo extends Za{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Za.className}}eo.className="LeCunNormal",n.registerClass(eo);class no extends Za{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Za.className}}no.className="LeCunNormal",n.registerClass(no);class so extends Ua{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new Cr("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,e){return s((()=>{if(t.length<2)throw new Cr("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const e=Fa(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let n=O.gramSchmidt(e);return t[0]>t[1]&&(n=C(n)),a(this.gain,n)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}so.className="Orthogonal",n.registerClass(so);const io={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function ro(t,e={}){return Br(t,n.SerializationMap.getMap().classNameMap,e,"initializer")}function ao(t){return Mr(t)}function oo(t){if("string"==typeof t){const e=t in io?io[t]:t;if("GlorotNormal"===e)return new Xa;if("GlorotUniform"===e)return new Ya;if("HeNormal"===e)return new Qa;if("HeUniform"===e)return new to;if("LeCunNormal"===e)return new eo;if("LeCunUniform"===e)return new no;{const t={};return t.className=e,t.config={},ro(t)}}return t instanceof Ua?t:ro(t)}var lo=Object.freeze({__proto__:null,zeros:function(){return new ja},ones:function(){return new Va},constant:function(t){return new Ka(t)},randomUniform:function(t){return new qa(t)},randomNormal:function(t){return new Ga(t)},truncatedNormal:function(t){return new Ha(t)},identity:function(t){return new Ja(t)},varianceScaling:function(t){return new Za(t)},glorotUniform:function(t){return new Ya(t)},glorotNormal:function(t){return new Xa(t)},heNormal:function(t){return new Qa(t)},heUniform:function(t){return new to(t)},leCunNormal:function(t){return new eo(t)},leCunUniform:function(t){return new no(t)},orthogonal:function(t){return new so(t)}});let uo=0;function ho(){return uo++}const co={};function po(t=""){return t in co||(co[t]=0),co[t]+=1,t+co[t].toString()}function fo(t){return Array.isArray(t)&&Array.isArray(t[0])}function go(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function mo(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new Ar(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function yo(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new Ar(`Expected exactly 1 Shape; got ${t.length}`)}return t}function bo(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}class wo{constructor(t,e="float32",n="Variable",s=!0,i=null){this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=ho(),n=null==n?"Variable":n,this.originalName=ma(n),this.name=ya(this.originalName),this.trainable_=s,this.constraint=i,this.val=B(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function ko(t){return t.map((t=>t.read()))}function vo(t){t.forEach((t=>{t[0].write(t[1])}))}class So{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class xo{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=ho(),null!=r&&(this.originalName=ma(r),this.name=ya(this.originalName)),this.rank=e.length}}let No=0;class Io{constructor(t,e){this.callArgs=e,this.id=No++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let Ao=0;class Co extends n.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=Ao++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=Lr(t)+"_"+po(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new Ir(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new Ar(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return Fr(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return Fr(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new Nr(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new Nr(`Layer ${this.name} is not connected, no input to return.`);return Fr(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new Nr(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new Nr(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return Fr(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=Dr(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=Dr(this.inputSpec);if(t.length!==e.length)throw new Ar(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new Ar(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new Ar(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new Ar(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new Ar(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new Ar(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new Ar(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=Dr(t);let s=!0;for(const t of n)if(!(t instanceof xo)){s=!1;break}let i=!0;for(const t of n)if(t instanceof xo){i=!1;break}if(s===i)throw new Ar("Arguments to apply() must be all SymbolicTensors or all Tensors");return ga(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of Dr(t))e.push(n.shape);this.build(Fr(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=Dr(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=Fr(r),null!=this.activityRegularizer)throw new Cr("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=Dr(t);const e=[];for(const n of t)e.push(n.shape);return Fr(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new xo(r,n,this,Dr(t),e,this.name,s))):new xo(r,s,this,Dr(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new Cr("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new Nr(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new Nr(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new Ir(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return bo(this.weights)}build(t){this.built=!0}getWeights(t=!1){return ko(t?this.trainableWeights:this.weights)}setWeights(t){s((()=>{const n=this.weights;if(n.length!==t.length)throw new Ar(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=ko(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.arraysEqual(a.shape,l.shape))throw new Ar(`Layer weight shape ${a.shape} not compatible with provided weight shape ${l.shape}`);s.push([o,l])}vo(s)}))}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new Ar(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=oo("zeros"));const o=s.apply(e,n),l=new wo(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss((()=>i.apply(l.read()))),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=Dr(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=Dr(t);e=Dr(e),n=Dr(n),s=Dr(s),i=go(i),r=go(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new Io({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function zo(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=zo(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class To extends Co{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:po("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new Ar("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new Ar("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new Ar("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new xo(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new Io({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new Ar(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function $o(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new Ar("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new To({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function Eo(t){if(null==t)return;const e=[],n=[],s=[];for(const i in t){const r=t[i];if("number"!=typeof r){const t=r;e.push(t.data()),n.push(i),s.push(t)}}if(e.length>0){const i=await Promise.all(e);for(let e=0;e<i.length;++e)t[n[e]]=i[e][0];W(s)}}function Fo(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var Do;To.className="InputLayer",n.registerClass(To),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(Do||(Do={}));class Lo{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class _o{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class Ro extends Lo{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,e){null==e&&(e={});const n=null==e.size?0:e.size;this.seen+=n;for(const t in e){const i=e[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*n;else{let e;t in this.totals?e=this.totals[t]:this.totals[t]=0;const r=s((()=>u(this.totals[t],a(i,n))));this.totals[t]=r,null!=e&&e.dispose()}}}async onEpochEnd(t,e){if(null!=e)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?e[t]=this.totals[t]/this.seen:s((()=>{const n=a(l(1,this.seen),this.totals[t]);e[t]=n,this.totals[t].dispose(),U(e[t])})))}}class Mo extends Lo{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class Oo extends Lo{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.now();return(...r)=>{const a=e.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,e,n){const s=[];null!=this.yield&&(await Eo(n),s.push(this.yield(t,e,n))),s.push(P()),await Promise.all(s)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await Eo(e),await this.epochBegin(t,e))}async onEpochEnd(t,e){const n=[];null!=this.epochEnd&&(await Eo(e),n.push(this.epochEnd(t,e))),"epoch"===this.yieldEvery&&n.push(P()),await Promise.all(n)}async onBatchBegin(t,e){null!=this.batchBegin&&(await Eo(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await Eo(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(P()):e.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await Eo(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await Eo(t),await this.trainEnd(t))}}function Bo(t,e){if(null==t&&(t={}),t instanceof Lo)return[t];if(Array.isArray(t)&&t[0]instanceof Lo)return t;return Dr(t).map((t=>new Oo(t,e)))}class Wo{constructor(){}static registerCallbackConstructor(t,n){e.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),Wo.checkForDuplicate(n),null==Wo.constructors[t]&&(Wo.constructors[t]=[]),Wo.constructors[t].push(n)}static checkForDuplicate(t){for(const e in Wo.constructors){Wo.constructors[+e].forEach((e=>{if(e===t)throw new Ar("Duplicate callback constructor.")}))}}static clear(){Wo.constructors={}}static createCallbacks(t){const e=[];for(const n in Wo.constructors){const s=+n;t>=s&&e.push(...Wo.constructors[s])}return e.map((t=>new t))}}function Po(t,e,n,s,i,r,a,o,l){const u=new Mo,h=[new Ro,...Wo.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new _o(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function Uo(t,e={},s=!1){return Br(t,n.SerializationMap.getMap().classNameMap,e,"layer",s)}function jo(t,e){return s((()=>{"float32"!==t.dtype&&(t=y(t,"float32"));const n=r(_a(t),e,!0),s=Z(n.shape,xr()),a=i(Y(n,s));return l(t,a)}))}function Vo(t,e){return s((()=>K(_a(V(e,t)),-1)))}function Ko(t,e){return s((()=>K(x(V(e,t)),-1)))}function qo(t,e){return s((()=>{const n=V(t,e),s=o(x(t),xr(),Number.MAX_VALUE),i=x(l(n,s));return a(100,K(i,-1))}))}function Go(t,e,n=!1){return s((()=>{if(n)e=J(e);else{const t=r(e,e.shape.length-1,!0);e=l(e,t)}return e=o(e,xr(),1-xr()),q(r(a(y(t,"float32"),j(e)),e.shape.length-1))}))}function Ho(t,e,n=!1){return s((()=>{const s=y(X(function(t){const e=[ka(t.shape)];return w(t,e)}(t)),"int32"),i=(e=o(e,xr(),1-xr())).shape;return Go(w(Q(s,i[i.length-1]),i),e,n)}))}function Jo(t,n){return s((()=>{let i;return i=o(n,xr(),1-xr()),i=j(l(i,V(1,i))),K(function(t,n){if(!e.arraysEqual(t.shape,n.shape))throw new Ar(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return s((()=>{const e=h(n),s=q(x(n));return u(V(e,a(n,t)),G(H(s)))}))}(t,i),-1)}))}function Zo(t,e){return s((()=>{const n=jo(t,-1),s=jo(e,-1),i=a(n,s);return q(r(i,-1))}))}Wo.constructors={};const Yo={meanSquaredError:Vo,meanAbsoluteError:Ko,meanAbsolutePercentageError:qo,meanSquaredLogarithmicError:function(t,e){return s((()=>{const n=o(e,xr(),Number.MAX_VALUE),s=j(u(1,n)),i=o(t,xr(),Number.MAX_VALUE),r=j(u(1,i));return K(_a(V(s,r)),-1)}))},squaredHinge:function(t,e){return s((()=>{const n=Y(0,V(1,a(t,e)));return K(_a(n),-1)}))},hinge:function(t,e){return s((()=>{const n=Y(0,V(1,a(t,e)));return K(n,-1)}))},categoricalHinge:function(t,e){return s((()=>{const n=r(a(t,e),-1),s=tt(a(V(1,t),e),-1);return Y(0,u(1,V(s,n)))}))},logcosh:function(t,e){return s((()=>{const n=Math.log(2),s=V(e,t),i=V(u(s,et(a(-2,s))),n);return K(i,-1)}))},categoricalCrossentropy:Go,sparseCategoricalCrossentropy:Ho,binaryCrossentropy:Jo,kullbackLeiblerDivergence:function(t,e){return s((()=>{const n=o(t,xr(),1),s=o(e,xr(),1);return r(a(t,j(l(n,s))),-1)}))},poisson:function(t,e){return s((()=>{const n=j(u(xr(),e));return K(V(e,a(t,n)),-1)}))},cosineProximity:Zo};function Xo(t){if("string"==typeof t){if(t in Yo)return Yo[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new Ar(e)}return t}function Qo(t,e){return s((()=>{const n=a(.5,nt(e)),s=Na(st(e,n),t.dtype);return K(it(t,s),-1)}))}function tl(t,e){return s((()=>Na(it(rt(t,-1),rt(e,-1)),"float32")))}function el(t,e){return s((()=>y(r(at(it(t,1),it(e,1))),"float32")))}function nl(t,e){return s((()=>{const n=el(t,e),i=function(t,e){return s((()=>y(r(at(it(t,0),it(e,1))),"float32")))}(t,e),a=u(n,i);return y(ot(st(a,0),l(n,a),0),"float32")}))}function sl(t,e){return s((()=>{const n=el(t,e),i=function(t,e){return s((()=>y(r(at(it(t,1),it(e,0))),"float32")))}(t,e),a=u(n,i);return y(ot(st(a,0),l(n,a),0),"float32")}))}function il(t,e){return Jo(t,e)}function rl(t,e){return t.rank===e.rank&&(t=lt(t,[t.rank-1])),(e=rt(e,-1)).dtype!==t.dtype&&(e=y(e,t.dtype)),y(it(t,e),"float32")}const al=Go,ol=Ho,ll={binaryAccuracy:Qo,categoricalAccuracy:tl,precision:nl,categoricalCrossentropy:al,sparseCategoricalCrossentropy:ol,mse:Vo,MSE:Vo,mae:Ko,MAE:Ko,mape:qo,MAPE:qo,cosine:Zo};function ul(t){if("string"==typeof t&&t in ll)return ll[t];if("string"!=typeof t&&null!=t)return t;throw new Ar(`Unknown metric ${t}`)}function hl(t){if($r(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const n of Object.keys(Yo))if(Yo[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(ll))if(ll[n]===t){e=n;break}return void 0!==e?e:t.name}}function cl(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!pl(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>1048576&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function pl(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!=typeof n)return!1;if(!pl(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!pl(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function dl(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),fl(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?gl(o[t],n,s):ml(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?bo(t.collectedTrainableWeights):bo(t.trainableWeights);return e}(t),u=bo(t.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(e))}function fl(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function gl(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}fl([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function ml(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];fl([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)fl(["","","",r[t]],e,s)}function yl(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function bl(t,e){if(null===t)return null;if("string"==typeof t)return _r(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];yl(e,i,s)?n.push(s):n.push(bl(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=_r(n);e[t]=bl(s,t)}}return e}}function wl(t,e){if(null==t)return null;if("string"==typeof t)return Lr(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];yl(e,i,s)?n.push(s):n.push(wl(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=Lr(n);e[i]="name"!==n&&"className"!==n||"string"!=typeof s?wl(s,n):s}return e}}const kl="3.8.0";class vl{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof vl)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,n){if(null!=this.id2Value[t.id])throw new Ar(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return y(e,t.dtype)}catch(n){throw new Ar(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=n&&(this.id2Mask[t.id]=n),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof xo){if(null==this.id2Value[t.id])throw new Ar(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new Ar(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof xo){if(null==this.id2Value[t.id])throw new Ar(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new Ar(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&W(this.id2Mask)}}const Sl={},xl={};function Nl(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map((t=>t.name)),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==Sl[c]){const t=function(t,n){e.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const e=Al(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=Al(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:Il(i)}}(o,n);p=t.sorted,d=t.recipientCounts,Sl[c]=p,xl[c]=d}p=Sl[c],d={},r||Object.assign(d,xl[c]);const f=new vl(n);for(let t=0;t<p.length;++t){if(null!=i){const t=ht().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const e=p[t],a=e.sourceLayer;if(a instanceof To)continue;const o=[],h=[],c=[];let g=!1;for(const t of e.inputs){const e=f.getValue(t),s=f.getMask(t);o.push(e),h.push(s),null!=s&&(g=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||c.push(e))}g&&((s=s||{}).mask=h[0]);const m=Dr(a.apply(o,s));let y=null;a.supportsMasking&&(y=a.computeMask(o,h));const b=Cl(e),w=Array.isArray(b)?b:[b];for(let t=0;t<w.length;++t){f.hasKey(w[t])||f.add(w[t],m[t],Array.isArray(y)?y[0]:y);const e=l.indexOf(w[t].name);-1!==e&&(u[e]=m[t])}r||W(c)}return f.disposeMasks(),a?u:u[0]}function Il(t){const e={};for(const n in t)e[n]=t[n].size;return e}function Al(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function Cl(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class zl extends Co{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=po(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],Pr(this.inputs).length!==this.inputs.length)throw new Ar(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);Pr(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;$r(0===n,"input layer has >1 nodes"),$r(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof To))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new Ir(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(zl.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort(Wr);this.layers=[];for(const t of d){const e=p[t];e.sort(((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0}));for(const t of e)t instanceof zl&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map((t=>parseInt(t,10))).sort(Wr);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new Ir(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${t.name}". The following previous layers were accessed without issue: ${g}`);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map((t=>t.name));for(const t of m){const e=m.filter((e=>e===t)).length;if(1!==e)throw new Ir(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new Io({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new Ar("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new Ar(`Duplicate weight name: ${e.originalName}`);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new Ar(`Provided weight data has no target variable: ${s}`);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new Ar(`${t.length} of ${s} weights are not set: ${t}`)}vo(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.8.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=wl(this.updatedConfig());return e?JSON.stringify(n):n}call(t,e){return s((()=>{t=Dr(t);const n=new vl;for(let e=0;e<this.inputs.length;++e)n.add(this.inputs[e],t[e]);return Nl(this.outputs,n,e)}))}computeMask(t,e){return s((()=>{let n;return t=Dr(t),n=null==e?Tr(null,t.length):Dr(e),this.runInternalGraph(t,n)[1]}))}computeOutputShape(t){const e=go(t);if(e.length!==this.inputLayers.length)throw new Ar(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(Wr);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=go(e.computeOutputShape(Fr(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];$r(e in n),i.push(n[e])}return Fr(i)}runInternalGraph(t,e){null==e&&(e=Tr(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(Wr);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=Dr(e.call(t,u)),l=Dr(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map((t=>t[0])),a=r.map((t=>t[1])),null==u.mask&&(u.mask=a),o=Dr(e.call(s,u)),l=Dr(e.computeMask(s,a));if(e.activityRegularizer)throw new Cr("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){$r(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof zl?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=zl.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new Ar(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new Ar("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new Ar(`No such layer: ${t}`)}calculateLosses(){return s((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=zl.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=zl.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[zl.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=zl.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=zl.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(Fr(n),s)}function l(t){const n=t.name,r=Uo(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new Ar(`Corrupted configuration, expected array for nodeData: ${t}`);a(r,t)}))}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!Ur(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];$r(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];$r(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new Ar("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){s((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function Tl(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function $l(t,e,n,i){if(null!=e||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=n){const e=s((()=>{if(1===t.shape.length)return ct(t);if(2===t.shape.length){if(t.shape[1]>1){return rt(t,1)}if(1===t.shape[1])return w(t,[t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await e.data());W(e);const r=[];return i.forEach((t=>{if(null==n[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(n[t])})),m(r,"float32")}return null}function El(t,e){return a(t,e)}function Fl(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${n}`));const a=Dl("input",t.inputNames,s),o=Dl("output",t.outputNames,i),l=a[0].shape[0];e.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),e.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let n=0;n<a.length;n++)e.assert(a[n].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let n=0;n<o.length;n++)e.assert(o[n].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function Dl(t,n,s){if(s instanceof pt)return[s];if(Array.isArray(s))return e.assert(s.length===n.length,(()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`)),s;{const e=[];for(const i of n){if(null==s[i])throw new Ar(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function Ll(t,n,s){const i=null!=s.batchesPerEpoch;if(e.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),e.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),e.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),e.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),e.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(_l(s.validationData))e.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new Cr("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const c=Bo(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=Po(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const e={};await d.onEpochBegin(g);let h=0,c=0;for(i||(m=await n.iterator());!i||h<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${h} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:e,ys:i}=Fl(t,n.value),r={};r.batch=c,r.size=e[0].shape[0],await d.onBatchBegin(c,r);const a=[];if(null!=s.classWeight){const e=Tl(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)a.push(await $l(i[t],null,e[t]))}const o=e.concat(i).concat(a),p=l(o);W(o);for(let t=0;t<u.length;++t){const e=u[t],n=p[t];r[e]=n,U(n)}await d.onBatchEnd(c,r),Fo(r),c++,h++}if(i?h>=s.batchesPerEpoch:n.done){if(r){let n;n=_l(s.validationData)?Dr(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):Dr(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)e[`val_${t.metricsNames[s]}`]=n[s]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,e),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function _l(t){return"function"==typeof t.iterator}function Rl(t){e.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function Ml(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>Aa(t,e,n-e))):Aa(t,e,n-e)}function Ol(t,e){return s((()=>null==t?null:Array.isArray(t)?t.map((t=>Ol(t,e))):La(t,"int32"===e.dtype?e:y(e,"int32"))))}function Bl(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function Wl(t,n,i,r={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let a,o,l,u,h,c,p;t.isTraining=!0;try{const d=null==r.batchSize?32:r.batchSize;Rl(d);const f=!1,g=await t.standardizeUserData(n,i,r.sampleWeight,r.classWeight,f,d);a=g[0],o=g[1],p=g[2];let y,b=!1;if(null!=r.validationData&&r.validationData.length>0){if(b=!0,2!==r.validationData.length)throw 3===r.validationData.length?new Cr("validationData including sample weights is not supported yet."):new Ar(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${r.validationData} is invalid.`);l=r.validationData[0],u=r.validationData[1];const e=!0,n=await t.standardizeUserData(l,u,null,null,e,d);h=n[0],c=n[1],y=h.concat(c)}else if(null!=r.validationSplit&&r.validationSplit>0&&r.validationSplit<1){b=!0;const t=Math.floor(a[0].shape[0]*(1-r.validationSplit)),e=a[0].shape[0];h=Ml(a,t,e),a=Ml(a,0,t),c=Ml(o,t,e),o=Ml(o,0,t),y=h.concat(c)}else null!=r.validationSteps&&(b=!0);const w=a.concat(o).concat(p);t.checkTrainableWeightsConsistency();const k=t.makeTrainFunction(),v=t.getDedupedMetricsNames();let S,x;b?(t.makeTestFunction(),S=t.testFunction,x=v.slice().concat(v.map((t=>"val_"+t)))):(S=null,y=[],x=v.slice());const N=Bo(r.callbacks,r.yieldEvery);return await async function(t,n,i,r,a,o,l,u,h,c,p,d,f,g,y){null==a&&(a=32),null==o&&(o=1),null==p&&(p=!0),null==f&&(f=0);let b=!1;if(null!=h&&null!=c&&(b=!0),null!=y&&(b=!0,null==g))throw new Ar("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const w=t.checkNumSamples(i,a,g,"steps_per_epoch");let k;null!=w&&(k=xa(0,w)),null==l&&(l=1);const{callbackList:v,history:S}=Po(u,l,o,f,w,g,a,b,d);v.setModel(t),t.history=S,await v.onTrainBegin(),t.stopTraining_=!1;for(let l=f;l<o;++l){await v.onEpochBegin(l);const o={};if(null!=g)throw new Cr("stepsPerEpoch mode is not implemented yet.");{if("batch"===p)throw new Cr("batch shuffling is not implemneted yet");p&&e.shuffle(k);const l=m(k),u=Bl(w,a);for(let e=0;e<u.length;++e){const p={};if(await v.onBatchBegin(e,p),s((()=>{const s=u[e][0],d=u[e][1],f=Aa(l,s,d-s);p.batch=e,p.size=d-s;const g=Ol(i,f),m=n(g);for(let t=0;t<r.length;++t){const e=r[t],n=m[t];p[e]=n,U(n)}if(e===u.length-1&&b){const e=t.testLoop(h,c,a);for(let t=0;t<r.length;++t){const n=r[t],s=e[t];U(s),o["val_"+n]=s}}})),await v.onBatchEnd(e,p),Fo(p),t.stopTraining_)break}l.dispose()}if(await v.onEpochEnd(l,o),t.stopTraining_)break}return await v.onTrainEnd(),await t.history.syncData(),t.history}(t,k,w,v,d,r.epochs,r.verbose,N,S,y,r.shuffle,x,r.initialEpoch,null,null)}finally{t.isTraining=!1,Ul(a,n),Ul(o,i),Ul(h,l),Ul(c,u),null!=p&&W(p)}}function Pl(t){const e=[];t instanceof pt&&(t=[t]);for(let n=0;n<t.length;++n){const s=t[n];if(1===s.rank)e.push(Ia(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(s)}}return e}function Ul(t,e){if(null==t)return;const n=[];if(e instanceof pt)n.push(e.id);else if(Array.isArray(e))e.forEach((t=>n.push(t.id)));else if(null!=e)for(const t in e){const s=e[t];n.push(s.id)}const s=[];if(t instanceof pt)-1===n.indexOf(t.id)&&s.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===n.indexOf(t.id)&&s.push(t)}));else if(null!=t)for(const e in t){const i=t[e];-1===n.indexOf(i.id)&&s.push(i)}s.forEach((t=>{t.isDisposed||t.dispose()}))}function jl(t){return Array.isArray(t)}function Vl(t){return!function(t){return t instanceof pt}(t)&&!jl(t)}function Kl(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(jl(t)&&t.length>0)e=!0;else if(Vl(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new Ar(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let r;if(Vl(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new Ar(`No data provided for "${n}". Need data for each key in: ${e}`);r.push(t[n])}}else if(jl(t)){if((t=t).length!==e.length)throw new Ar(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);r=t}else{if(t=t,e.length>1)throw new Ar(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);r=[t]}if(r=Pl(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new Ar(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape ${a.shape}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new Ar(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function ql(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new Ar(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new Ar(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new Ar(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape ${JSON.stringify(a.shape)}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new Ar(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class Gl extends zl{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new Ar("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");dl(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const e={Adagrad:()=>ut.adagrad(.01),Adadelta:()=>ut.adadelta(1,.95,xr()),Adam:()=>ut.adam(.001,.9,.999,xr()),Adamax:()=>ut.adamax(.002,.9,.999,xr(),0),RMSProp:()=>ut.rmsprop(.001,.9,0,xr()),SGD:()=>ut.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new Ar(`Unknown Optimizer ${t}`)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof dt))throw new Ar("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new Ar(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const n=t.loss;e=n.map((t=>Xo(t)))}else{const n=Xo(t.loss);this.outputs.forEach((t=>{e.push(n)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new Ar(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const n of this.outputNames)null==t.loss[n]&&console.warn(`Output "${n}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${n} during training`),e.push(Xo(t.loss[n]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const n=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],ga("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const s=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),i=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};ga("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;(e=>{let n,s,r;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let i;1===e[e.length-1]||this.lossFunctions[t]===Jo?-1!==["accuracy","acc"].indexOf(a)?s=Qo:-1!==["crossentropy","ce"].indexOf(a)&&(s=il):this.lossFunctions[t]===Ho?-1!==["accuracy","acc"].indexOf(a)?s=rl:-1!==["crossentropy","ce"].indexOf(a)&&(s=ol):-1!==["accuracy","acc"].indexOf(a)?s=tl:-1!==["crossentropy","ce"].indexOf(a)&&(s=al),-1!==["accuracy","acc"].indexOf(a)?i="acc":-1!==["crossentropy","ce"].indexOf(a)&&(i="ce"),r=s,n=""+i}else{const t=ul(a);r=t,n=""+hl(a)}let e;ga(n,(()=>{e=r})),i(t,n,e)}})(s[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;Rl(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return Fr(this.testLoop(a,r,s,n.verbose,n.steps))}finally{Ul(i[0],t),Ul(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,i){const r=null!=(i=i||{}).batches,o=t.testFunction;let h=[];if(i.verbose>0)throw new Cr("Verbose mode is not implemented yet.");e.assert(!r||i.batches>0&&Number.isInteger(i.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(i.batches)}`));const c="function"==typeof n.next?n:await n.iterator();let p=0,d=0;for(;!r||d<i.batches;){const e=await c.next();if(h=s((()=>{if(e.value){const{xs:n,ys:i}=Fl(t,e.value),r=n.concat(i),l=s((()=>o(r)));if(W(r),0===d)for(let t=0;t<l.length;++t)h.push(L(0));const c=r[0].shape[0];for(let t=0;t<l.length;++t){const e=l[t],n=h[t];h[t]=s((()=>u(h[t],a(c,e)))),d>0&&W(n)}W(l),p+=c,++d}return h})),e.done){r&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${i.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<h.length;++t){const e=h[t];h[t]=l(h[t],p),W(e)}return Fr(h)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new Ar(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new Ar(`Either the input data should have a defined shape, or ${s} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new Ar("`outputs` is an empty Array, which is not allowed.");const n=Array.isArray(e),s=n?e:[e],i=this.retrieveSymbolicTensors(s),r=new vl;if(t instanceof pt&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new Ar(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new Ar(`No value is provided for the model's input ${e.name}`);r.add(e,n)}const a=Nl(i,r);return n?a:a[0]}retrieveSymbolicTensors(t){const e=Tr(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map((t=>t.name));for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new Ar(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return e}predictLoop(t,e=32,n=!1){return s((()=>{const i=this.checkNumSamples(t);if(n)throw new Cr("Verbose predictLoop() is not implemented yet.");const r=Bl(i,e),a=this.outputs.map((t=>[]));for(let e=0;e<r.length;++e){s((()=>{const n=r[e][0],s=r[e][1],i=Ml(t,n,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new vl(a);return Nl(this.outputs,o)})).forEach(((t,e)=>a[e].push(t)))}return Fr(a.map((t=>I(t,0))))}))}predict(t,e={}){const n=Pl(t);ql(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return Rl(s),this.predictLoop(n,s)}finally{Ul(n,t)}}predictOnBatch(t){ql(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new Ir("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===Ho?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=Pr(t.map((t=>t.shape[0])));i.sort();const r=Pr(n.map((t=>t.shape[0])));if(r.sort(),i.length>1)throw new Ar(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(r.length>1)throw new Ar(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(n.map((t=>t.shape)))}`);if(i.length>0&&r.length>0&&!e.arraysEqual(i,r))throw new Ar(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=Kl(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=Kl(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[Vo,Jo,Go];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===Go&&1===r.shape[r.shape.length-1])throw new Ar(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new Ar(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new Ar(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=Tl(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await $l(o[e],null,t[e]))}return[a,o,l]}testLoop(t,e,n,i=0,r){return s((()=>{const s=this.checkNumSamples(e,n,r,"steps"),o=[];if(i>0)throw new Cr("Verbose mode is not implemented yet.");if(null!=r)throw new Cr("steps mode in testLoop() is not implemented yet");{const i=Bl(s,n),r=m(xa(0,s));for(let n=0;n<i.length;++n){const s=i[n][0],l=i[n][1],h=Aa(r,s,l-s),c=Ol(e,h),p=t(c);if(0===n)for(let t=0;t<p.length;++t)o.push(L(0));for(let t=0;t<p.length;++t){const e=p[t];o[t]=u(o[t],a(l-s,e))}}for(let t=0;t<o.length;++t)o[t]=l(o[t],s)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(Er(t,s)>1){i+=`_${Er(t.slice(0,n),s)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],n=t.slice(0,this.inputs.length),s=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),i=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),r=[],a=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:n[e]});const a=new vl(t),o=Nl(this.outputs,a,{training:!0});let l;for(let t=0;t<this.lossFunctions.length;++t){let n=(0,this.lossFunctions[t])(s[t],o[t]);null!=i[t]&&(n=El(n,i[t]));const r=K(n);e.push(r),l=0===t?n:u(l,n)}for(let t=0;t<this.metricsTensors.length;++t){let n;if(this.outputs.length>1&&t<this.outputs.length)n=e[t];else{const e=this.metricsTensors[t][0],i=this.metricsTensors[t][1];n=K(e(s[i],o[i]))}U(n),r.push(n)}return l=K(l),this.calculateLosses().forEach((t=>{l=u(l,t)})),l}),!0,a)].concat(r)}}makeTestFunction(){this.testFunction=t=>s((()=>{const e=[];let n;const s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=[];for(let t=0;t<this.inputs.length;++t)r.push({key:this.inputs[t],value:s[t]});const a=new vl(r),o=Nl(this.outputs,a);for(let t=0;t<this.lossFunctions.length;++t){const s=this.lossFunctions[t],r=K(s(i[t],o[t]));n=0===t?r:u(n,r),e.push(n)}for(let t=0;t<this.metricsTensors.length;++t){const n=this.metricsTensors[t][0],s=this.metricsTensors[t][1],r=K(n(i[s],o[s]));e.push(r)}return e}))}async fit(t,e,n={}){return Wl(this,t,e,n)}async fitDataset(t,e){return Ll(this,t,e)}async trainOnBatch(t,e){const n=await this.standardizeUserData(t,e),s=n[0],i=n[1],r=this.makeTrainFunction()(s.concat(i)),a=[];for(const t of r){const e=await t.data();a.push(e[0])}return W(r),Fr(a)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=ht().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-ht().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=Lr(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>Lr(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=Lr(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[Lr(hl(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>Lr(hl(t))));{const t={};for(const e in this.metrics)t[e]=Lr(hl(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=Uo(bl(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=_r(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>_r(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=_r(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>_r(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=_r(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,e){if("string"==typeof t){const e=ft.getSaveHandlers(t);if(0===e.length)throw new Ar(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new Ar(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new Ar("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const n=await ft.encodeWeights(this.getNamedWeights(e)),s={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.8.0",convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){s.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:i}=await ft.encodeWeights(await this.optimizer.getWeights(),t);n.specs.push(...i),n.data=ft.concatenateArrayBuffers([n.data,e])}if(null!=this.userDefinedMetadata){const t=!0;cl(this.userDefinedMetadata,this.name,t),s.userDefinedMetadata=this.userDefinedMetadata}return s.weightData=n.data,s.weightSpecs=n.specs,t.save(s)}setUserDefinedMetadata(t){cl(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}Gl.className="Model",n.registerClass(Gl);class Hl extends Gl{}async function Jl(t,e){if(null==e&&(e={}),"string"==typeof t){const n=ft.getLoadHandlers(t,e);if(0===n.length)n.push(ft.browserHTTPRequest(t,e));else if(n.length>1)throw new Ar(`Found more than one (${n.length}) load handlers for URL '${t}'`);t=n[0]}return async function(t,e,n){null==n&&(n={});if(null==t.load)throw new Ar("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const s=await t.load();let i=s.modelTopology;null!=i.model_config&&(i=i.model_config);const r=null==n.strict||n.strict,a=null!=s.weightData&&null!=s.weightSpecs&&r,o=Uo(bl(i),e,a),l=s.trainingConfig;null!=l&&o.loadTrainingConfig(l);null!=s.userDefinedMetadata&&o.setUserDefinedMetadata(s.userDefinedMetadata);if(null!=s.weightData){if(null==s.weightSpecs)throw new Ar("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const n=ft.decodeWeights(t,e),s={},i=[];return e.forEach((t=>{"optimizer"===t.group?i.push({name:t.name,tensor:n[t.name]}):s[t.name]=n[t.name]})),{modelWeights:s,optimizerWeights:i}}(s.weightData,s.weightSpecs);o.loadWeights(t,r),null!=o.optimizer&&e.length>0&&await o.optimizer.setWeights(e),W(t),W(e.map((t=>t.tensor)))}return o}(t,void 0,e)}Hl.className="Functional",n.registerClass(Hl);class Zl extends Gl{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:po("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new Ar(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof Zl||t instanceof Gl;let n;if(e){if(n=t,1!==n.outputs.length)throw new Ar("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new Ar("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new Ar("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=$o({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new Ar(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new Ar("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=zo(this.outputs[0])}this.inboundNodes=[],new Io({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:Tr(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(yo(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new Gl({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new Ir("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new Ir("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new Ir("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new Ir("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new Ar("Legacy serialization format not supported yet.");r=n}else e.assert(null!=n.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof Zl))throw new Cr(`Sequential.fromConfig called on non-Sequential input: ${o}`);for(const t of r){const e=Uo(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new Ar("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new Ar("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function Yl(t){return new Gl(t)}function Xl(t){return new Zl(t)}function Ql(t,e){return null==e&&(e={}),Jl(t,e)}function tu(t){return $o(t)}function eu(t,e){Wo.registerCallbackConstructor(t,e)}Zl.className="Sequential",n.registerClass(Zl);class nu extends n.Serializable{getConfig(){return{}}}class su extends nu{apply(t,e=1){return function(t,e=1){if(1!==e)throw new Cr(`Support for alpha values other than 1 (${e}) is not implemented yet.`);return S(t)}(t,e)}}su.className="elu",n.registerClass(su);class iu extends nu{apply(t){return gt(t)}}iu.className="selu",n.registerClass(iu);class ru extends nu{apply(t){return h(t)}}ru.className="relu",n.registerClass(ru);class au extends nu{apply(t){return s((()=>mt(6,h(t))))}}au.className="relu6",n.registerClass(au);class ou extends nu{apply(t){return t}}ou.className="linear",n.registerClass(ou);class lu extends nu{apply(t){return yt(t)}}lu.className="sigmoid",n.registerClass(lu);class uu extends nu{apply(t){return function(t){return s((()=>{const e=u(.5,a(.2,t));return o(e,0,1)}))}(t)}}uu.className="hardSigmoid",n.registerClass(uu);class hu extends nu{apply(t){return et(t)}}hu.className="softplus",n.registerClass(hu);class cu extends nu{apply(t){return function(t){return s((()=>l(t,u(x(t),1))))}(t)}}cu.className="softsign",n.registerClass(cu);class pu extends nu{apply(t){return bt(t)}}pu.className="tanh",n.registerClass(pu);class du extends nu{apply(t,e=-1){return J(t,e)}}du.className="softmax",n.registerClass(du);class fu extends nu{apply(t,e=-1){return wt(t,e)}}fu.className="logSoftmax",n.registerClass(fu);class gu extends nu{apply(t,e=1){return s((()=>a(yt(a(t,e)),t)))}}gu.className="swish",n.registerClass(gu);class mu extends nu{apply(t){return s((()=>a(t,bt(et(t)))))}}function yu(t){return t.getClassName()}function bu(t,e={}){return Br(t,n.SerializationMap.getMap().classNameMap,e,"activation")}function wu(t){if(null==t){const t={className:"linear",config:{}};return bu(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},bu(e)}return t instanceof nu?t:bu(t)}function ku(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}mu.className="mish",n.registerClass(mu);class vu extends n.Serializable{}class Su extends vu{constructor(t){super(),ku(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return s((()=>{let e=F([1]);return this.hasL1&&(e=u(e,r(a(this.l1,x(t))))),this.hasL2&&(e=u(e,r(a(this.l2,_a(t))))),w(e,[])}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}Su.className="L1L2",n.registerClass(Su);const xu={l1l2:"L1L2"};function Nu(t){return Mr(t)}function Iu(t,e={}){return Br(t,n.SerializationMap.getMap().classNameMap,e,"regularizer")}function Au(t){if(null==t)return null;if("string"==typeof t){return Iu({className:t in xu?xu[t]:t,config:{}})}return t instanceof vu?t:Iu(t)}class Cu extends Co{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=mo(t);let n=h(t);return null!=this.maxValue&&(n=o(n,0,this.maxValue)),n}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}Cu.className="ReLU",n.registerClass(Cu);class zu extends Co{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=mo(t);return kt(n,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}zu.className="LeakyReLU",n.registerClass(zu);class Tu extends Co{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=oo(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=Au(t.alphaRegularizer),this.alphaConstraint=sa(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new Ar(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=yo(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new So({ndim:t.length,axes:n})],this.built=!0}call(t,e){return t=mo(t),vt(t,this.alpha.read())}getConfig(){const t={alphaInitializer:ao(this.alphaInitializer),alphaRegularizer:Nu(this.alphaRegularizer),alphaConstraint:ea(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}Tu.className="PReLU",n.registerClass(Tu);class $u extends Co{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new Cr(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=mo(t);return S(n)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}$u.className="ELU",n.registerClass($u);class Eu extends Co{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=mo(t);return a(n,y(st(n,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}Eu.className="ThresholdedReLU",n.registerClass(Eu);class Fu extends Co{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new du).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=mo(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Du(t,e,n){if("number"==typeof t)return Tr(t,e);if(t.length!==e)throw new Ar(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new Ar(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${r}`)}return t;var s}function Lu(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function _u(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+Sa([n-e,0]);else{if("same"!==s)throw new Ar(`Unsupport padding mode: ${s}.`);t*=e}return t}function Ru(t,e){return s((()=>(ca(e),"channelsFirst"===e?C(t,[0,2,3,1]):t)))}function Mu(t,e){return s((()=>(ca(e),"channelsFirst"===e?C(t,[0,2,3,4,1]):t)))}function Ou(t,e,n,i=[1,1],r="valid",a,o,l=null){return s((()=>{if(null==a&&(a="channelsLast"),ca(a),3!==t.rank&&4!==t.rank)throw new Ar(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==e.rank&&4!==e.rank)throw new Ar(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let s=Ru(t,a);if("causal"===r)throw new Cr("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return s=A.conv2d({x:s,filter:e,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:n,activation:l}),"channelsFirst"===a&&(s=C(s,[0,3,1,2])),s}))}Fu.className="Softmax",n.registerClass(Fu);class Bu extends Co{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",Bu.verifyArgs(e),this.rank=t,Kr(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new Cr(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=Du(e.kernelSize,t,"kernelSize"),this.strides=Du(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,pa(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,ca(this.dataFormat),this.activation=wu(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=oo(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=sa(e.biasConstraint),this.biasRegularizer=Au(e.biasRegularizer),this.activityRegularizer=Au(e.activityRegularizer),this.dilationRate=Du(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new Ar(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new Ar(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new Ar(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if($r("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!Vr(t.kernelSize,"number",1,3))throw new Ar(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:yu(this.activation),useBias:this.useBias,biasInitializer:ao(this.biasInitializer),biasRegularizer:Nu(this.biasRegularizer),activityRegularizer:Nu(this.activityRegularizer),biasConstraint:ea(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class Wu extends Bu{constructor(t,e){super(t,e),this.kernel=null,Wu.verifyArgs(e),this.filters=e.filters,Kr(this.filters,"filters"),this.kernelInitializer=oo(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=sa(e.kernelConstraint),this.kernelRegularizer=Au(e.kernelRegularizer)}build(t){t=yo(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Ar(`The channel dimension of the input should be defined. Found ${t[e]}`);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,e){return s((()=>{let e;t=mo(t);const n=null==this.bias?null:this.bias.read(),i=Gr(this.activation.getClassName());if(null!=i&&2===this.rank)e=Ou(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)e=function(t,e,n,i=1,r="valid",a,o=1){return s((()=>{if(null==a&&(a="channelsLast"),ca(a),3!==t.shape.length)throw new Ar(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==e.shape.length)throw new Ar(`The kernel for a conv1dWithBias operation should be 3, but is ${e.shape.length} instead`);if(null!=n&&1!==n.shape.length)throw new Ar(`The bias for a conv1dWithBias operation should be 1, but is ${e.shape.length} instead`);if("channelsFirst"===a&&(t=C(t,[0,2,1])),"causal"===r)throw new Cr("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let s=It(t,e,i,"same"===r?"same":"valid","NWC",o);return null!=n&&(s=Ma(s,n)),s}))}(t,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=Ou(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new Cr("convolutions greater than 3D are not implemented yet.");e=function(t,e,n,i=[1,1,1],r="valid",a,o){return s((()=>{if(null==a&&(a="channelsLast"),ca(a),4!==t.rank&&5!==t.rank)throw new Ar(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==e.rank&&5!==e.rank)throw new Ar(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let s=Mu(t,a);if("causal"===r)throw new Cr("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return s=At(s,e,i,"same"===r?"same":"valid","NDHWC",o),null!=n&&(s=Ma(s,n)),"channelsFirst"===a&&(s=C(s,[0,4,1,2,3])),s}))}(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e}))}computeOutputShape(t){t=yo(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=Lu(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:ao(this.kernelInitializer),kernelRegularizer:Nu(this.kernelRegularizer),kernelConstraint:ea(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new Ar(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class Pu extends Wu{constructor(t){super(2,t),Pu.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Vr(t.kernelSize,"number",1,2))throw new Ar(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}Pu.className="Conv2D",n.registerClass(Pu);class Uu extends Wu{constructor(t){super(3,t),Uu.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new Ar(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}Uu.className="Conv3D",n.registerClass(Uu);class ju extends Pu{constructor(t){if(super(t),this.inputSpec=[new So({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new Ar(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=yo(t)).length)throw new Ar("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Ar("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new So({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e=mo(t);if(4!==e.shape.length)throw new Ar(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const n=e.shape,s=n[0];let i,r;"channelsFirst"===this.dataFormat?(i=2,r=3):(i=1,r=2);const a=n[i],o=n[r],l=this.kernelSize[0],u=this.kernelSize[1],h=this.strides[0],c=this.strides[1],p=[s,_u(a,h,l,this.padding),_u(o,c,u,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=C(e,[0,2,3,1]));let d=St(e,this.kernel.read(),p,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(d=C(d,[0,3,1,2])),null!=this.bias&&(d=Ma(d,this.bias.read(),this.dataFormat)),null!=this.activation&&(d=this.activation.apply(d)),d}))}computeOutputShape(t){const e=(t=yo(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=_u(e[s],o,r,this.padding),e[i]=_u(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}ju.className="Conv2DTranspose",n.registerClass(ju);class Vu extends Uu{constructor(t){if(super(t),this.inputSpec=[new So({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new Ar(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=yo(t)).length)throw new Ar("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Ar("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new So({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e=mo(t);if(5!==e.shape.length)throw new Ar(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const n=e.shape,s=n[0];let i,r,a;"channelsFirst"===this.dataFormat?(a=2,i=3,r=4):(a=1,i=2,r=3);const o=n[a],l=n[i],u=n[r],h=this.kernelSize[0],c=this.kernelSize[1],p=this.kernelSize[2],d=this.strides[0],f=this.strides[1],g=this.strides[2],m=[s,_u(o,d,h,this.padding),_u(l,f,c,this.padding),_u(u,g,p,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=C(e,[0,2,3,4,1]));let y=xt(e,this.kernel.read(),m,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(y=C(y,[0,4,1,2,3])),null!==this.bias&&(y=Ma(y,this.bias.read(),this.dataFormat)),null!==this.activation&&(y=this.activation.apply(y)),y}))}computeOutputShape(t){const e=(t=yo(t)).slice();let n,s,i,r;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,r=4):(n=4,s=1,i=2,r=3);const a=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[n]=this.filters,e[s]=_u(e[s],u,a,this.padding),e[i]=_u(e[i],h,o,this.padding),e[r]=_u(e[r],c,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Vu.className="Conv3DTranspose",n.registerClass(Vu);class Ku extends Wu{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new Ar("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new Ar("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new Ar(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=oo(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=Au(e.depthwiseRegularizer),this.depthwiseConstraint=sa(e.depthwiseConstraint),this.pointwiseInitializer=oo(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=Au(e.pointwiseRegularizer),this.pointwiseConstraint=sa(e.pointwiseConstraint)}build(t){if((t=yo(t)).length<this.rank+2)throw new Ar(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new Ar(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new So({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e;if(t=mo(t),1===this.rank)throw new Cr("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=C(t,[0,2,3,1])),e=Ct(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=Ma(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=C(e,[0,3,1,2])),e}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=ao(this.depthwiseInitializer),t.pointwiseInitializer=ao(this.pointwiseInitializer),t.depthwiseRegularizer=Nu(this.depthwiseRegularizer),t.pointwiseRegularizer=Nu(this.pointwiseRegularizer),t.depthwiseConstraint=ea(this.depthwiseConstraint),t.pointwiseConstraint=ea(this.pointwiseConstraint),t}}Ku.className="SeparableConv";class qu extends Ku{constructor(t){super(2,t)}}qu.className="SeparableConv2D",n.registerClass(qu);class Gu extends Wu{constructor(t){super(1,t),Gu.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Vr(t.kernelSize,"number",1,1))throw new Ar(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}Gu.className="Conv1D",n.registerClass(Gu);class Hu extends Co{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return s((()=>{if(t=mo(t),"channelsLast"===this.dataFormat){const e=za(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return za(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=za(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return za(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Hu.className="Cropping2D",n.registerClass(Hu);class Ju extends Co{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ca(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,jr(aa,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,e){return s((()=>{let e=mo(t);const n=e.shape;if("channelsFirst"===this.dataFormat){e=C(e,[0,2,3,1]);const t=this.size[0]*n[2],s=this.size[1]*n[3],i="nearest"===this.interpolation?Nt.resizeNearestNeighbor(e,[t,s]):Nt.resizeBilinear(e,[t,s]);return C(i,[0,3,1,2])}{const t=this.size[0]*n[1],s=this.size[1]*n[2];return"nearest"===this.interpolation?Nt.resizeNearestNeighbor(e,[t,s]):Nt.resizeBilinear(e,[t,s])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Ju.className="UpSampling2D",n.registerClass(Ju);class Zu extends Bu{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=oo(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=sa(t.depthwiseConstraint),this.depthwiseRegularizer=Au(t.depthwiseRegularizer)}build(t){if((t=yo(t)).length<4)throw new Ar(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new Ar(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{let e=function(t,e,n=[1,1],i="valid",r,a){return s((()=>{null==r&&(r="channelsLast"),ca(r);let s=Ru(t,r);if(4!==t.rank)throw new Ar(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==e.rank)throw new Ar(`depthwiseKernel is required to be 4-D, but is instead ${e.rank}-D`);return s=zt(s,e,n,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(s=C(s,[0,3,1,2])),s}))}(t=mo(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=Ma(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e}))}computeOutputShape(t){t=yo(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=Lu(e,this.kernelSize[0],this.padding,this.strides[0]),r=Lu(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=ao(this.depthwiseInitializer),t.depthwiseRegularizer=Nu(this.depthwiseRegularizer),t.depthwiseConstraint=ea(this.depthwiseRegularizer),t}}function Yu(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new Ar("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function Xu(t,e,n,i=!1,r,o,l=!1,h=!1){return s((()=>{const c=e.shape.length;if(c<3)throw new Ar(`Input should be at least 3D, but is ${c}D.`);const p=[1,0].concat(xa(2,c));if(e=C(e,p),null!=o)throw new Cr("The rnn() functoin of the deeplearn.js backend does not support constants yet.");l&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=y(y(r,"bool"),"float32")).rank===c-1&&(r=Tt(r,-1)),r=C(r,p)),i&&(e=$t(e,0),null!=r&&(r=$t(r,0)));const d=[];let f,g=n;const m=e.shape[0],b=Et(e);let w,k;null!=r&&(w=Et(r));for(let e=0;e<m;++e){const n=b[e],i=s((()=>t(n,g)));if(null==r)f=i[0],g=i[1];else{const t=s((()=>{const t=w[e],n=V(nt(t),t);return{output:u(a(i[0],t),a(g[0],n)),newStates:g.map(((e,s)=>u(a(i[1][s],t),a(e,n))))}}));f=t.output,g=t.newStates}h&&d.push(f)}if(h){k=Ft(d,1)}return[f,k,g]}))}Zu.className="DepthwiseConv2D",n.registerClass(Zu);class Qu extends Co{constructor(t){let e;if(super(t),null==t.cell)throw new Ar("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new oh({cells:t.cell}):t.cell,null==e.stateSize)throw new Ar("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new So({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return xa(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){fo(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,e){return s((()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new Cr("Constants support is not implemented in RNN yet.");fo(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new So({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new Ar(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new So({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,n=!1){s((()=>{if(!this.stateful)throw new Nr("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new Ar("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>F([s,t]))):this.states_=[F([s,this.cell.stateSize])];else if(null==t)W(this.states_),null!=this.keptStates&&(W(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>F([s,t]))):this.states_[0]=F([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Ar(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===n?this.keptStates.push(this.states_.slice()):W(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.arraysEqual(i.shape,a))throw new Ar(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map((t=>U(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Yu(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new So({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof xo){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s((()=>{const n=null==e?null:e.mask,s=null==e?null:e.training;let i=null==e?null:e.initialState;t=mo(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new Ar(`RNN Layer has ${r} state(s) but was passed ${i.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=Xu(((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]}),t,i,this.goBackwards,n,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c}))}getInitialState(t){return s((()=>{let e=F(t.shape);return e=r(e,[1,2]),e=Ia(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?Ea(e,[1,t]):e)):this.cell.stateSize>1?[Ea(e,[1,this.cell.stateSize])]:[e]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===Qu.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=Uo(e.cell,n);return new t(Object.assign(e,{cell:s}))}}Qu.className="RNN",n.registerClass(Qu);class th extends Co{}class eh extends th{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Kr(this.units,"units"),this.activation=wu(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=oo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=oo(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=oo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Au(t.kernelRegularizer),this.recurrentRegularizer=Au(t.recurrentRegularizer),this.biasRegularizer=Au(t.biasRegularizer),this.kernelConstraint=sa(t.kernelConstraint),this.recurrentConstraint=sa(t.recurrentConstraint),this.biasConstraint=sa(t.biasConstraint),this.dropout=va([1,Sa([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=va([1,Sa([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=yo(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{if(2!==(t=t).length)throw new Ar(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let n=t[1];t=t[0];const s=null!=e.training&&e.training;let i;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=lh({ones:()=>nt(t),rate:this.dropout,training:s})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=lh({ones:()=>nt(n),rate:this.recurrentDropout,training:s}));const r=this.dropoutMask,o=this.recurrentDropoutMask;i=Da(null!=r?a(t,r):t,this.kernel.read()),null!=this.bias&&(i=Ma(i,this.bias.read())),null!=o&&(n=a(n,o));let l=u(i,Da(n,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:yu(this.activation),useBias:this.useBias,kernelInitializer:ao(this.kernelInitializer),recurrentInitializer:ao(this.recurrentInitializer),biasInitializer:ao(this.biasInitializer),kernelRegularizer:Nu(this.kernelRegularizer),recurrentRegularizer:Nu(this.recurrentRegularizer),biasRegularizer:Nu(this.biasRegularizer),activityRegularizer:Nu(this.activityRegularizer),kernelConstraint:ea(this.kernelConstraint),recurrentConstraint:ea(this.recurrentConstraint),biasConstraint:ea(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}eh.className="SimpleRNNCell",n.registerClass(eh);class nh extends Qu{constructor(t){t.cell=new eh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(W(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(W(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return new t(e)}}nh.className="SimpleRNN",n.registerClass(nh);class sh extends th{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new Ar("GRUCell does not support reset_after parameter set to true.");this.units=t.units,Kr(this.units,"units"),this.activation=wu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=wu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=oo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=oo(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=oo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Au(t.kernelRegularizer),this.recurrentRegularizer=Au(t.recurrentRegularizer),this.biasRegularizer=Au(t.biasRegularizer),this.kernelConstraint=sa(t.kernelConstraint),this.recurrentConstraint=sa(t.recurrentConstraint),this.biasConstraint=sa(t.biasConstraint),this.dropout=va([1,Sa([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=va([1,Sa([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=yo(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{if(2!==(t=t).length)throw new Ar(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const n=null!=e.training&&e.training;let s=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=lh({ones:()=>nt(t),rate:this.dropout,training:n,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=lh({ones:()=>nt(s),rate:this.recurrentDropout,training:n,count:3}));const i=this.dropoutMask,r=this.recurrentDropoutMask;let o,l,h;0<this.dropout&&this.dropout<1&&(t=a(t,i[0]));let c=Da(t,this.kernel.read());this.useBias&&(c=Ma(c,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,r[0]));const p=this.recurrentKernel.read(),[d,f]=Dt(p,[2*this.units,this.units],p.rank-1),g=Da(s,d),[m,y,b]=Dt(c,3,c.rank-1),[w,k]=Dt(g,2,g.rank-1);o=this.recurrentActivation.apply(u(m,w)),l=this.recurrentActivation.apply(u(y,k));const v=Da(a(l,s),f);h=this.activation.apply(u(b,v));const S=u(a(o,s),a(u(1,q(o)),h));return[S,S]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:yu(this.activation),recurrentActivation:yu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:ao(this.kernelInitializer),recurrentInitializer:ao(this.recurrentInitializer),biasInitializer:ao(this.biasInitializer),kernelRegularizer:Nu(this.kernelRegularizer),recurrentRegularizer:Nu(this.recurrentRegularizer),biasRegularizer:Nu(this.biasRegularizer),activityRegularizer:Nu(this.activityRegularizer),kernelConstraint:ea(this.kernelConstraint),recurrentConstraint:ea(this.recurrentConstraint),biasConstraint:ea(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}sh.className="GRUCell",n.registerClass(sh);class ih extends Qu{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new sh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(W(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(W(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}ih.className="GRU",n.registerClass(ih);class rh extends th{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Kr(this.units,"units"),this.activation=wu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=wu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=oo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=oo(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=oo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=Au(t.kernelRegularizer),this.recurrentRegularizer=Au(t.recurrentRegularizer),this.biasRegularizer=Au(t.biasRegularizer),this.kernelConstraint=sa(t.kernelConstraint),this.recurrentConstraint=sa(t.recurrentConstraint),this.biasConstraint=sa(t.biasConstraint),this.dropout=va([1,Sa([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=va([1,Sa([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=yo(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends Ua{apply(e,s){const i=t.apply([n]),r=(new Va).apply([n]),a=t.apply([2*n]);return $a($a(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return s((()=>{const n=null!=e.training&&e.training;if(3!==(t=t).length)throw new Ar(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let s=t[1];const i=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=lh({ones:()=>nt(t),rate:this.dropout,training:n,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=lh({ones:()=>nt(s),rate:this.recurrentDropout,training:n,count:4}));const r=this.dropoutMask,o=this.recurrentDropoutMask;let l,h,c,p;0<this.dropout&&this.dropout<1&&(t=a(t,r[0]));let d=Da(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,o[0])),d=u(d,Da(s,this.recurrentKernel.read())),this.useBias&&(d=Ma(d,this.bias.read()));const[f,g,m,y]=Dt(d,4,d.rank-1);l=this.recurrentActivation.apply(f),h=this.recurrentActivation.apply(g),c=u(a(h,i),a(l,this.activation.apply(m))),p=this.recurrentActivation.apply(y);const b=a(p,this.activation.apply(c));return[b,b,c]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:yu(this.activation),recurrentActivation:yu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:ao(this.kernelInitializer),recurrentInitializer:ao(this.recurrentInitializer),biasInitializer:ao(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:Nu(this.kernelRegularizer),recurrentRegularizer:Nu(this.recurrentRegularizer),biasRegularizer:Nu(this.biasRegularizer),activityRegularizer:Nu(this.activityRegularizer),kernelConstraint:ea(this.kernelConstraint),recurrentConstraint:ea(this.recurrentConstraint),biasConstraint:ea(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}rh.className="LSTMCell",n.registerClass(rh);class ah extends Qu{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new rh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(W(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(W(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}ah.className="LSTM",n.registerClass(ah);class oh extends th{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return s((()=>{let n=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(n.splice(0,t.stateSize.length)):s.push(n.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];n=s[a],r=0===a?[t[0]].concat(n):[r[0]].concat(n),r=o.call(r,e),i.push(r.slice(1))}n=[];for(const t of i.slice().reverse())n.push(...t);return[r[0]].concat(n)}))}build(t){let e;fo(t)&&(t=t[0]),t=t,this.cells.forEach(((n,s)=>{ga(`RNNCell_${s}`,(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(Uo(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return ko(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}vo(e)}}function lh(t){const{ones:e,rate:n,training:s=!1,count:i=1}=t,r=()=>Oa(e(),n),a=()=>Ba(r,e,s);if(!i||i<=1)return U(a().clone());return Array(i).fill(void 0).map(a).map((t=>U(t.clone())))}oh.className="StackedRNNCells",n.registerClass(oh);class uh extends Qu{constructor(t){if(t.unroll)throw new Cr("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new Cr("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new So({ndim:5})]}call(t,e){return s((()=>{if(null!=this.cell.dropoutMask&&(W(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(W(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new Ar("ConvRNN2D cell does not support constants");const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return s((()=>{const{stateSize:e}=this.cell,n=t.shape,s=this.computeSingleOutputShape(n),i=[s[0],...s.slice(2)],r=F(i);return Array.isArray(e)?Array(e.length).fill(r):[r]}))}resetStates(t,n=!1){s((()=>{if(!this.stateful)throw new Nr("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new Ar("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>F(r))):this.states_=[F(r)];else if(null==t)W(this.states_),null!=this.keptStates&&(W(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>F(r))):this.states_[0]=F(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Ar(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);n?this.keptStates.push(this.states_.slice()):W(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.arraysEqual(s.shape,i))throw new Ar(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map((t=>U(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=Lu(l,s[0],i,r[0],a[0]),c=Lu(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}uh.className="ConvRNN2D";class hh extends rh{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,Kr(this.filters,"filters"),this.kernelSize=Du(n,2,"kernelSize"),this.kernelSize.forEach((t=>Kr(t,"kernelSize"))),this.strides=Du(s||1,2,"strides"),this.strides.forEach((t=>Kr(t,"strides"))),this.padding=i||"valid",pa(this.padding),this.dataFormat=r||"channelsLast",ca(this.dataFormat),this.dilationRate=Du(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>Kr(t,"dilationRate")))}build(t){var e;t=yo(t);const n="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[n])throw new Ar(`The channel dimension of the input should be defined. Found ${t[n]}`);const s=t[n],i=this.kernelSize.concat([s,4*this.filters]);this.kernel=this.addWeight("kernel",i,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const r=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",r,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const n=this.biasInitializer,s=this.filters;t=new((e=class extends Ua{apply(t,e){return Ta([n.apply([s]),D([s]),n.apply([2*s])])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return s((()=>{if(3!==t.length)throw new Ar(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const n=e.training||!1,s=t[0],i=t[1],r=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=lh({ones:()=>nt(s),rate:this.dropout,training:n,count:4}));const o=this.dropoutMask,l=(t,e,n)=>e&&e[n]?a(e[n],t):t;let h=l(s,o,0),c=l(s,o,1),p=l(s,o,2),d=l(s,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=lh({ones:()=>nt(i),rate:this.recurrentDropout,training:n,count:4}));const f=this.recurrentDropoutMask;let g=l(i,f,0),m=l(i,f,1),y=l(i,f,2),b=l(i,f,3);const[w,k,v,S]=Dt(this.kernel.read(),4,3),[x,N,I,A]=this.useBias?Dt(this.bias.read(),4):[null,null,null,null];h=this.inputConv(h,w,x,this.padding),c=this.inputConv(c,k,N,this.padding),p=this.inputConv(p,v,I,this.padding),d=this.inputConv(d,S,A,this.padding);const[C,z,T,$]=Dt(this.recurrentKernel.read(),4,3);g=this.recurrentConv(g,C),m=this.recurrentConv(m,z),y=this.recurrentConv(y,T),b=this.recurrentConv(b,$);const E=this.recurrentActivation.apply(u(h,g)),F=this.recurrentActivation.apply(u(c,m)),D=u(a(F,r),a(E,this.activation.apply(u(p,y)))),L=a(this.recurrentActivation.apply(u(d,b)),this.activation.apply(D));return[L,L,D]}))}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,e,n,s){const i=Lt(t,e,this.strides,s||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?Ma(i,n,this.dataFormat):i}recurrentConv(t,e){return Lt(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}hh.className="ConvLSTM2DCell",n.registerClass(hh);class ch extends uh{constructor(t){const e=new hh(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}ch.className="ConvLSTM2D",n.registerClass(ch);class ph extends Co{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,s=this.getNoiseShape(n);return Ba((()=>Oa(n,this.rate,s,this.seed)),(()=>n),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}ph.className="Dropout",n.registerClass(ph);class dh extends ph{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}dh.className="SpatialDropout1D",n.registerClass(dh);class fh extends Co{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,Kr(this.units,"units"),this.activation=wu(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=oo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=oo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=sa(t.kernelConstraint),this.biasConstraint=sa(t.biasConstraint),this.kernelRegularizer=Au(t.kernelRegularizer),this.biasRegularizer=Au(t.biasRegularizer),this.activityRegularizer=Au(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=yo(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=yo(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t),s=Gr(this.activation.getClassName());let i;return null!=s?i=Da(n,this.kernel.read(),s,this.bias?this.bias.read():null):(i=Da(n,this.kernel.read()),null!=this.bias&&(i=Ma(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:yu(this.activation),useBias:this.useBias,kernelInitializer:ao(this.kernelInitializer),biasInitializer:ao(this.biasInitializer),kernelRegularizer:Nu(this.kernelRegularizer),biasRegularizer:Nu(this.biasRegularizer),activityRegularizer:Nu(this.activityRegularizer),kernelConstraint:ea(this.kernelConstraint),biasConstraint:ea(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}fh.className="Dense",n.registerClass(fh);class gh extends Co{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=yo(t);for(const e of t.slice(1))if(null==e)throw new Ar(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],ka(t,1)]}call(t,e){return s((()=>{this.invokeCallHook(t,e);let n=mo(t);if("channelsFirst"===this.dataFormat&&n.rank>1){const t=[0];for(let e=2;e<n.rank;++e)t.push(e);t.push(1),n=C(n,t)}return function(t){if(t.rank<=1)throw new Ar(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],ka(t.shape,1)];return w(t,e)}(n)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}gh.className="Flatten",n.registerClass(gh);class mh extends Co{constructor(t){super(t),this.supportsMasking=!0,this.activation=wu(t.activation)}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t);return this.activation.apply(n)}))}getConfig(){const t={activation:yu(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}mh.className="Activation",n.registerClass(mh);class yh extends Co{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return s((()=>{return t=mo(t),e=t,n=this.n,s((()=>{if(2!==e.shape.length)throw new Ar(`repeat() expects a rank-2 tensor, but received a rank-${e.shape.length} tensor.`);return Ea(Ia(e,1),[1,n,1])}));var e,n}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}yh.className="RepeatVector",n.registerClass(yh);class bh extends Co{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new Ar("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=ka(t);if(null!==r){if(0===i||a%i!=0)throw new Ar(n);s[r]=a/i}else if(a!==i)throw new Ar(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t),s=n.shape,i=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return w(n,i)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}bh.className="Reshape",n.registerClass(bh);class wh extends Co{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const n=xa(1,t.dims.length+1);if(!e.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new So({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=yo(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,e){return C(mo(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}wh.className="Permute",n.registerClass(wh);class kh extends Co{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const n=mo(t);return _t(Rt(n,this.maskValue),-1)}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t),s=_t(Rt(n,this.maskValue),-1,!0);return a(n,y(s,n.dtype))}))}}kh.className="Masking",n.registerClass(kh);class vh extends Co{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(Dr(t.inputLength))}this.inputDim=t.inputDim,Kr(this.inputDim,"inputDim"),this.outputDim=t.outputDim,Kr(this.outputDim,"outputDim"),this.embeddingsInitializer=oo(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=Au(t.embeddingsRegularizer),this.activityRegularizer=Au(t.activityRegularizer),this.embeddingsConstraint=sa(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return s((()=>this.maskZero?(t=mo(t),Rt(t,Mt(t))):null))}computeOutputShape(t){if(t=yo(t),null==this.inputLength)return[...t,this.outputDim];const e=Dr(this.inputLength);if(e.length!==t.length-1)throw new Ar(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new Ar(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,e){return s((()=>{this.invokeCallHook(t,e);let n=mo(t);"int32"!==n.dtype&&(n=Na(n,"int32"));const s=La(this.embeddings.read(),w(n,[n.size]));return w(s,yo(this.computeOutputShape(n.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:ao(this.embeddingsInitializer),embeddingsRegularizer:Nu(this.embeddingsRegularizer),activityRegularizer:Nu(this.activityRegularizer),embeddingsConstraint:ea(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}vh.className="Embedding",n.registerClass(vh);class Sh extends Co{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new Cr}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new Ar("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[yo(t)]),(t=t).length<2)throw new Ar(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=Pr(e),e.length>1)throw new Ar(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===Pr(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return s((()=>{if(t=t,this.reshapeRequired){const e=[],n=t.map((t=>t.rank));if(-1===n.indexOf(null)){const s=Sa(n);for(let n of t){const t=n.rank;for(let e=0;e<s-t;++e)n=Ia(n,1);e.push(n)}return this.mergeFunction(e)}{let n=!1;for(const s of t){const t=s.rank;if(null==t){const t=s.shape,i=t[0],r=t.slice(1).concat([i]);let a=w(s,[i].concat(ka(t.slice(1))));a=C(a,[1,0]),a=w(a,r),e.push(a),n=!0}else if(t>1){const i=xa(1,t).concat([0]);e.push(C(s,i)),n=!0}else e.push(s)}let s=this.mergeFunction(e);const i=s.rank;if(n)if(null==i){const t=s.shape,e=t[t.length-1],n=[e].concat(t.slice(0,t.length-1));s=w(C(w(s,[-1,e]),[1,0]),n)}else if(i>1){const t=[i-1].concat(xa(0,i-1));s=C(s,t)}return s}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=Pr(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,e){return s((()=>{if(null==e)return null;if(!Array.isArray(e))throw new Ar("`mask` should be an Array");if(!Array.isArray(t))throw new Ar("`inputs` should be an Array");if(e.length!==t.length)throw new Ar(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${e.length})`);if(e.every((t=>null==t)))return null;let n=(e=e.map((t=>null==t?t:Tt(t,0))))[0];for(let t=1;t<e.length-1;++t)n=at(n,e[t]);return n}))}}class xh extends Sh{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return e}))}}xh.className="Add",n.registerClass(xh);class Nh extends Sh{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=a(e,t[n]);return e}))}}Nh.className="Multiply",n.registerClass(Nh);class Ih extends Sh{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return a(1/t.length,e)}))}}Ih.className="Average",n.registerClass(Ih);class Ah extends Sh{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=Y(e,t[n]);return e}))}}Ah.className="Maximum",n.registerClass(Ah);class Ch extends Sh{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=mt(e,t[n]);return e}))}}Ch.className="Minimum",n.registerClass(Ch);class zh extends Sh{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new Ar("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new Ar("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return s((()=>Ta(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new Ar("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new Ar("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new Ar("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new Ar(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return s((()=>{let n=!0;if(e.forEach((t=>{null==t||(n=!1)})),n)return null;const s=[];for(let n=0;n<t.length;++n)null==e[n]?s.push(y(nt(t[n]),"bool")):e[n].rank<t[n].rank?s.push(Tt(e[n],-1)):s.push(e[n]);const i=I(s,this.axis);return Ot(i,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Th(t,e){for(;t<0;)t+=e;return t}zh.className="Concatenate",n.registerClass(zh);class $h extends Sh{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new Cr("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new Ar(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new Ar(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,i=t[0],o=t[1];return n=Array.isArray(this.axes)?this.axes.map(((e,n)=>Th(e,t[n].shape.length))):[Th(this.axes,i.shape.length),Th(this.axes,o.shape.length)],this.normalize&&(i=jo(i,n[0]),o=jo(o,n[1])),function(t,n,i){if(t.shape.length>3||n.shape.length>3)throw new Cr("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),e.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`)),"number"==typeof i&&(i=[i,i]),"complex64"===t.dtype||"complex64"===n.dtype)throw new Cr("batchDot is not implemented for complex64-type Tensors yet.");const o=t.shape.length,l=n.shape.length;null==i&&(i=[o-1,l-2]);const u=i;return s((()=>{let e,s;if(o>l){e=o-l;const t=[];for(let n=0;n<e;++n)t.push(1);n=w(n,n.shape.concat(t))}else if(l>o){e=l-o;const n=[];for(let t=0;t<e;++t)n.push(1);t=w(t,t.shape.concat(n))}else e=0;if(2===t.shape.length&&2===n.shape.length)s=u[0]===u[1]?r(a(t,n),u[0]):r(a(C(t,[1,0]),n),u[1]);else{const e=u[0]!==t.shape.length-1,i=u[1]===n.shape.length-1;s=Bt(t,n,e,i)}if(e>0){let t;t=o>l?o+l-3:o-1;const n=[];for(let s=t;s<t+e;++s)n.push(s);s=lt(s,n)}return 1===s.shape.length&&(s=Tt(s,1)),s}))}(i,o,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[Th(this.axes,t.length),Th(this.axes,e.length)],n}computeOutputShape(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new Cr("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}$h.className="Dot",n.registerClass($h);class Eh extends Co{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t);return Ba((()=>u(Fa(n.shape,0,this.stddev),n)),(()=>n),e.training||!1)}))}}Eh.className="GaussianNoise",n.registerClass(Eh);class Fh extends Co{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=mo(t);if(this.rate>0&&this.rate<1){return Ba((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return a(n,Fa(n.shape,1,t))}),(()=>n),e.training||!1)}return n}))}}Fh.className="GaussianDropout",n.registerClass(Fh);class Dh extends Co{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||mo(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s((()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(t);return Ba((()=>{const e=mo(t),s=-1.7580993408473766;let i=Wt(_(n),this.rate);i=Na(i,"float32");const r=((1-this.rate)*(1+this.rate*s**2))**-.5,o=-r*s*this.rate,l=u(a(e,i),a(u(i,-1),s));return u(a(l,r),o)}),(()=>mo(t)),e.training||!1)}return t}))}}function Lh(t,e,n,s,i,r=.001){let a;if(2===t.rank)a=Ut(t,e,n,s,i,r);else if(3===t.rank)a=jt(t,e,n,s,i,r);else{if(4!==t.rank)throw new Cr(`batchNormalization is not implemented for array of rank ${t.rank} yet`);a=Vt(t,e,n,s,i,r)}return a}function _h(t,n,i,r,a=.001){return e.arraysEqual(r.slice().sort(),xa(0,t.rank-1))?function(t,e,n,i,r=.001){return s((()=>{const s=Pt(t,i),a=s.mean,o=s.variance;return[Lh(t,a,o,n,e,r),a,o]}))}(t,n,i,r,a):function(t,e,n,i,r=.001){return s((()=>{const s=Pt(t,i),a=s.mean,o=s.variance,l=[];for(const e of xa(0,t.rank))-1!==i.indexOf(e)?l.push(1):l.push(t.shape[e]);const u=w(a,l),h=w(o,l),c=null==e?null:w(e,l),p=null==n?null:w(n,l);return[Lh(t,u,h,p,c,r),a,o]}))}(t,n,i,r,a)}Dh.className="AlphaDropout",n.registerClass(Dh);class Rh extends Co{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=oo(t.betaInitializer||"zeros"),this.gammaInitializer=oo(t.gammaInitializer||"ones"),this.movingMeanInitializer=oo(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=oo(t.movingVarianceInitializer||"ones"),this.betaConstraint=sa(t.betaConstraint),this.gammaConstraint=sa(t.gammaConstraint),this.betaRegularizer=Au(t.betaRegularizer),this.gammaRegularizer=Au(t.gammaRegularizer)}build(t){t=yo(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new Ar(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new So({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return s((()=>{const i=null!=n.training&&n.training,r=mo(t),o=r.shape,l=o.length,u=xa(0,l),h=this.axis>=0?this.axis:this.axis+l;u.splice(h,1);const c=Tr(1,l);c[h]=o[h];const p=u.slice();p.sort();const d=!e.arraysEqual(p,xa(0,l).slice(0,l-1));if(!i)return(()=>{if(d){const t=w(this.movingMean.read(),c),e=w(this.movingVariance.read(),c),n=this.center?w(this.beta.read(),c):null,s=this.scale?w(this.gamma.read(),c):null;return Lh(r,t,e,n,s,this.epsilon)}return Lh(r,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[f,g,m]=_h(r,this.gamma.read(),this.beta.read(),u,this.epsilon),y=(t,e,n)=>{s((()=>{const s=1-n,i=t.read(),r=a(V(i,e),s);t.write(V(i,r))}))};return(()=>{y(this.movingMean,g,this.momentum),y(this.movingVariance,m,this.momentum)})(),f}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:ao(this.betaInitializer),gammaInitializer:ao(this.gammaInitializer),movingMeanInitializer:ao(this.movingMeanInitializer),movingVarianceInitializer:ao(this.movingVarianceInitializer),betaRegularizer:Nu(this.betaRegularizer),gammaRegularizer:Nu(this.gammaRegularizer),betaConstraint:ea(this.betaConstraint),gammaConstraint:ea(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Rh.className="BatchNormalization",n.registerClass(Rh);class Mh extends Co{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=oo(t.betaInitializer||"zeros"),this.gammaInitializer=oo(t.gammaInitializer||"ones"),this.betaRegularizer=Au(t.betaRegularizer),this.gammaRegularizer=Au(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=yo(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==Pr(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(t,e){const n=mo(t),i=n.shape,r=i.length;return s((()=>{let{mean:t,variance:e}=Pt(n,this.axis,!0);const s=Tr(1,r);for(const t of this.axis)s[t]=i[t];const a=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?w(t,s):t;let o=a(this.gamma.read()),l=a(this.beta.read());const u=[],h=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(u.push(i[t]),h.push(1)):(u.push(1),h.push(i[t]));return t=k(t,u),e=k(e,u),o=k(o,h),l=k(l,h),Lh(n,t,e,l,o,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:ao(this.betaInitializer),gammaInitializer:ao(this.gammaInitializer),betaRegularizer:Nu(this.betaRegularizer),gammaRegularizer:Nu(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}Mh.className="LayerNormalization",n.registerClass(Mh);class Oh extends Co{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new Ar(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new Ar(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new Ar(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new So({ndim:4})]}computeOutputShape(t){let e,n;return t=yo(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,e){return s((()=>{return e=mo(t),n=this.padding,i=this.dataFormat,s((()=>{if(4!==e.rank)throw new Ar(`temporalPadding expects input tensor to be 4-D, but received a ${e.rank}-D tensor.`);if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new Ar("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new Ar(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],Kt(e,t)}));var e,n,i}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function Bh(t,e,n,i,r,a){return s((()=>{let s;ca(r),da(a),pa(i),null==n&&(n=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=Ru(t,r);const o="same"===i?"same":"valid";return s="max"===a?qt(t,e,n,o):Gt(t,e,n,o),"channelsFirst"===r&&(s=C(s,[0,3,1,2])),s}))}function Wh(t,e,n,i,r,a){return s((()=>{let s;ca(r),da(a),pa(i),null==n&&(n=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=Mu(t,r);const o="same"===i?"same":"valid";return s="max"===a?Ht(t,e,n,o):Jt(t,e,n,o),"channelsFirst"===r&&(s=C(s,[0,4,1,2,3])),s}))}Oh.className="ZeroPadding2D",n.registerClass(Oh);class Ph extends Co{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new Ar(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if(Kr(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new Ar(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}Kr(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,pa(this.padding),this.inputSpec=[new So({ndim:3})]}computeOutputShape(t){const e=Lu((t=yo(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return s((()=>{this.invokeCallHook(t,e),t=Ia(mo(t),2);const n=this.poolingFunction(mo(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return lt(n,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class Uh extends Ph{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ca(i),pa(s),Bh(t,e,n,s,i,"max")}}Uh.className="MaxPooling1D",n.registerClass(Uh);class jh extends Ph{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ca(i),pa(s),Bh(t,e,n,s,i,"avg")}}jh.className="AveragePooling1D",n.registerClass(jh);class Vh extends Co{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new Ar(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];Kr(this.poolSize,"poolSize"),Kr(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ca(this.dataFormat),pa(this.padding),this.inputSpec=[new So({ndim:4})]}computeOutputShape(t){t=yo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=Lu(e,this.poolSize[0],this.padding,this.strides[0]),n=Lu(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,e){return s((()=>(this.invokeCallHook(t,e),this.poolingFunction(mo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Kh extends Vh{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ca(i),pa(s),Bh(t,e,n,s,i,"max")}}Kh.className="MaxPooling2D",n.registerClass(Kh);class qh extends Vh{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ca(i),pa(s),Bh(t,e,n,s,i,"avg")}}qh.className="AveragePooling2D",n.registerClass(qh);class Gh extends Co{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new Ar(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];Kr(this.poolSize,"poolSize"),Kr(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ca(this.dataFormat),pa(this.padding),this.inputSpec=[new So({ndim:5})]}computeOutputShape(t){t=yo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=Lu(e,this.poolSize[0],this.padding,this.strides[0]),n=Lu(n,this.poolSize[1],this.padding,this.strides[1]),s=Lu(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,e){return s((()=>(this.invokeCallHook(t,e),this.poolingFunction(mo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Hh extends Gh{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ca(i),pa(s),Wh(t,e,n,s,i,"max")}}Hh.className="MaxPooling3D",n.registerClass(Hh);class Jh extends Gh{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ca(i),pa(s),Wh(t,e,n,s,i,"avg")}}Jh.className="AveragePooling3D",n.registerClass(Jh);class Zh extends Co{constructor(t){super(t),this.inputSpec=[new So({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new Cr}}class Yh extends Zh{constructor(t){super(t||{})}call(t,e){return s((()=>{const e=mo(t);return K(e,1)}))}}Yh.className="GlobalAveragePooling1D",n.registerClass(Yh);class Xh extends Zh{constructor(t){super(t||{})}call(t,e){return s((()=>{const e=mo(t);return tt(e,1)}))}}Xh.className="GlobalMaxPooling1D",n.registerClass(Xh);class Qh extends Co{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ca(this.dataFormat),this.inputSpec=[new So({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new Cr}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class tc extends Qh{call(t,e){return s((()=>{const e=mo(t);return"channelsLast"===this.dataFormat?K(e,[1,2]):K(e,[2,3])}))}}tc.className="GlobalAveragePooling2D",n.registerClass(tc);class ec extends Qh{call(t,e){return s((()=>{const e=mo(t);return"channelsLast"===this.dataFormat?tt(e,[1,2]):tt(e,[2,3])}))}}ec.className="GlobalMaxPooling2D",n.registerClass(ec);class nc extends Co{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=Uo(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class sc extends nc{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=yo(t)).length<3)throw new Ar(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=yo(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,e){return s((()=>Xu(((t,n)=>[mo(this.layer.call(t,e)),[]]),t=mo(t),[],!1,null,null,!1,!0)[1]))}}sc.className="TimeDistributed",n.registerClass(sc);class ic extends nc{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=Uo(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=Uo(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,jr(ua,"BidirectionalMergeMode",i),t.weights)throw new Cr("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):Fr(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Yu(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new Ar("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map((t=>new So({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new Cr("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof xo;for(const t of r)if(t instanceof xo!==o)throw new Ar("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s((()=>{const n=e.initialState;let s,i,r,o;if(null==n)s=this.forwardLayer.call(t,e),i=this.backwardLayer.call(t,e);else{const r=n.slice(0,n.length/2),a=n.slice(n.length/2);s=this.forwardLayer.call(t,Object.assign(e,{initialState:r})),i=this.backwardLayer.call(t,Object.assign(e,{initialState:a}))}return this.returnState&&(Array.isArray(s)&&(r=s.slice(1).concat(i.slice(1))),s=s[0],i=i[0]),this.returnSequences&&(i=$t(i,1)),"concat"===this.mergeMode?o=Ta([s,i]):"sum"===this.mergeMode?o=u(s,i):"ave"===this.mergeMode?o=a(.5,u(s,i)):"mul"===this.mergeMode?o=a(s,i):null==this.mergeMode&&(o=[s,i]),this.returnState?null==this.mergeMode?o.concat(r):[o].concat(r):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){ga(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),ga(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=Uo(e.layer);if(delete e.layer,null!=e.numConstants)throw new Cr("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function rc(t){return new jh(t)}function ac(t){return new qh(t)}function oc(t){return new Jh(t)}function lc(t){return new Xh(t)}function uc(t){return new ec(t)}function hc(t){return new Uh(t)}function cc(t){return new Kh(t)}ic.className="Bidirectional",n.registerClass(ic);const pc=lc,dc=uc,fc=hc,gc=cc;var mc=Object.freeze({__proto__:null,inputLayer:function(t){return new To(t)},elu:function(t){return new $u(t)},reLU:function(t){return new Cu(t)},leakyReLU:function(t){return new zu(t)},prelu:function(t){return new Tu(t)},softmax:function(t){return new Fu(t)},thresholdedReLU:function(t){return new Eu(t)},conv1d:function(t){return new Gu(t)},conv2d:function(t){return new Pu(t)},conv2dTranspose:function(t){return new ju(t)},conv3d:function(t){return new Uu(t)},conv3dTranspose:function(t){return new Vu(t)},separableConv2d:function(t){return new qu(t)},cropping2D:function(t){return new Hu(t)},upSampling2d:function(t){return new Ju(t)},depthwiseConv2d:function(t){return new Zu(t)},activation:function(t){return new mh(t)},dense:function(t){return new fh(t)},dropout:function(t){return new ph(t)},spatialDropout1d:function(t){return new dh(t)},flatten:function(t){return new gh(t)},repeatVector:function(t){return new yh(t)},reshape:function(t){return new bh(t)},permute:function(t){return new wh(t)},embedding:function(t){return new vh(t)},add:function(t){return new xh(t)},average:function(t){return new Ih(t)},concatenate:function(t){return new zh(t)},maximum:function(t){return new Ah(t)},minimum:function(t){return new Ch(t)},multiply:function(t){return new Nh(t)},dot:function(t){return new $h(t)},batchNormalization:function(t){return new Rh(t)},layerNormalization:function(t){return new Mh(t)},zeroPadding2d:function(t){return new Oh(t)},averagePooling1d:rc,avgPool1d:function(t){return rc(t)},avgPooling1d:function(t){return rc(t)},averagePooling2d:ac,avgPool2d:function(t){return ac(t)},avgPooling2d:function(t){return ac(t)},averagePooling3d:oc,avgPool3d:function(t){return oc(t)},avgPooling3d:function(t){return oc(t)},globalAveragePooling1d:function(t){return new Yh(t)},globalAveragePooling2d:function(t){return new tc(t)},globalMaxPooling1d:lc,globalMaxPooling2d:uc,maxPooling1d:hc,maxPooling2d:cc,maxPooling3d:function(t){return new Hh(t)},gru:function(t){return new ih(t)},gruCell:function(t){return new sh(t)},lstm:function(t){return new ah(t)},lstmCell:function(t){return new rh(t)},simpleRNN:function(t){return new nh(t)},simpleRNNCell:function(t){return new eh(t)},convLstm2d:function(t){return new ch(t)},convLstm2dCell:function(t){return new hh(t)},rnn:function(t){return new Qu(t)},stackedRNNCells:function(t){return new oh(t)},bidirectional:function(t){return new ic(t)},timeDistributed:function(t){return new sc(t)},globalMaxPool1d:pc,globalMaxPool2d:dc,maxPool1d:fc,maxPool2d:gc,Layer:Co,RNN:Qu,RNNCell:th,input:tu,gaussianNoise:function(t){return new Eh(t)},gaussianDropout:function(t){return new Fh(t)},alphaDropout:function(t){return new Dh(t)},masking:function(t){return new kh(t)}});var yc=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return Qo(t,e)},binaryCrossentropy:function(t,e){return il(t,e)},sparseCategoricalAccuracy:function(t,e){return rl(t,e)},categoricalAccuracy:function(t,e){return tl(t,e)},categoricalCrossentropy:function(t,e){return al(t,e)},precision:function(t,e){return nl(t,e)},recall:function(t,e){return sl(t,e)},cosineProximity:function(t,e){return Zo(t,e)},meanAbsoluteError:function(t,e){return Ko(t,e)},meanAbsolutePercentageError:function(t,e){return qo(t,e)},MAPE:function(t,e){return qo(t,e)},mape:function(t,e){return qo(t,e)},meanSquaredError:function(t,e){return Vo(t,e)},MSE:function(t,e){return Vo(t,e)},mse:function(t,e){return Vo(t,e)}}),bc=Object.freeze({__proto__:null,modelFromJSON:async function(t,e){"modelTopology"in t||(t={modelTopology:t});let n=(t=t).modelTopology;null!=n.model_config&&(n=n.model_config);const s=Uo(bl(n),e);if(null!=t.weightsManifest){const e=await ft.loadWeights(t.weightsManifest,t.pathPrefix,s.weights.map((t=>t.originalName))),n={};for(const t of s.weights)n[t.originalName]=e[t.originalName];s.loadWeights(n),W(e)}return s}});var wc=Object.freeze({__proto__:null,l1l2:function(t){return new Su(t)},l1:function(t){return ku(e=t),new Su({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return ku(e=t),new Su({l2:null!=e?e.l2:null,l1:0});var e}});class kc extends Lo{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof Gl))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function vc(t,e){return t<e}function Sc(t,e){return t>e}class xc extends kc{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new Cr("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=vc:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=Sc:this.monitorFunc=vc,this.monitorFunc===vc&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===vc?1/0:-1/0}async onEpochEnd(t,e){await Eo(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(t)}`),e}}const Nc={earlyStopping:function(t){return new xc(t)}};export{kc as Callback,_o as CallbackList,Oo as CustomCallback,xc as EarlyStopping,Mo as History,So as InputSpec,wo as LayerVariable,Gl as LayersModel,Qu as RNN,Zl as Sequential,xo as SymbolicTensor,Nc as callbacks,ia as constraints,lo as initializers,tu as input,mc as layers,Ql as loadLayersModel,yc as metrics,Yl as model,bc as models,eu as registerCallbackConstructor,wc as regularizers,Xl as sequential,kl as version_layers};
//# sourceMappingURL=tf-layers.fesm.min.js.map
