/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
!function(t,e){"object"==typeof exports&&"undefined"!=typeof module?e(exports,require("@tensorflow/tfjs-core")):"function"==typeof define&&define.amd?define(["exports","@tensorflow/tfjs-core"],e):e((t=t||self).tf=t.tf||{},t.tf)}(this,(function(t,e){"use strict";const n="Add",s="BatchMatMul",i="BatchToSpaceND",r="Cast",a="Concat",o="Conv2D",l="Conv2DBackpropInput",u="Cosh",h="Cumsum",c="RealDiv",p="ExpandDims",d="Floor",f="FloorDiv",g="GatherV2",m="GreaterEqual",y="Identity",b="Maximum",w="Multiply",k="Pack",v="PadV2",S="Reshape",x="Reverse",N="Rsqrt",z="Select",I="Slice",A="Sinh",C="Sigmoid",T="Sqrt",$="SpaceToBatchND",E="SplitV",F="Tile",D="Transpose",L="Unpack",_="UnsortedSegmentSum",R="ZerosLike",M="Step";function O(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function B(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function P(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||q(t)&&!n)for(let s=0;s<t.length;++s)P(t[s],e,n);else e.push(t);return e}function W(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function U(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function j(t){return t%1==0}function V(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function K(t,e){const n=e.length;return B((t=null==t?e.map(((t,e)=>e)):[].concat(t)).every((t=>t>=-n&&t<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${t}`)),B(t.every((t=>j(t))),(()=>`All values in axis param must be integers but got axis ${t}`)),t.map((t=>t<0?n+t:t))}function q(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function G(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error(`Unknown dtype ${t}`)}function H(t){return"string"==typeof t||t instanceof String}function J(t){return Array.isArray(t)?J(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":H(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function Z(t){return!!(t&&t.constructor&&t.call&&t.apply)}function Y(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function X(t,e,n,s=!1){const i=new Array;if(1===e.length){const r=e[0]*(s?2:1);for(let e=0;e<r;e++)i[e]=n[t+e]}else{const r=e[0],a=e.slice(1),o=a.reduce(((t,e)=>t*e))*(s?2:1);for(let e=0;e<r;e++)i[e]=X(t+e*o,a,n,s)}return i}function Q(t,e,n=!1){if(0===t.length)return e[0];const s=t.reduce(((t,e)=>t*e))*(n?2:1);if(0===s)return[];if(s!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}${n?" for a complex tensor":""}.`);return X(0,t,e,n)}function tt(t,e){const n=et(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function et(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error(`Unknown data type ${e}`)}function nt(t){return t&&t.then&&"function"==typeof t.then}const st="tfjsflags";class it{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=rt,this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(nt(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=this.getQueryParams(this.global.location.search);if(st in t){t.tfjsflags.split(",").forEach((t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)}))}}}function rt(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("=")))),e}function at(){return lt}let ot,lt=null;function ut(){if(null==ot){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}ot=t}return ot}function ht(t,e){const n=function(){const t=ut();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const ct=ht("kernelRegistry",(()=>new Map)),pt=ht("gradRegistry",(()=>new Map));function dt(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return ct.get(n)}function ft(t){return pt.get(t)}function gt(t){const e=ct.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function mt(t){const{kernelName:e}=t;pt.has(e)&&at().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),pt.set(e,t)}function yt(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=P(t)),at().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error(`Unknown data type ${e}`)}function bt(){return at().platform.now()}function wt(t,e="utf-8"){return e=e||"utf-8",at().platform.decode(t,e)}class kt{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new St)}profileKernel(t,e,n){let s;const i=()=>{s=n()};let r;const a=bt();if(this.backendTimer.timerAvailable())r=this.backendTimer.time(i);else{i();for(const t of s)t.dataSync();r=Promise.resolve({kernelMs:bt()-a})}if(at().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then((e=>{vt(e,n.dtype,t)}))}return{kernelName:t,outputs:s,inputs:e,timeMs:r.then((t=>t.kernelMs)),extraInfo:r.then((t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():""))}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach((t=>{Promise.all([t.data(),s,r]).then((n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])}))}))}}function vt(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class St{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?V(`${s}ms`,9):s.error,o=V(t,25),l=e.rank,u=e.size,h=V(e.shape.toString(),14);let c="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;c+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${h}\t%c${u}\t%c${c}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function xt(t,e,n,s){const i=Y(e),r=function(t,e,n,s){const i=W(e),r=s[s.length-1],a=new Array(r).fill(0),o=e.length,l="complex64"===n?At(t):t;if(o>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)a[t]=Math.max(a[t],Nt(l[e+t],0,n).length)}return a}(t,e,n,i),a=e.length,o=It(t,e,n,i,r),l=["Tensor"];return s&&(l.push(`  dtype: ${n}`),l.push(`  rank: ${a}`),l.push(`  shape: [${e}]`),l.push("  values:")),l.push(o.map((t=>"    "+t)).join("\n")),l.join("\n")}function Nt(t,e,n){let s;return s=Array.isArray(t)?`${parseFloat(t[0].toFixed(7))} + ${parseFloat(t[1].toFixed(7))}j`:H(t)?`'${t}'`:"bool"===n?zt(t):parseFloat(t.toFixed(7)).toString(),V(s,e)}function zt(t){return 0===t?"false":"true"}function It(t,e,n,s,i,r=!0){const a="complex64"===n?2:1,o=e[0],l=e.length;if(0===l){if("complex64"===n){return[Nt(At(t)[0],0,n)]}return"bool"===n?[zt(t[0])]:[t[0].toString()]}if(1===l){if(o>20){const e=3*a;let s=Array.from(t.slice(0,e)),r=Array.from(t.slice((o-3)*a,o*a));return"complex64"===n&&(s=At(s),r=At(r)),["["+s.map(((t,e)=>Nt(t,i[e],n))).join(", ")+", ..., "+r.map(((t,e)=>Nt(t,i[o-3+e],n))).join(", ")+"]"]}return["["+("complex64"===n?At(t):Array.from(t)).map(((t,e)=>Nt(t,i[e],n))).join(", ")+"]"]}const u=e.slice(1),h=s.slice(1),c=s[0]*a,p=[];if(o>20){for(let e=0;e<3;e++){const s=e*c,r=s+c;p.push(...It(t.slice(s,r),u,n,h,i,!1))}p.push("...");for(let e=o-3;e<o;e++){const s=e*c,r=s+c;p.push(...It(t.slice(s,r),u,n,h,i,e===o-1))}}else for(let e=0;e<o;e++){const s=e*c,r=s+c;p.push(...It(t.slice(s,r),u,n,h,i,e===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let t=1;t<p.length-1;t++)p[t]=" "+p[t]+d;let f=",\n";for(let t=2;t<l;t++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(r?"":f),p}function At(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let Ct=null,Tt=null;class $t{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=W(t),this.strides=Y(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return Tt.buffer(this.shape,this.dtype,t)}bufferSync(){return Tt.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return Q(this.shape,t,"complex64"===this.dtype)}arraySync(){return Q(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const t=Ct().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map((t=>wt(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=Ct().readSync(this.dataId);if("string"===this.dtype)try{return t.map((t=>wt(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await Ct().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(Ct().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return Tt.print(this,t)}clone(){return this.throwIfDisposed(),Tt.clone(this)}toString(t=!1){return xt(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),Tt.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),Ct().makeVariable(this,t,e,n)}}Object.defineProperty($t,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),ht("Tensor",(()=>$t));class Et extends $t{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!U(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);Ct().disposeTensor(this),this.dataId=t.dataId,Ct().incRef(this,null)}dispose(){Ct().disposeVariable(this),this.isDisposedInternal=!0}}var Ft,Dt,Lt,_t,Rt;Object.defineProperty(Et,Symbol.hasInstance,{value:t=>t instanceof $t&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(Ft||(Ft={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(Dt||(Dt={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(Lt||(Lt={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(_t||(_t={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(Rt||(Rt={}));const Mt={float32:_t,int32:Dt,bool:Lt,complex64:Rt};function Ot(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Mt[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function Bt(t){const e=[];return Pt(t,e,new Set),e}function Pt(t,e,n){if(null==t)return;if(t instanceof $t)return void e.push(t);if(s=t,!Array.isArray(s)&&"object"!=typeof s)return;var s;const i=t;for(const t in i){const s=i[t];n.has(s)||(n.add(s),Pt(s,e,n))}}function Wt(t){return null!=t.kernelName}class Ut{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((t=>t.name))))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class jt{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new Ut}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(`${t} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new kt(this.backendInstance),!0}setupRegisteredKernels(){gt(this.backendName).forEach((t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(t){gt(t).forEach((e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])}))}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const n=e.factory();if(!n||n instanceof class{refCount(t){return O("refCount")}incRef(t){return O("incRef")}timerAvailable(){return!0}time(t){return O("time")}read(t){return O("read")}readSync(t){return O("readSync")}numDataIds(){return O("numDataIds")}disposeData(t,e){return O("disposeData")}write(t,e,n){return O("write")}move(t,e,n,s,i){return O("move")}memory(){return O("memory")}floatPrecision(){return O("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return O("dispose")}}||"function"!=typeof n.then)return this.registry[t]=n,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,s=n.then((n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0))).catch((n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1)));return this.pendingBackendInit=s,{success:s,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(`${t} backend not found in registry`);this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority))}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e),r=s.refCount(e);s.disposeData(e,!0),n.backend=t,t.move(e,i,n.shape,n.dtype,r),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return jt.nextTensorId++}nextVariableId(){return jt.nextVariableId++}clone(t){const e=Vt.runKernel(y,{x:t}),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],(t=>({x:()=>{const e={x:t},n={dtype:"float32"};return Vt.runKernel(r,e,n)}})),[],{}),e}runKernel(t,e,n){null==this.backendName&&this.backend;if(!(null!=dt(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach((t=>{i+="complex64"===t.dtype?3:1}));const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const s=this.isTapeOn(),i=this.state.numBytes,r=this.state.numTensors;let a,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const l=Wt(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(Wt(t)){const{kernelName:e,inputs:i,attrs:r}=t;null==this.backendName&&this.backend;const l=dt(e,this.backendName);B(null!=l,(()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`)),a=()=>{const t=this.backend.numDataIds();o=l.kernelFunc({inputs:i,attrs:r,backend:this.backend});const a=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,a);const u=a.map((t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)}));if(s){const t=this.getTensorsForGradient(e,i,u);n=this.saveTensorsForBackwardMode(t)}return u}}else{const{forwardFunc:e}=t,i=t=>{s&&(n=t.map((t=>this.keep(this.clone(t)))))};a=()=>{const t=this.backend.numDataIds();o=this.tidy((()=>e(this.backend,i)));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(l,t,n),n}}const{inputs:u,attrs:h}=t,c=Wt(t)?null:t.backwardsFunc;let p;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(p=this.profiler.profileKernel(l,u,(()=>a())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(p),e=p.outputs):e=a()})),s&&this.addTapeNode(l,u,e,c,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:l,bytesAdded:this.state.numBytes-i,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-r,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(u).map((t=>null!=u[t]?u[t].shape:null)),outputShapes:e.map((t=>t.shape)),kernelTimeMs:p.timeMs,extraInfo:p.extraInfo}),Array.isArray(o)?e:e[0]}saveTensorsForBackwardMode(t){return t.map((t=>this.keep(this.clone(t))))}getTensorsForGradient(t,e,n){const s=ft(t);if(null!=s){const t=s.inputsToSave||[],i=s.outputsToSave||[];let r;s.saveAllInputs?(B(Array.isArray(e),(()=>"saveAllInputs is true, expected inputs to be an array.")),r=Object.keys(e).map((t=>e[t]))):r=t.map((t=>e[t]));const a=n.filter(((t,e)=>i[e]));return r.concat(a)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&H(t[0])&&(i=t.map((t=>function(t,e="utf-8"){return e=e||"utf-8",at().platform.encode(t,e)}(t))));const r=s.write(i,e,n),a=new $t(e,n,r,this.nextTensorId());if(this.trackTensor(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach((t=>e+=t.length)),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new $t(e,n=n||"float32",t,this.nextTensorId());return this.trackTensor(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new Et(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}trackTensor(t,e){this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*G(t.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(t.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n})),t instanceof Et||this.track(t)}incRef(t,e){this.trackTensor(t,e),this.backend.incRef(t.dataId)}removeDataId(t,e){this.state.tensorInfo.has(t)&&this.state.tensorInfo.get(t).backend===e&&(this.state.tensorInfo.delete(t),this.state.numDataBuffers--)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;const e=this.state.tensorInfo.get(t.dataId);if(this.state.numTensors--,"string"===t.dtype&&(this.state.numStringTensors--,this.state.numBytes-=e.bytes),"complex64"!==t.dtype&&"string"!==t.dtype){const e=t.size*G(t.dtype);this.state.numBytes-=e}e.backend.disposeData(t.dataId)&&this.removeDataId(t.dataId,e.backend)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((t=>t.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=ft(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map(((t,e)=>{if(null==t){const t=n[e],s=et(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t})),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=Bt(t),n=new Set(e.map((t=>t.id)));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach((t=>{t.kept||t.scopeId!==s.id||this.track(t)}))}gradients(t,e,n,s=!1){if(B(e.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const i=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",t)));B(i instanceof $t,(()=>"The result y returned by f() must be a tensor."));const r=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach((t=>s[t.id]=!0)),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,i);if(!s&&0===r.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const t={};t[i.id]=null==n?function(t){const e=tt(W(t),"float32");return Vt.makeTensor(e,t,"float32")}(i.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach((e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)})),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const o=r.gradient(a);for(const e in r.inputs){if(!(e in o))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(o)}.`);const i=n((()=>o[e]()));if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!U(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,r,(t=>this.tidy(t)),Kt);const s=e.map((e=>t[e.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((t=>{for(const e of t.saved)e.dispose()})),this.state.activeTape=null),{value:i,grads:s}}))}customGrad(t){return B(Z(t),(()=>"The f passed in customGrad(f) must be a function.")),(...e)=>{let n;B(e.every((t=>t instanceof $t)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const s={};e.forEach(((t,e)=>{s[e]=t}));return this.runKernelFunc({forwardFunc:(s,i)=>(n=t(...e,i),B(n.value instanceof $t,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),B(Z(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(t,s)=>{const i=n.gradFunc(t,s),r=Array.isArray(i)?i:[i];B(r.length===e.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),B(r.every((t=>t instanceof $t)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const a={};return r.forEach(((t,e)=>{a[e]=()=>t})),a},inputs:s})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=bt(),n=await this.backend.time(t);return n.wallMs=bt()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new Ut;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}jt.nextTensorId=0,jt.nextVariableId=0;const Vt=function(){const t=ut();if(null==t._tfengine){const e=new it(t);t._tfengine=new jt(e)}var e;return e=t._tfengine.ENV,lt=e,Ct=()=>t._tfengine,t._tfengine}();function Kt(t,e){const s={a:t,b:e};return Vt.runKernel(n,s)}function qt(t,e){let n=t;if(q(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||q(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&at().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&Gt(t,s,[]),s}function Gt(t,e,n){if(n=n||[],!Array.isArray(t)&&!q(t))return void B(0===e.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${e[0]} elements`));B(e.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${t.length} elements`)),B(t.length===e[0],(()=>`Element arr[${n.join("][")}] should have ${e[0]} elements, but has ${t.length} elements`));const s=e.slice(1);for(let e=0;e<t.length;++e)Gt(t[e],s,n.concat(e))}function Ht(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function Jt(t,e,n,s="numeric"){if(t instanceof $t)return Ht(s,t.dtype,e,n),t;let i=J(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),Ht(s,i,e,n),null==t||!q(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const r=qt(t,i);q(t)||Array.isArray(t)||(t=[t]);const a="string"!==i?yt(t,i):P(t,[],!0);return Vt.makeTensor(a,r,i)}function Zt(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map(((t,i)=>Jt(t,`${e}[${i}]`,n,s)))}function Yt(t){const e=Object.keys(t);if(1!==e.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${e.length} keys.`);let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{Vt.startScope(n);try{const e=s(...t);return nt(e)&&console.error("Cannot return a Promise inside of tidy."),Vt.endScope(e),e}catch(t){throw Vt.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const Xt=Yt({cast_:function(t,e){const n=Jt(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error(`Failed to cast to unknown dtype ${e}`);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return Vt.runKernel(r,s,i)}});const Qt=Yt({mul_:function(t,e){let n=Jt(t,"a","mul"),s=Jt(e,"b","mul");[n,s]=Ot(n,s);const i={a:n,b:s};return Vt.runKernel(w,i)}});const te=Yt({step_:function(t,e=0){const n={x:Jt(t,"x","step")},s={alpha:e};return Vt.runKernel(M,n,s)}}),ee={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,te(Xt(n,"float32"),-1))}}};const ne=Yt({floorDiv_:function(t,e){let n=Jt(t,"a","floorDiv"),s=Jt(e,"b","floorDiv");[n,s]=Ot(n,s);const i={a:n,b:s};return Vt.runKernel(f,i)}});const se=Yt({div_:function(t,e){let n=Jt(t,"a","div"),s=Jt(e,"b","div");if([n,s]=Ot(n,s),"int32"===n.dtype&&"int32"===s.dtype)return ne(n,s);const i={a:n,b:s};return Vt.runKernel(c,i,{})}});const ie=Yt({neg_:function(t){const e={x:Jt(t,"x","neg")};return Vt.runKernel("Neg",e)}});function re(t,e,n,s){if(null==s&&(s=J(t)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!q(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach((e=>{B(Number.isInteger(e)&&e>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`))}))}(e);const t=W(e),s=W(n);B(t===s,(()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${s}`));for(let t=0;t<n.length;++t){const s=n[t],i=t!==n.length-1||s!==W(e.slice(t));B(n[t]===e[t]||!i,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `))}}return q(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==s?yt(t,s):P(t,[],!0),Vt.makeTensor(t,e,s)}function ae(t,e){if((q(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&q(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return re(t,[],[],e)}const oe=Yt({sqrt_:function(t){const e={x:Jt(t,"x","sqrt")};return Vt.runKernel(T,e)}});const le=Yt({square_:function(t){const e=Jt(t,"x","square");return Vt.runKernel("Square",{x:e},{})}});const ue=Yt({sub_:function(t,e){let n=Jt(t,"a","sub"),s=Jt(e,"b","sub");[n,s]=Ot(n,s);const i={a:n,b:s};return Vt.runKernel("Sub",i)}}),he={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=le(Xt(n,"float32")),s=oe(ue(ae(1),e));return ie(se(t,s))}}}},ce={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=oe(ue(le(Xt(n,"float32")),1));return se(t,e)}}}};function pe(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function de(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const fe=Yt({reshape_:function(t,e){const n={x:Jt(t,"x","reshape","string_or_numeric")},s={shape:e};return Vt.runKernel(S,n,s)}});const ge=Yt({sum_:function(t,e=null,n=!1){let s=Jt(t,"x","sum");"bool"===s.dtype&&(s=Xt(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Vt.runKernel("Sum",i,r)}}),me={kernelName:n,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{let e=t;const s=pe(n.shape,i);return s.length>0&&(e=ge(e,s)),fe(e,n.shape)},b:()=>{let e=t;const n=pe(s.shape,i);return n.length>0&&(e=ge(e,n)),fe(e,s.shape)}}}},ye={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}};const be=Yt({zerosLike_:function(t){const e={x:Jt(t,"x","zerosLike")};return Vt.runKernel(R,e)}}),we={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>be(n)}}},ke={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>be(n)}}},ve={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,oe(ue(ae(1),le(Xt(n,"float32")))))}}};const Se=Yt({add_:function(t,e){let s=Jt(t,"a","add"),i=Jt(e,"b","add");[s,i]=Ot(s,i);const r={a:s,b:i};return Vt.runKernel(n,r)}}),xe={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=oe(Se(ae(1),le(Xt(n,"float32"))));return se(t,e)}}}},Ne={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{const e=Se(le(n),le(s));let r=Qt(t,se(s,e));const a=pe(n.shape,i);return a.length>0&&(r=ge(r,a)),fe(r,n.shape)},b:()=>{const e=Se(le(n),le(s));let r=ie(Qt(t,se(n,e)));const a=pe(s.shape,i);return a.length>0&&(r=ge(r,a)),fe(r,s.shape)}}}},ze={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Se(le(Xt(n,"float32")),1))}}},Ie={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,ue(ae(1),le(Xt(n,"float32"))))}}};const Ae=Yt({avgPool3dGrad_:function(t,e,n,s,i,r){const a=Jt(t,"dy","avgPool3dGrad"),o=Jt(e,"input","avgPool3dGrad");let l=a,u=o,h=!1;4===o.rank&&(h=!0,l=fe(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),u=fe(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),B(5===l.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${l.rank}.`)),B(5===u.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${u.rank}.`)),null!=r&&B(j(i),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const c={dy:l,input:u},p={filterSize:n,strides:s,pad:i,dimRoundingMode:r},d=Vt.runKernel("AvgPool3DGrad",c,p);return h?fe(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}}),Ce={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>Ae(t,s,i,r,a,o)}}};const Te=Yt({avgPoolGrad_:function(t,e,n,s,i){const r=Jt(t,"dy","avgPoolGrad"),a=Jt(e,"input","avgPoolGrad");B(a.rank===r.rank,(()=>`Rank of input (${a.rank}) does not match rank of dy (${r.rank})`));let o=a,l=r,u=!1;3===a.rank&&(u=!0,o=fe(a,[1,a.shape[0],a.shape[1],a.shape[2]]),l=fe(r,[1,r.shape[0],r.shape[1],r.shape[2]])),B(4===l.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${l.rank}.`)),B(4===o.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${o.rank}.`));const h={dy:l,input:o},c={filterSize:n,strides:s,pad:i},p=Vt.runKernel("AvgPoolGrad",h,c);return u?fe(p,[p.shape[1],p.shape[2],p.shape[3]]):p}}),$e={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>Te(t,s,i,r,a)}}};const Ee=Yt({matMul_:function(t,e,n=!1,i=!1){let r=Jt(t,"a","matMul"),a=Jt(e,"b","matMul");[r,a]=Ot(r,a);const o={a:r,b:a},l={transposeA:n,transposeB:i};return Vt.runKernel(s,o,l)}}),Fe={kernelName:s,inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>Ee(t,i,!1,!1),b:()=>Ee(t,s,!0,!1)}:r&&!a?{a:()=>Ee(i,t,!1,!0),b:()=>Ee(s,t,!1,!1)}:{a:()=>Ee(i,t,!0,!0),b:()=>Ee(t,s,!0,!0)}:{a:()=>Ee(t,i,!1,!0),b:()=>Ee(s,t,!0,!1)}}};const De=Yt({spaceToBatchND_:function(t,e,n){const s=Jt(t,"x","spaceToBatchND");B(s.rank>=1+e.length,(()=>`input rank ${s.rank} should be > than [blockShape] ${e.length}`)),B(n.length===e.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`)),B(s.shape.reduce(((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t),!0),(()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`));const i={x:s},r={blockShape:e,paddings:n};return Vt.runKernel($,i,r)}}),Le={kernelName:i,gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>De(t,s,i)}}},_e={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>ge(t,o,!0)}}},Re={kernelName:r,gradFunc:t=>({x:()=>t.clone()})},Me={kernelName:"Ceil",gradFunc:t=>({x:()=>be(t)})};const Oe=Yt({greaterEqual_:function(t,e){let n=Jt(t,"a","greaterEqual","string_or_numeric"),s=Jt(e,"b","greaterEqual","string_or_numeric");[n,s]=Ot(n,s),de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel(m,i)}});const Be=Yt({lessEqual_:function(t,e){let n=Jt(t,"a","lessEqual","string_or_numeric"),s=Jt(e,"b","lessEqual","string_or_numeric");[n,s]=Ot(n,s),de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel("LessEqual",i)}});const Pe=Yt({logicalAnd_:function(t,e){const n=Jt(t,"a","logicalAnd","bool"),s=Jt(e,"b","logicalAnd","bool");de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel("LogicalAnd",i)}});const We=Yt({clone_:function(t){const e={x:Jt(t,"x","clone","string_or_numeric")};return Vt.runKernel(y,e)}});const Ue=Yt({broadcastTo_:function(t,e){let n=Jt(t,"broadcastTo","x");const s=n.shape;if(e.some((t=>!(t>0)||t%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=fe(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map(((t,e)=>t>1?e:-1)).filter((t=>t>=0)).length)return We(n);const a={x:n},o={reps:r};return Vt.runKernel(F,a,o)}});const je=Yt({where_:function(t,e,n){const s=Jt(e,"a","where"),i=Jt(n,"b","where"),r=Jt(t,"condition","where","bool"),a=de(de(r.shape,s.shape),i.shape),o={condition:Ue(r,a),t:Ue(s,a),e:Ue(i,a)};return Vt.runKernel(z,o)}}),Ve={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>je(Pe(Oe(s,i),Be(s,r)),t,be(t))}}},Ke={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:ee.gradFunc};const qe=Yt({split_:function(t,e,n=0){const s={x:Jt(t,"x","split")},i={numOrSizeSplits:e,axis:n};return Vt.runKernel(E,s,i)}}),Ge={kernelName:a,saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,r=K(i,e[0].shape)[0],a=s.map((t=>t[r]));return qe(t,a,r).map((t=>()=>t))}};const He=Yt({conv2DBackpropFilter_:function(t,e,n,s,i,r="NHWC",a){let o=t;3===t.rank&&(o=fe(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=fe(e,[1,e.shape[0],e.shape[1],e.shape[2]])),B(4===o.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${o.shape}.`)),B(4===l.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${l.shape}.`)),B(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const u="NHWC"===r?o.shape[3]:o.shape[1],h="NHWC"===r?l.shape[3]:l.shape[1];B(u===n[2],(()=>`Error in conv2dDerFilter: depth of input ${u}) must match input depth in filter (${n[2]}.`)),B(h===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`)),null!=a&&B(j(i),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const c={x:o,dy:l},p={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,filterShape:n};return Vt.runKernel("Conv2DBackpropFilter",c,p)}});const Je=Yt({conv2DBackpropInput_:function(t,e,n,s,i,r="NHWC",a){B(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let o=t,u=e,h=!1;3===e.rank&&(h=!0,u=fe(e,[1,e.shape[0],e.shape[1],e.shape[2]]),o=[1,t[0],t[1],t[2]]),B(4===o.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${o.length}.`)),B(4===u.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${u.rank}`)),B(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const c="NHWC"===r?o[3]:o[1],p="NHWC"===r?u.shape[3]:u.shape[1];B(c===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${c}) must match input depth for filter ${n.shape[2]}.`)),B(p===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${p}) must match output depth for filter ${n.shape[3]}.`)),null!=a&&B(j(i),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const d={dy:u,filter:n},f={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,inputShape:o},g=Vt.runKernel(l,d,f);return h?fe(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});function Ze(t){const[e,n,s]=function(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}(t);return 1===e&&1===n&&1===s}function Ye(t,e){return Ze(t)||Ze(e)}const Xe={kernelName:o,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:r,strides:a,pad:o,dataFormat:l}=n;return B(Ze(r),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${r}'`)),{x:()=>Je(s.shape,t,i,a,o,l),filter:()=>He(s,t,i.shape,a,o,l)}}};const Qe=Yt({conv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const l=Jt(t,"x","conv2d"),u=Jt(e,"filter","conv2d");let h=l,c=!1;3===l.rank&&(c=!0,h=fe(l,[1,l.shape[0],l.shape[1],l.shape[2]])),B(4===h.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${h.rank}.`)),B(4===u.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${u.rank}.`)),null!=a&&B(j(s),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const p="NHWC"===i?h.shape[3]:h.shape[1];B(p===u.shape[2],(()=>`Error in conv2d: depth of input (${p}) must match input depth for filter ${u.shape[2]}.`)),B(Ye(n,r),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${r}'`));const d={x:h,filter:u},f={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},g=Vt.runKernel(o,d,f);return c?fe(g,[g.shape[1],g.shape[2],g.shape[3]]):g}}),tn={kernelName:l,inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>Qe(t,i,r,a,o,1,l),filter:()=>He(t,s,i.shape,r,a,o,l)}}};const en=Yt({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=fe(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let a=e;4===a.rank&&(a=fe(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),B(5===r.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${r.shape}.`)),B(5===a.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${a.shape}.`)),B(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),B(r.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${r.shape[4]}) must match input depth in filter (${n[3]}.`)),B(a.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${a.shape[4]}) must match output depth for filter (${n[4]}).`));const o={x:r,dy:a},l={strides:s,pad:i,filterShape:n};return Vt.runKernel("Conv3DBackpropFilterV2",o,l)}});const nn=Yt({conv3DBackpropInput_:function(t,e,n,s,i){B(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let r=t,a=e,o=!1;4===e.rank&&(o=!0,a=fe(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),r=[1,t[0],t[1],t[2],t[3]]);const l=r[4],u=a.shape[4];B(5===r.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${r.length}.`)),B(5===a.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${a.rank}`)),B(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),B(l===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${l}) must match input depth for filter ${n.shape[3]}.`)),B(u===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${u}) must match output depth for filter ${n.shape[4]}.`));const h={dy:a,filter:n},c={pad:i,strides:s,inputShape:r},p=Vt.runKernel("Conv3DBackpropInputV2",h,c);return o?fe(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}}),sn={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r}=n;B(Ze(s),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`));const[a,o]=e;return{x:()=>nn(a.shape,t,o,i,r),filter:()=>en(a,t,o.shape,i,r)}}};const rn=Yt({sin_:function(t){const e={x:Jt(t,"x","sin")};return Vt.runKernel("Sin",e)}}),an={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(ie(rn(Xt(n,"float32"))),t)}}};const on=Yt({sinh_:function(t){const e={x:Jt(t,"x","sinh")};return Vt.runKernel(A,e)}}),ln={kernelName:u,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(on(Xt(n,"float32")),t)}}};function un(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map((t=>1)),e)}function hn(t){return t.map(((t,e)=>[e,t])).sort(((t,e)=>t[1]-e[1])).map((t=>t[0]))}const cn=Yt({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:Jt(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return Vt.runKernel(h,i,r)}});const pn=Yt({transpose_:function(t,e){const n=Jt(t,"x","transpose");if(null==e&&(e=n.shape.map(((t,e)=>e)).reverse()),B(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`)),e.forEach((t=>{B(t>=0&&t<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${e}`))})),n.rank<=1)return n.clone();const s={x:n},i={perm:e};return Vt.runKernel(D,s,i)}}),dn={kernelName:h,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach((t=>n.push(t))),n}([i],s.rank);let n=cn(t,i,r,!a);return null!=e&&(n=pn(n,e)),n}}}};const fn=Yt({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=fe(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=fe(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return Vt.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const gn=Yt({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=fe(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=Vt.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?fe(c,[c.shape[1],c.shape[2],c.shape[3]]):c}}),mn={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r,dimRoundingMode:a}=n,o=null==s?[1,1]:s;B(Ze(o),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`));const[l,u]=e;return B(4===l.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${l.rank}.`)),B(4===u.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${u.rank}.`)),B(l.shape[3]===u.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${l.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`)),B(Ye(i,o),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${i} and dilations '${o}'.`)),null!=a&&B(j(r),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`)),{x:()=>gn(l.shape,t,u,i,r,s,a),filter:()=>fn(l,t,u.shape,i,r,s,a)}}},yn={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>Vt.runKernel("Dilation2DBackpropInput",r,n),filter:()=>Vt.runKernel("Dilation2DBackpropFilter",a,n)}}},bn={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>Vt.runKernel("EluGrad",s)}}};const wn=Yt({exp_:function(t){const e={x:Jt(t,"x","exp")};return Vt.runKernel("Exp",e)}}),kn={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=Qt(wn(ie(le(n))),2/Math.sqrt(Math.PI));return{x:()=>Qt(t,s)}}},vn={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,n)}}},Sn={kernelName:p,inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>fe(t,n.shape)}}},xn={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,wn(n))}}},Nn={kernelName:d,gradFunc:t=>({x:()=>be(t)})},zn={kernelName:f,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{const e=se(t,Xt(s,"float32")),r=pe(n.shape,i);return r.length>0?fe(ge(e,r),n.shape):e},b:()=>{let e=Qt(t,Xt(n,"float32"));const r=pe(s.shape,i);r.length>0&&(e=fe(ge(e,r),s.shape));const a=le(s);return ie(se(e,Xt(a,"float32")))}}}};const In=Yt({rsqrt_:function(t){const e={x:Jt(t,"x","rsqrt")};return Vt.runKernel(N,e)}});const An=Yt({tile_:function(t,e){const n=Jt(t,"x","tile","string_or_numeric");B(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`));const s={x:n},i={reps:e};return Vt.runKernel(F,s,i)}}),Cn={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?ae(1):o,u=pe(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=ue(i,r),p=Qt(t,l),d=In(Se(a,ae(s))),f=Qt(Qt(Qt(d,d),d),ae(-.5));return{x:()=>1===r.rank?fe(Qt(Qt(t,An(fe(d,[1,1,1,r.shape[0]]),h)),l),i.shape):fe(Qt(Qt(t,d),l),i.shape),mean:()=>{let t=Qt(Qt(d,ae(-1)),p);return 1===r.rank&&(t=ge(t,u)),fe(t,r.shape)},variance:()=>{let t=Qt(Qt(f,c),p);return 1===r.rank&&(t=ge(t,u)),fe(t,r.shape)},scale:()=>{const e=Qt(c,d);let n=Qt(t,e);return 1===r.rank&&(n=ge(n,u)),fe(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=ge(e,u)),fe(e,r.shape)}}}};const Tn=Yt({unsortedSegmentSum_:function(t,e,n){const s=Jt(t,"x","unsortedSegmentSum"),i=Jt(e,"segmentIds","unsortedSegmentSum","int32");B(j(n),(()=>"numSegments must be of dtype int"));const r={x:s,segmentIds:i},a={numSegments:n};return Vt.runKernel(_,r,a)}}),$n={kernelName:g,inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=K(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=En(0,l),p=En(l+1,l+1+h),d=Fn([o,[n],u]),f=fe(t,d),g=fe(i,[n]),m=Fn([[l],c,p]),y=pn(f,m);let b=Tn(y,g,s.shape[a]);const w=hn(m);return b=pn(b,w),b},indices:()=>i}}};function En(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Fn(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const Dn={kernelName:m,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>be(n),b:()=>be(s)}}},Ln={kernelName:y,gradFunc:t=>({x:()=>Xt(t,"float32")})},_n={kernelName:"IsFinite",gradFunc:t=>({x:()=>be(t)})},Rn={kernelName:"IsInf",gradFunc:t=>({x:()=>be(t)})},Mn={kernelName:"IsNan",gradFunc:t=>({x:()=>be(t)})};const On=Yt({greater_:function(t,e){let n=Jt(t,"a","greater","string_or_numeric"),s=Jt(e,"b","greater","string_or_numeric");[n,s]=Ot(n,s),de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel("Greater",i)}}),Bn={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=On(s,0);return{x:()=>je(r,t,Qt(t,i))}}},Pn={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Se(n,1))}}},Wn={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Xt(n,"float32"))}}},Un={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=wn(s);return ue(t,Qt(ge(t,i,!0),e))}}}};const jn=Yt({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return Vt.runKernel("LRNGrad",o,l)}}),Vn={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>jn(s,i,t,r,a,o,l)}}};const Kn=Yt({equal_:function(t,e){let n=Jt(t,"a","equal","string_or_numeric"),s=Jt(e,"b","equal","string_or_numeric");[n,s]=Ot(n,s),de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel("Equal",i)}});function qn(t,e,n,s){return e.rank<n.rank&&(e=fe(e,un(e.shape,s))),t.rank<n.rank&&(t=fe(t,un(t.shape,s))),{x:()=>Qt(t,Xt(Kn(n,e),t.dtype))}}const Gn={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=qn(t,e[1],r,K(i,r.shape));return{x:()=>a.x()}}};const Hn=Yt({less_:function(t,e){let n=Jt(t,"a","less","string_or_numeric"),s=Jt(e,"b","less","string_or_numeric");[n,s]=Ot(n,s),de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel("Less",i)}}),Jn={kernelName:b,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Qt(t,Xt(Oe(n,s),"float32")),b:()=>Qt(t,Xt(Hn(n,s),"float32"))}}};const Zn=Yt({maxPool3dGrad_:function(t,e,n,s,i,r,a){const o=Jt(t,"dy","maxPool3dGrad"),l=Jt(e,"input","maxPool3dGrad"),u=Jt(n,"output","maxPool3dGrad");let h=o,c=l,p=u,d=!1;4===l.rank&&(d=!0,h=fe(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=fe(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=fe(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),B(5===h.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),B(5===c.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),B(5===p.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${p.rank}.`)),null!=a&&B(j(r),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const f={dy:h,input:c,output:p},g={filterSize:s,strides:i,pad:r,dimRoundingMode:a},m=Vt.runKernel("MaxPool3DGrad",f,g);return d?fe(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),Yn={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>Zn(t,s,i,r,a,o,l)}}};const Xn=Yt({maxPoolGrad_:function(t,e,n,s,i,r,a){const o=Jt(t,"dy","maxPoolGrad"),l=Jt(e,"input","maxPoolGrad"),u=Jt(n,"output","maxPoolGrad");B(l.rank===o.rank,(()=>`Rank of input (${l.rank}) does not match rank of dy (${o.rank})`)),B(4===o.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${o.rank}.`)),B(4===l.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${l.rank}.`)),null!=a&&B(j(r),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const h={dy:o,input:l,output:u},c={filterSize:s,strides:i,pad:r,dimRoundingMode:a};return Vt.runKernel("MaxPoolGrad",h,c)}}),Qn={kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>Xn(t,s,i,r,a,o)}}};const ts=Yt({complex_:function(t,e){const n=Jt(t,"real","complex"),s=Jt(e,"imag","complex");!function(t,e,n=""){B(U(t,e),(()=>n+` Shapes ${t} and ${e} must match`))}(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const i={real:n,imag:s};return Vt.runKernel("Complex",i)}});function es(t,e="float32"){if("complex64"===e){const e=es(t,"float32"),n=es(t,"float32");return ts(e,n)}const n=et(W(t),e);return Vt.makeTensor(n,t,e)}function ns(t,e="float32"){if("complex64"===e){const e=ns(t,"float32"),n=es(t,"float32");return ts(e,n)}const n=tt(W(t),e);return Vt.makeTensor(n,t,e)}const ss={kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=K(i,s.shape),a=W(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map((e=>t[e]))]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach((t=>{e[t]=1}));const n=fe(t,e);return se(Qt(n,ns(s.shape,"float32")),a)}}}},is={kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=qn(t,a,r,K(i,r.shape));return{x:()=>o.x()}}},rs={kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Qt(t,Xt(Be(n,s),"float32")),b:()=>Qt(t,Xt(On(n,s),"float32"))}}};const as=Yt({slice_:function(t,e,n){const s=Jt(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return Vt.runKernel(I,i,r)}}),os={kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>as(t,r,s.shape)}}};const ls=Yt({floor_:function(t){const e={x:Jt(t,"x","floor")};return Vt.runKernel(d,e)}}),us={kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{const e=pe(n.shape,i);return e.length>0?fe(ge(t,e),n.shape):t},b:()=>{const e=Qt(t,ie(ls(se(n,s)))),r=pe(s.shape,i);return r.length>0?fe(ge(e,r),s.shape):e}}}},hs={kernelName:w,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{const e=Qt(t,Xt(s,"float32")),r=pe(n.shape,i);return r.length>0?fe(ge(e,r),n.shape):e},b:()=>{const e=Qt(t,Xt(n,"float32")),r=pe(s.shape,i);return r.length>0?fe(ge(e,r),s.shape):e}}}},cs={kernelName:"Neg",gradFunc:t=>({x:()=>ie(t)})},ps={kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>es(n.shape,"float32")}}},ds={kernelName:"OnesLike",gradFunc:t=>({x:()=>be(t)})};const fs=Yt({unstack_:function(t,e=0){const n=Jt(t,"x","unstack","string_or_numeric");B(e>=-n.shape.length&&e<n.shape.length,(()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`));const s={value:n},i={axis:e};return Vt.runKernel(L,s,i)}}),gs={kernelName:k,saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return fs(t,s).map((t=>()=>t))}},ms={kernelName:v,inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>as(t,r,s.shape)}}};const ys=Yt({log_:function(t){const e={x:Jt(t,"x","log")};return Vt.runKernel("Log",e)}});const bs=Yt({pow_:function(t,e){let n=Jt(t,"base","pow"),s=Jt(e,"exp","pow");[n,s]=Ot(n,s);const i={a:n,b:s};return Vt.runKernel("Pow",i)}}),ws={kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=de(r.shape,a.shape);return{a:()=>{const e=Xt(a,"float32");let n=Qt(t,Qt(e,bs(r,ue(e,ae(1)))));const s=pe(r.shape,o);return s.length>0&&(n=ge(n,s)),fe(n,r.shape)},b:()=>{const e=On(r,0),n=je(e,ys(r),be(r));let s=Qt(t,Qt(i,n));const l=pe(a.shape,o);return l.length>0&&(s=ge(s,l)),fe(s,a.shape)}}}},ks={kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=On(n,0);return{x:()=>je(i,t,Qt(t,s)),alpha:()=>{let e=je(i,be(t),Qt(t,n));const r=pe(s.shape,t.shape);return r.length>0&&(e=ge(e,r)),fe(e,s.shape)}}}},vs={kernelName:c,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{const e=se(t,Xt(s,"float32")),r=pe(n.shape,i);return r.length>0?fe(ge(e,r),n.shape):e},b:()=>{let e=Qt(t,Xt(n,"float32"));const r=pe(s.shape,i);r.length>0&&(e=fe(ge(e,r),s.shape));const a=le(s);return ie(se(e,Xt(a,"float32")))}}}},Ss={kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,ie(le(n)))}}},xs={kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=Qt(Be(n,6),te(n));return{x:()=>Qt(t,Xt(s,"float32"))}}},Ns={kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,Xt(te(n),"float32"))}}},zs={kernelName:S,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>fe(t,n.shape)}}},Is={kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Vt.runKernel("ResizeBilinearGrad",i,n)}}},As={kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Vt.runKernel("ResizeNearestNeighborGrad",i,n)}}};const Cs=Yt({reverse_:function(t,e){const n={x:Jt(t,"x","reverse")},s={dims:e};return Vt.runKernel(x,n,s)}}),Ts={kernelName:x,gradFunc:(t,e,n)=>{const{dims:s}=n,i=K(s,t.shape);return{x:()=>Cs(t,i)}}},$s={kernelName:"Round",gradFunc:t=>({x:()=>be(t)})},Es={kernelName:N,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(se(t,Qt(bs(n,1.5),2)))}}};const Fs=Yt({logicalNot_:function(t){const e={x:Jt(t,"x","logicalNot","bool")};return Vt.runKernel("LogicalNot",e)}}),Ds={kernelName:z,inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>Xt(be(n),"float32"),t:()=>Qt(t,Xt(n,t.dtype)),e:()=>Qt(t,Xt(Fs(n),t.dtype))}}},Ls={kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=On(n,ae(0)),s=ae(1.7580993408473768),i=ae(1.0507009873554805),r=Qt(t,i),a=Qt(Qt(t,s),wn(Xt(n,"float32")));return je(e,r,a)}}}},_s={kernelName:C,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,Qt(n,ue(ae(1),n)))}}},Rs={kernelName:"Sign",gradFunc:t=>({x:()=>be(t)})};const Ms=Yt({cos_:function(t){const e={x:Jt(t,"x","cos")};return Vt.runKernel("Cos",e)}}),Os={kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(Ms(Xt(n,"float32")),t)}}};const Bs=Yt({cosh_:function(t){const e={x:Jt(t,"x","cosh")};return Vt.runKernel(u,e)}}),Ps={kernelName:A,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(Bs(Xt(n,"float32")),t)}}};const Ws=Yt({pad_:function(t,e,n=0){const s=Jt(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return Vt.runKernel(v,r,i)}});const Us={kernelName:I,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:r}=n,a=s.shape,[o,l]=function(t,e,n){let s;const i=t.shape.length;let r;return s="number"==typeof e?[e,...new Array(i-1).fill(0)]:e.length<i?e.concat(new Array(i-e.length).fill(0)):e.slice(),s.forEach((t=>{B(-1!==t,(()=>"slice() does not support negative begin indexing."))})),r=null==n?new Array(i).fill(-1):"number"==typeof n?[n,...new Array(i-1).fill(-1)]:n.length<i?n.concat(new Array(i-n.length).fill(-1)):n,r=r.map(((e,n)=>e>=0?e:(B(-1===e,(()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`)),t.shape[n]-s[n]))),[s,r]}(s,i,r),u=[];for(let e=0;e<t.rank;e++)u.push([o[e],a[e]-o[e]-l[e]]);return{x:()=>Ws(t,u)}}},js={kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=Qt(t,s);return{logits:()=>ue(r,Qt(ge(r,[i],true),s))}}};const Vs=Yt({sigmoid_:function(t){const e={x:Jt(t,"x","sigmoid")};return Vt.runKernel(C,e)}}),Ks={kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,Vs(n))}}};const qs=Yt({batchToSpaceND_:function(t,e,n){const s=Jt(t,"x","batchToSpaceND"),r=e.reduce(((t,e)=>t*e));B(s.rank>=1+e.length,(()=>`input rank is ${s.rank} but should be > than blockShape.length ${e.length}`)),B(n.length===e.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`)),B(s.shape[0]%r==0,(()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${r}`));const a={x:s},o={blockShape:e,crops:n};return Vt.runKernel(i,a,o)}}),Gs={kernelName:$,gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>qs(t,s,i)}}};const Hs=Yt({concat_:function(t,e=0){B(t.length>=1,(()=>"Pass at least one tensor to concat"));const n=Zt(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)})),1===n.length)return We(n[0]);const s=n,i={axis:e};return Vt.runKernel(a,s,i)}}),Js={kernelName:E,gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>Hs(t,s)}}},Zs={kernelName:T,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Qt(oe(Xt(n,"float32")),2))}}},Ys={kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(t,Qt(Xt(n,"float32"),2))}}},Xs={kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ae(2);return{a:()=>Qt(t,Qt(i,ue(n,s))),b:()=>Qt(t,Qt(i,ue(s,n)))}}},Qs={kernelName:M,gradFunc:t=>({x:()=>be(t)})},ti={kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=de(n.shape,s.shape);return{a:()=>{let e=t;const s=pe(n.shape,i);return s.length>0&&(e=ge(e,s)),fe(e,n.shape)},b:()=>{let e=t;const n=pe(s.shape,i);return n.length>0&&(e=ge(e,n)),fe(ie(e),s.shape)}}}},ei={kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;K(r,s.shape).forEach((t=>{i[t]=1}));const a=fe(t,i),o=Qt(a,ns(s.shape,"float32"));return{x:()=>o}}},ni={kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,le(Ms(n)))}}},si={kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Qt(ue(ae(1),le(n)),t)}}},ii={kernelName:F,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=be(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=Se(e,as(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=Se(e,as(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=Se(e,as(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=Se(e,as(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},ri={kernelName:D,gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=hn(i);return{x:()=>pn(t,r)}}};const ai=Yt({stack_:function(t,e=0){const n=Zt(t,"tensors","stack","string_or_numeric");B(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&B(e<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const s=n,i={axis:e};return Vt.runKernel(k,s,i)}}),oi={kernelName:L,gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>ai(t,i)}}};const li=Yt({expandDims_:function(t,e=0){const n=Jt(t,"x","expandDims","string_or_numeric");B(e<=n.rank,(()=>"Axis must be <= rank of the tensor"));const s={input:n},i={dim:e};return Vt.runKernel(p,s,i)}});const ui=Yt({gather_:function(t,e,n=0,s=0){const i={x:Jt(t,"x","gather"),indices:Jt(e,"indices","gather","int32")},r={axis:n,batchDims:s};return Vt.runKernel(g,i,r)}});const hi=Yt({maximum_:function(t,e){let n=Jt(t,"a","maximum"),s=Jt(e,"b","maximum");[n,s]=Ot(n,s),"bool"===n.dtype&&(n=Xt(n,"int32"),s=Xt(s,"int32")),de(n.shape,s.shape);const i={a:n,b:s};return Vt.runKernel(b,i)}});const ci=[ee,he,ce,me,ye,we,ke,ve,xe,Ne,ze,Ie,Ce,$e,Fe,Le,_e,Re,Me,Ve,Ke,Ge,tn,Xe,sn,an,ln,dn,mn,yn,vs,bn,kn,vn,Sn,xn,zn,Nn,Cn,$n,Dn,Ln,_n,Rn,Mn,Bn,Pn,Wn,Un,Vn,Gn,Gn,Jn,Yn,Qn,ss,is,rs,os,us,hs,cs,ps,ds,gs,ms,ms,ws,ks,Ss,xs,Ns,zs,Is,As,Ts,$s,Es,Ds,Ls,_s,Rs,Os,Ps,Us,js,Ks,Gs,Gs,Js,Js,Zs,Xs,Ys,Qs,ti,ei,ni,si,ii,ri,oi,{kernelName:_,inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=hi(e,be(e)),s=ui(t,n);let i=Oe(e,ae(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=li(i,t+1);i=Pe(i,ns(s.shape,"bool"));const a=be(s);return je(i,s,a)}(t,n)}}},{kernelName:R,gradFunc:t=>({x:()=>be(t)})}];for(const t of ci)mt(t);let pi;function di(){return null==pi&&(pi=e.backend().epsilon()),pi}class fi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,fi.prototype)}}class gi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,gi.prototype)}}class mi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,mi.prototype)}}class yi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,yi.prototype)}}class bi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,bi.prototype)}}function wi(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function ki(t,e){if(!t)throw new bi(e)}function vi(t,e){let n=0;for(const s of t)s===e&&n++;return n}function Si(t){return 1===t.length?t[0]:t}function xi(t){return Array.isArray(t)?t:[t]}function Ni(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function zi(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let Ii={};function Ai(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function Ci(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>Ci(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?Ci(e):t[n]=e.value)}}}function Ti(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in Ii)r=Ii[i];else if(r=e[i],null==r)throw new mi(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new mi(`${s}: Improper config format: ${JSON.stringify(r)}.\n'className' and 'config' must set.`);const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in Ii?[o,l]=Ii.className:a in e&&([o,l]=e[a]),null==o)throw new mi(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(Ii))t[e]=Ii[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},Ii);for(const t of Object.keys(n))Ii[t]=n[t];Ci(r.config);const s=l(o,r.config,n,i);return Ii=Object.assign({},e),s}{const t=Object.assign({},Ii);for(const t of Object.keys(n))Ii[t]=n[t];const e=new o(r.config);return Ii=Object.assign({},t),e}}}function $i(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function Ei(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function Fi(t){if(null==t)throw new mi(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function Di(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new mi(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function Li(t,e,n=0,s=1/0){return ki(n>=0),ki(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function _i(t,n){Array.isArray(t)?(e.util.assert(t.length>0,(()=>`${n} is unexpectedly an empty array.`)),t.forEach(((t,e)=>_i(t,`element ${e+1} of ${n}`)))):e.util.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${n} to be a positive integer, but got ${Ri(t)}.`))}function Ri(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>Ri(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function Mi(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function Oi(t,n){return e.tidy((()=>e.sqrt(e.sum(e.mul(t,t),n,!0))))}class Bi extends e.serialization.Serializable{getConfig(){return{}}}class Pi extends Bi{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>{const n=Oi(t,this.axis),s=e.clipByValue(n,0,this.maxValue);return e.mul(t,e.div(s,e.add(di(),n)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}Pi.className="MaxNorm",e.serialization.registerClass(Pi);class Wi extends Bi{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>e.div(t,e.add(di(),Oi(t,this.axis)))))}getConfig(){return{axis:this.axis}}}Wi.className="UnitNorm",e.serialization.registerClass(Wi);class Ui extends Bi{apply(t){return e.relu(t)}}Ui.className="NonNeg",e.serialization.registerClass(Ui);class ji extends Bi{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>{const n=Oi(t,this.axis),s=e.add(e.mul(this.rate,e.clipByValue(n,this.minValue,this.maxValue)),e.mul(1-this.rate,n));return e.mul(t,e.div(s,e.add(di(),n)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}ji.className="MinMaxNorm",e.serialization.registerClass(ji);const Vi={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function Ki(t){return Ai(t)}function qi(t,n={}){return Ti(t,e.serialization.SerializationMap.getMap().classNameMap,n,"constraint")}function Gi(t){if(null==t)return null;if("string"==typeof t){return qi({className:t in Vi?Vi[t]:t,config:{}})}return t instanceof Bi?t:qi(t)}var Hi=Object.freeze({__proto__:null,maxNorm:function(t){return new Pi(t)},unitNorm:function(t){return new Wi(t)},nonNeg:function(){return new Ui},minMaxNorm:function(t){return new ji(t)}});const Ji=["channelsFirst","channelsLast"],Zi=["nearest","bilinear"],Yi=["valid","same","causal"],Xi=["max","avg"],Qi=["sum","mul","concat","ave"],tr=new Map;function er(t){Di(Ji,"DataFormat",t)}function nr(t){Di(Yi,"PaddingMode",t)}function sr(t){Di(Xi,"PoolMode",t)}const ir=[];function rr(t,e){ir.push(t);try{const t=e();return ir.pop(),t}catch(t){throw ir.pop(),t}}function ar(t){if(!ur(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===ir.length?"":ir.join("/")+"/")+t}function or(t){if(!ur(t))throw new Error("Not a valid tensor name: '"+t+"'");tr.has(t)||tr.set(t,0);const e=tr.get(t);if(tr.set(t,tr.get(t)+1),e>0){const n=`${t}_${e}`;return tr.set(n,1),n}return t}const lr=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function ur(t){return!!t.match(lr)}function hr(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function cr(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s<e&&(e=s)}return e}function pr(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s>e&&(e=s)}return e}function dr(t,e){if(e<t)throw new mi(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function fr(t,n){return e.cast(t,n)}function gr(t,n=-1){const s=t.shape.slice();return n<0&&(n=s.length+n+1),s.splice(n,0,1),e.reshape(t,s)}function mr(t,n,s){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[n,0],[s,t.shape[1]]);case 3:return e.slice3d(t,[n,0,0],[s,t.shape[1],t.shape[2]]);case 4:return e.slice4d(t,[n,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3]]);case 5:return e.slice(t,[n,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return e.slice(t,[n,0,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new mi(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function yr(t,n,s){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[0,n],[t.shape[0],s]);case 3:return e.slice3d(t,[0,0,n],[t.shape[0],t.shape[1],s]);case 4:return e.slice4d(t,[0,0,0,n],[t.shape[0],t.shape[1],t.shape[2],s]);default:throw new mi(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function br(t,n,s,i){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:switch(i){case 1:return mr(t,n,s);case 2:return yr(t,n,s);default:throw new mi(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return mr(t,n,s);case 2:return e.slice3d(t,[0,n,0],[t.shape[0],s,t.shape[2]]);case 3:return yr(t,n,s);default:throw new mi(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return mr(t,n,s);case 2:return e.slice4d(t,[0,n,0,0],[t.shape[0],s,t.shape[2],t.shape[3]]);case 3:return e.slice4d(t,[0,0,n,0],[t.shape[0],t.shape[1],s,t.shape[3]]);case 4:return yr(t,n,s);default:throw new mi(`The axis is not within the rank of the tensor ${i}`)}default:throw new mi(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function wr(t,n=-1){let s;return n<0&&(s=t[0].rank,n=0!==s?s:0),n===t[0].rank&&(n=-1),e.concat(t,n)}function kr(t,n){switch(t.rank){case 1:return e.concat1d([t,n]);case 2:return e.concat2d([t,n],0);case 3:return e.concat3d([t,n],0);case 4:return e.concat4d([t,n],0);default:throw new mi(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function vr(t,n){if(Array.isArray(n)||(n=[n]),t.rank!==n.length)throw new mi(`The length of input n (${n.length}) does not match the number of dimensions in input x (${t.rank})`);return e.tile(t,n)}function Sr(t,n=0,s=1,i,r){return e.randomNormal(t,n,s,i,r)}function xr(t,n,s,i){if(t.rank<2||n.rank<2)throw new yi(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${n.shape}`);if(n.rank>=3){if(t.shape.slice(-1)[0]!==n.shape.slice(-2)[0])throw new yi(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${n.shape}`)}if(2===t.rank&&2===n.rank){const r=!1,a=!1;return e.fused.matMul({a:t,b:n,transposeA:r,transposeB:a,bias:i?Ir(t.rank,i,"channelsLast"):null,activation:s})}{const r=t.shape.slice(),a=r.pop();t=e.reshape(t,[-1,a]);const o=n.shape.slice(),l=o.pop(),u=o.pop(),h=[...o,l],c=Array.from({length:n.rank},((t,e)=>0===e?n.rank-2:e<=n.rank-2?e-1:e));n=e.reshape(e.transpose(n,c),[u,-1]);const p=[...r,...h],d=!1,f=!1;return e.reshape(e.fused.matMul({a:t,b:n,transposeA:d,transposeB:f,bias:i?Ir(t.rank,i,"channelsLast"):null,activation:s}),p)}}function Nr(t,n,s){return e.tidy((()=>(n=Array.isArray(n)?e.tensor1d(n,"int32"):e.cast(n,"int32"),e.gather(t,n,s))))}function zr(t){return e.mul(t,t)}function Ir(t,n,s){const i=n.shape;if(1!==n.rank&&n.rank!==t)throw new mi(`Unexpected bias dimensions: ${n.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===s)return 1===i.length?e.reshape(n,[1,i[0],1,1,1]):e.reshape(n,[1,i[3],i[0],i[1],i[2]]);if("channelsLast"===s)return 1===i.length?e.reshape(n,[1,1,1,1,i[0]]):e.reshape(n,[1].concat(i))}else if(4===t){if("channelsFirst"===s)return 1===i.length?e.reshape(n,[1,i[0],1,1]):e.reshape(n,[1,i[2],i[0],i[1]]);if("channelsLast"===s)return 1===i.length?e.reshape(n,[1,1,1,i[0]]):e.reshape(n,[1].concat(i))}else if(3===t){if("channelsFirst"===s)return 1===i.length?e.reshape(n,[1,i[0],1]):e.reshape(n,[1,i[1],i[0]]);if("channelsLast"===s)return 1===i.length?e.reshape(n,[1,1,i[0]]):e.reshape(n,[1].concat(i))}else if(t<3)return n;throw new mi(`Unsupported input rank by biasAdd: ${n.rank}`)}function Ar(t,n,s){return e.tidy((()=>(null==s&&(s="channelsLast"),er(s),e.add(t,Ir(t.rank,n,s)))))}function Cr(t,n,s,i){return e.tidy((()=>e.dropout(t,n,s,i)))}function Tr(t,e,n=!1){return n?t():e()}const $r=["fanIn","fanOut","fanAvg"],Er=["normal","uniform","truncatedNormal"];class Fr extends e.serialization.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class Dr extends Fr{apply(t,n){return e.zeros(t,n)}}Dr.className="Zeros",e.serialization.registerClass(Dr);class Lr extends Fr{apply(t,n){return e.ones(t,n)}}Lr.className="Ones",e.serialization.registerClass(Lr);class _r extends Fr{constructor(t){if(super(),"object"!=typeof t)throw new mi(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new mi(`config must have value set but got ${t}`);this.value=t.value}apply(t,n){return e.tidy((()=>e.mul(e.scalar(this.value),e.ones(t,n))))}getConfig(){return{value:this.value}}}_r.className="Constant",e.serialization.registerClass(_r);class Rr extends Fr{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,n){return e.randomUniform(t,this.minval,this.maxval,n)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}Rr.className="RandomUniform",e.serialization.registerClass(Rr);class Mr extends Fr{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new yi(`randomNormal does not support dType ${e}.`);return Sr(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Mr.className="RandomNormal",e.serialization.registerClass(Mr);class Or extends Fr{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,n){if("float32"!==(n=n||"float32")&&"int32"!==n)throw new yi(`truncatedNormal does not support dType ${n}.`);return e.truncatedNormal(t,this.mean,this.stddev,n,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Or.className="TruncatedNormal",e.serialization.registerClass(Or);class Br extends Fr{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,n){return e.tidy((()=>{if(2!==t.length||t[0]!==t[1])throw new mi("Identity matrix initializer can only be used for 2D square matrices.");return e.mul(this.gain,e.eye(t[0]))}))}getConfig(){return{gain:this.gain}}}Br.className="Identity",e.serialization.registerClass(Br);class Pr extends Fr{constructor(t){if(super(),t.scale<0)throw new mi(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,Di($r,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){Di(Er,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,n){const s=function(t,e="channelsLast"){let n,s;if(er(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=hr(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=hr(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=hr(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),i=s[0],r=s[1];let a=this.scale;if("fanIn"===this.mode?a/=Math.max(1,i):"fanOut"===this.mode?a/=Math.max(1,r):a/=Math.max(1,(i+r)/2),"normal"===this.distribution){const s=Math.sqrt(a);if("float32"!==(n=n||"float32")&&"int32"!==n)throw new yi(`${this.getClassName()} does not support dType ${n}.`);return e.truncatedNormal(t,0,s,n,this.seed)}{const s=Math.sqrt(3*a);return e.randomUniform(t,-s,s,n)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}Pr.className="VarianceScaling",e.serialization.registerClass(Pr);class Wr extends Pr{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Pr.className}}Wr.className="GlorotUniform",e.serialization.registerClass(Wr);class Ur extends Pr{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Pr.className}}Ur.className="GlorotNormal",e.serialization.registerClass(Ur);class jr extends Pr{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Pr.className}}jr.className="HeNormal",e.serialization.registerClass(jr);class Vr extends Pr{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Pr.className}}Vr.className="HeUniform",e.serialization.registerClass(Vr);class Kr extends Pr{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Pr.className}}Kr.className="LeCunNormal",e.serialization.registerClass(Kr);class qr extends Pr{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Pr.className}}qr.className="LeCunNormal",e.serialization.registerClass(qr);class Gr extends Fr{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new yi("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,n){return e.tidy((()=>{if(t.length<2)throw new yi("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const n=Sr(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let s=e.linalg.gramSchmidt(n);return t[0]>t[1]&&(s=e.transpose(s)),e.mul(this.gain,s)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}Gr.className="Orthogonal",e.serialization.registerClass(Gr);const Hr={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function Jr(t,n={}){return Ti(t,e.serialization.SerializationMap.getMap().classNameMap,n,"initializer")}function Zr(t){return Ai(t)}function Yr(t){if("string"==typeof t){const e=t in Hr?Hr[t]:t;if("GlorotNormal"===e)return new Ur;if("GlorotUniform"===e)return new Wr;if("HeNormal"===e)return new jr;if("HeUniform"===e)return new Vr;if("LeCunNormal"===e)return new Kr;if("LeCunUniform"===e)return new qr;{const t={};return t.className=e,t.config={},Jr(t)}}return t instanceof Fr?t:Jr(t)}var Xr=Object.freeze({__proto__:null,zeros:function(){return new Dr},ones:function(){return new Lr},constant:function(t){return new _r(t)},randomUniform:function(t){return new Rr(t)},randomNormal:function(t){return new Mr(t)},truncatedNormal:function(t){return new Or(t)},identity:function(t){return new Br(t)},varianceScaling:function(t){return new Pr(t)},glorotUniform:function(t){return new Wr(t)},glorotNormal:function(t){return new Ur(t)},heNormal:function(t){return new jr(t)},heUniform:function(t){return new Vr(t)},leCunNormal:function(t){return new Kr(t)},leCunUniform:function(t){return new qr(t)},orthogonal:function(t){return new Gr(t)}});let Qr=0;function ta(){return Qr++}const ea={};function na(t=""){return t in ea||(ea[t]=0),ea[t]+=1,t+ea[t].toString()}function sa(t){return Array.isArray(t)&&Array.isArray(t[0])}function ia(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function ra(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new mi(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function aa(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new mi(`Expected exactly 1 Shape; got ${t.length}`)}return t}function oa(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}const la="Variable";class ua{constructor(t,n="float32",s="Variable",i=!0,r=null){this.dtype=null==n?"float32":n,this.shape=t.shape,this.id=ta(),s=null==s?la:s,this.originalName=ar(s),this.name=or(this.originalName),this.trainable_=i,this.constraint=r,this.val=e.variable(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function ha(t){return t.map((t=>t.read()))}function ca(t){t.forEach((t=>{t[0].write(t[1])}))}class pa{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class da{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=ta(),null!=r&&(this.originalName=ar(r),this.name=or(this.originalName)),this.rank=e.length}}let fa=0;class ga{constructor(t,e){this.callArgs=e,this.id=fa++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let ma=0;class ya extends e.serialization.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=ma++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=Ni(t)+"_"+na(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new gi(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new mi(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return Si(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return Si(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new fi(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new fi(`Layer ${this.name} is not connected, no input to return.`);return Si(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new fi(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new fi(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return Si(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=xi(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=xi(this.inputSpec);if(t.length!==e.length)throw new mi(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new mi(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new mi(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new mi(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new mi(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new mi(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new mi(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=xi(t);let s=!0;for(const t of n)if(!(t instanceof da)){s=!1;break}let i=!0;for(const t of n)if(t instanceof da){i=!1;break}if(s===i)throw new mi("Arguments to apply() must be all SymbolicTensors or all Tensors");return rr(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of xi(t))e.push(n.shape);this.build(Si(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=xi(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=Si(r),null!=this.activityRegularizer)throw new yi("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=xi(t);const e=[];for(const n of t)e.push(n.shape);return Si(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new da(r,n,this,xi(t),e,this.name,s))):new da(r,s,this,xi(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new yi("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new fi(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new fi(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new gi(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return oa(this.weights)}build(t){this.built=!0}getWeights(t=!1){return ha(t?this.trainableWeights:this.weights)}setWeights(t){e.tidy((()=>{const n=this.weights;if(n.length!==t.length)throw new mi(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=ha(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.util.arraysEqual(a.shape,l.shape))throw new mi(`Layer weight shape ${a.shape} not compatible with provided weight shape ${l.shape}`);s.push([o,l])}ca(s)}))}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new mi(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=Yr("zeros"));const o=s.apply(e,n),l=new ua(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss((()=>i.apply(l.read()))),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=xi(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=xi(t);e=xi(e),n=xi(n),s=xi(s),i=ia(i),r=ia(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new ga({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function ba(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=ba(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class wa extends ya{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:na("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new mi("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new mi("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new mi("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new da(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new ga({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new mi(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function ka(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new mi("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new wa({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function va(t){if(null==t)return;const n=[],s=[],i=[];for(const e in t){const r=t[e];if("number"!=typeof r){const t=r;n.push(t.data()),s.push(e),i.push(t)}}if(n.length>0){const r=await Promise.all(n);for(let e=0;e<r.length;++e)t[s[e]]=r[e][0];e.dispose(i)}}function Sa(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var xa;wa.className="InputLayer",e.serialization.registerClass(wa),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(xa||(xa={}));class Na{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class za{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class Ia extends Na{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,n){null==n&&(n={});const s=null==n.size?0:n.size;this.seen+=s;for(const t in n){const i=n[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*s;else{let n;t in this.totals?n=this.totals[t]:this.totals[t]=0;const r=e.tidy((()=>e.add(this.totals[t],e.mul(i,s))));this.totals[t]=r,null!=n&&n.dispose()}}}async onEpochEnd(t,n){if(null!=n)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?n[t]=this.totals[t]/this.seen:e.tidy((()=>{const s=e.mul(e.div(1,this.seen),this.totals[t]);n[t]=s,this.totals[t].dispose(),e.keep(n[t])})))}}class Aa extends Na{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class Ca extends Na{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.util.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.util.now();return(...r)=>{const a=e.util.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,n,s){const i=[];null!=this.yield&&(await va(s),i.push(this.yield(t,n,s))),i.push(e.nextFrame()),await Promise.all(i)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await va(e),await this.epochBegin(t,e))}async onEpochEnd(t,n){const s=[];null!=this.epochEnd&&(await va(n),s.push(this.epochEnd(t,n))),"epoch"===this.yieldEvery&&s.push(e.nextFrame()),await Promise.all(s)}async onBatchBegin(t,e){null!=this.batchBegin&&(await va(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await va(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(e.nextFrame()):e.util.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await va(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await va(t),await this.trainEnd(t))}}function Ta(t,e){if(null==t&&(t={}),t instanceof Na)return[t];if(Array.isArray(t)&&t[0]instanceof Na)return t;return xi(t).map((t=>new Ca(t,e)))}class $a{constructor(){}static registerCallbackConstructor(t,n){e.util.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),$a.checkForDuplicate(n),null==$a.constructors[t]&&($a.constructors[t]=[]),$a.constructors[t].push(n)}static checkForDuplicate(t){for(const e in $a.constructors){$a.constructors[+e].forEach((e=>{if(e===t)throw new mi("Duplicate callback constructor.")}))}}static clear(){$a.constructors={}}static createCallbacks(t){const e=[];for(const n in $a.constructors){const s=+n;t>=s&&e.push(...$a.constructors[s])}return e.map((t=>new t))}}function Ea(t,e,n,s,i,r,a,o,l){const u=new Aa,h=[new Ia,...$a.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new za(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function Fa(t,n={},s=!1){return Ti(t,e.serialization.SerializationMap.getMap().classNameMap,n,"layer",s)}function Da(t,n){return e.tidy((()=>{"float32"!==t.dtype&&(t=e.cast(t,"float32"));const s=e.sum(zr(t),n,!0),i=e.fill(s.shape,di()),r=e.sqrt(e.maximum(s,i));return e.div(t,r)}))}function La(t,n){return e.tidy((()=>e.mean(zr(e.sub(n,t)),-1)))}function _a(t,n){return e.tidy((()=>e.mean(e.abs(e.sub(n,t)),-1)))}function Ra(t,n){return e.tidy((()=>{const s=e.sub(t,n),i=e.clipByValue(e.abs(t),di(),Number.MAX_VALUE),r=e.abs(e.div(s,i));return e.mul(100,e.mean(r,-1))}))}function Ma(t,n,s=!1){return e.tidy((()=>{if(s)n=e.softmax(n);else{const t=e.sum(n,n.shape.length-1,!0);n=e.div(n,t)}return n=e.clipByValue(n,di(),1-di()),e.neg(e.sum(e.mul(e.cast(t,"float32"),e.log(n)),n.shape.length-1))}))}function Oa(t,n,s=!1){return e.tidy((()=>{const i=e.cast(e.floor(function(t){const n=[hr(t.shape)];return e.reshape(t,n)}(t)),"int32"),r=(n=e.clipByValue(n,di(),1-di())).shape;return Ma(e.reshape(e.oneHot(i,r[r.length-1]),r),n,s)}))}function Ba(t,n){return e.tidy((()=>{let s;return s=e.clipByValue(n,di(),1-di()),s=e.log(e.div(s,e.sub(1,s))),e.mean(function(t,n){if(!e.util.arraysEqual(t.shape,n.shape))throw new mi(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return e.tidy((()=>{const s=e.relu(n),i=e.neg(e.abs(n));return e.add(e.sub(s,e.mul(n,t)),e.log1p(e.exp(i)))}))}(t,s),-1)}))}function Pa(t,n){return e.tidy((()=>{const s=Da(t,-1),i=Da(n,-1),r=e.mul(s,i);return e.neg(e.sum(r,-1))}))}$a.constructors={};const Wa={meanSquaredError:La,meanAbsoluteError:_a,meanAbsolutePercentageError:Ra,meanSquaredLogarithmicError:function(t,n){return e.tidy((()=>{const s=e.clipByValue(n,di(),Number.MAX_VALUE),i=e.log(e.add(1,s)),r=e.clipByValue(t,di(),Number.MAX_VALUE),a=e.log(e.add(1,r));return e.mean(zr(e.sub(i,a)),-1)}))},squaredHinge:function(t,n){return e.tidy((()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(zr(s),-1)}))},hinge:function(t,n){return e.tidy((()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(s,-1)}))},categoricalHinge:function(t,n){return e.tidy((()=>{const s=e.sum(e.mul(t,n),-1),i=e.max(e.mul(e.sub(1,t),n),-1);return e.maximum(0,e.add(1,e.sub(i,s)))}))},logcosh:function(t,n){return e.tidy((()=>{const s=Math.log(2),i=e.sub(n,t),r=e.sub(e.add(i,e.softplus(e.mul(-2,i))),s);return e.mean(r,-1)}))},categoricalCrossentropy:Ma,sparseCategoricalCrossentropy:Oa,binaryCrossentropy:Ba,kullbackLeiblerDivergence:function(t,n){return e.tidy((()=>{const s=e.clipByValue(t,di(),1),i=e.clipByValue(n,di(),1);return e.sum(e.mul(t,e.log(e.div(s,i))),-1)}))},poisson:function(t,n){return e.tidy((()=>{const s=e.log(e.add(di(),n));return e.mean(e.sub(n,e.mul(t,s)),-1)}))},cosineProximity:Pa};function Ua(t){if("string"==typeof t){if(t in Wa)return Wa[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new mi(e)}return t}function ja(t,n){return e.tidy((()=>{const s=e.mul(.5,e.onesLike(n)),i=fr(e.greater(n,s),t.dtype);return e.mean(e.equal(t,i),-1)}))}function Va(t,n){return e.tidy((()=>fr(e.equal(e.argMax(t,-1),e.argMax(n,-1)),"float32")))}function Ka(t,n){return e.tidy((()=>e.cast(e.sum(e.logicalAnd(e.equal(t,1),e.equal(n,1))),"float32")))}function qa(t,n){return e.tidy((()=>{const s=Ka(t,n),i=function(t,n){return e.tidy((()=>e.cast(e.sum(e.logicalAnd(e.equal(t,0),e.equal(n,1))),"float32")))}(t,n),r=e.add(s,i);return e.cast(e.where(e.greater(r,0),e.div(s,r),0),"float32")}))}function Ga(t,n){return e.tidy((()=>{const s=Ka(t,n),i=function(t,n){return e.tidy((()=>e.cast(e.sum(e.logicalAnd(e.equal(t,1),e.equal(n,0))),"float32")))}(t,n),r=e.add(s,i);return e.cast(e.where(e.greater(r,0),e.div(s,r),0),"float32")}))}function Ha(t,e){return Ba(t,e)}function Ja(t,n){return t.rank===n.rank&&(t=e.squeeze(t,[t.rank-1])),(n=e.argMax(n,-1)).dtype!==t.dtype&&(n=e.cast(n,t.dtype)),e.cast(e.equal(t,n),"float32")}const Za=Ma,Ya=Oa,Xa={binaryAccuracy:ja,categoricalAccuracy:Va,precision:qa,categoricalCrossentropy:Za,sparseCategoricalCrossentropy:Ya,mse:La,MSE:La,mae:_a,MAE:_a,mape:Ra,MAPE:Ra,cosine:Pa};function Qa(t){if("string"==typeof t&&t in Xa)return Xa[t];if("string"!=typeof t&&null!=t)return t;throw new mi(`Unknown metric ${t}`)}function to(t){if(ki(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const n of Object.keys(Wa))if(Wa[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(Xa))if(Xa[n]===t){e=n;break}return void 0!==e?e:t.name}}const eo=1048576;function no(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!so(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>eo&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function so(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!=typeof n)return!1;if(!so(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!so(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function io(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),ro(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?ao(o[t],n,s):oo(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?oa(t.collectedTrainableWeights):oa(t.trainableWeights);return e}(t),u=oa(t.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(e))}function ro(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function ao(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}ro([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function oo(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];ro([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)ro(["","","",r[t]],e,s)}function lo(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function uo(t,e){if(null===t)return null;if("string"==typeof t)return zi(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];lo(e,i,s)?n.push(s):n.push(uo(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=zi(n);e[t]=uo(s,t)}}return e}}function ho(t,e){if(null==t)return null;if("string"==typeof t)return Ni(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];lo(e,i,s)?n.push(s):n.push(ho(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=Ni(n);e[i]="name"!==n&&"className"!==n||"string"!=typeof s?ho(s,n):s}return e}}const co="3.8.0";class po{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof po)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,n,s){if(null!=this.id2Value[t.id])throw new mi(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,n){if(null==t.dtype||t.dtype===n.dtype)return n;try{return e.cast(n,t.dtype)}catch(e){throw new mi(`The dtype of the feed (${n.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,n),this.name2Id[t.name]=t.id,null!=s&&(this.id2Mask[t.id]=s),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof da){if(null==this.id2Value[t.id])throw new mi(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new mi(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof da){if(null==this.id2Value[t.id])throw new mi(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new mi(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&e.dispose(this.id2Mask)}}const fo={},go={};function mo(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map((t=>t.name)),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==fo[c]){const t=function(t,n){e.util.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const e=bo(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=bo(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:yo(i)}}(o,n);p=t.sorted,d=t.recipientCounts,fo[c]=p,go[c]=d}p=fo[c],d={},r||Object.assign(d,go[c]);const f=new po(n);for(let t=0;t<p.length;++t){if(null!=i){const t=e.memory().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const a=p[t],o=a.sourceLayer;if(o instanceof wa)continue;const h=[],c=[],g=[];let m=!1;for(const t of a.inputs){const e=f.getValue(t),s=f.getMask(t);h.push(e),c.push(s),null!=s&&(m=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||g.push(e))}m&&((s=s||{}).mask=c[0]);const y=xi(o.apply(h,s));let b=null;o.supportsMasking&&(b=o.computeMask(h,c));const w=wo(a),k=Array.isArray(w)?w:[w];for(let t=0;t<k.length;++t){f.hasKey(k[t])||f.add(k[t],y[t],Array.isArray(b)?b[0]:b);const e=l.indexOf(k[t].name);-1!==e&&(u[e]=y[t])}r||e.dispose(g)}return f.disposeMasks(),a?u:u[0]}function yo(t){const e={};for(const n in t)e[n]=t[n].size;return e}function bo(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function wo(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class ko extends ya{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=na(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],Ei(this.inputs).length!==this.inputs.length)throw new mi(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);Ei(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;ki(0===n,"input layer has >1 nodes"),ki(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof wa))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new gi(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(ko.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort($i);this.layers=[];for(const t of d){const e=p[t];e.sort(((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0}));for(const t of e)t instanceof ko&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map((t=>parseInt(t,10))).sort($i);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new gi(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${t.name}". The following previous layers were accessed without issue: ${g}`);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map((t=>t.name));for(const t of m){const e=m.filter((e=>e===t)).length;if(1!==e)throw new gi(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new ga({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new mi("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new mi(`Duplicate weight name: ${e.originalName}`);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new mi(`Provided weight data has no target variable: ${s}`);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new mi(`${t.length} of ${s} weights are not set: ${t}`)}ca(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.8.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=ho(this.updatedConfig());return e?JSON.stringify(n):n}call(t,n){return e.tidy((()=>{t=xi(t);const e=new po;for(let n=0;n<this.inputs.length;++n)e.add(this.inputs[n],t[n]);return mo(this.outputs,e,n)}))}computeMask(t,n){return e.tidy((()=>{let e;return t=xi(t),e=null==n?wi(null,t.length):xi(n),this.runInternalGraph(t,e)[1]}))}computeOutputShape(t){const e=ia(t);if(e.length!==this.inputLayers.length)throw new mi(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort($i);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=ia(e.computeOutputShape(Si(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];ki(e in n),i.push(n[e])}return Si(i)}runInternalGraph(t,e){null==e&&(e=wi(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort($i);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=xi(e.call(t,u)),l=xi(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map((t=>t[0])),a=r.map((t=>t[1])),null==u.mask&&(u.mask=a),o=xi(e.call(s,u)),l=xi(e.computeMask(s,a));if(e.activityRegularizer)throw new yi("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){ki(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof ko?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=ko.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new mi(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new mi("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new mi(`No such layer: ${t}`)}calculateLosses(){return e.tidy((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=ko.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=ko.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[ko.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=ko.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=ko.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(Si(n),s)}function l(t){const n=t.name,r=Fa(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new mi(`Corrupted configuration, expected array for nodeData: ${t}`);a(r,t)}))}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!Fi(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];ki(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];ki(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new mi("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){e.tidy((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function vo(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function So(t,n,s,i){if(null!=n||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=s){const n=e.tidy((()=>{if(1===t.shape.length)return e.clone(t);if(2===t.shape.length){if(t.shape[1]>1){const n=1;return e.argMax(t,n)}if(1===t.shape[1])return e.reshape(t,[t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await n.data());e.dispose(n);const r=[];return i.forEach((t=>{if(null==s[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(s[t])})),e.tensor1d(r,"float32")}return null}function xo(t,n){return e.mul(t,n)}function No(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.util.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${n}`));const a=zo("input",t.inputNames,s),o=zo("output",t.outputNames,i),l=a[0].shape[0];e.util.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),e.util.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let n=0;n<a.length;n++)e.util.assert(a[n].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let n=0;n<o.length;n++)e.util.assert(o[n].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function zo(t,n,s){if(s instanceof e.Tensor)return[s];if(Array.isArray(s))return e.util.assert(s.length===n.length,(()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`)),s;{const e=[];for(const i of n){if(null==s[i])throw new mi(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function Io(t,n,s){const i=null!=s.batchesPerEpoch;if(e.util.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),e.util.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),e.util.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),e.util.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),e.util.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(Ao(s.validationData))e.util.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new yi("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const c=Ta(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=Ea(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const h={};await d.onEpochBegin(g);let c=0,p=0;for(i||(m=await n.iterator());!i||c<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${c} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:i,ys:r}=No(t,n.value),a={};a.batch=p,a.size=i[0].shape[0],await d.onBatchBegin(p,a);const o=[];if(null!=s.classWeight){const e=vo(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)o.push(await So(r[t],null,e[t]))}const h=i.concat(r).concat(o),f=l(h);e.dispose(h);for(let t=0;t<u.length;++t){const n=u[t],s=f[t];a[n]=s,e.keep(s)}await d.onBatchEnd(p,a),Sa(a),p++,c++}if(i?c>=s.batchesPerEpoch:n.done){if(r){let e;e=Ao(s.validationData)?xi(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):xi(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let n=0;n<t.metricsNames.length;++n)h[`val_${t.metricsNames[n]}`]=e[n]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,h),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function Ao(t){return"function"==typeof t.iterator}function Co(t){e.util.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function To(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>mr(t,e,n-e))):mr(t,e,n-e)}function $o(t,n){return e.tidy((()=>null==t?null:Array.isArray(t)?t.map((t=>$o(t,n))):Nr(t,"int32"===n.dtype?n:e.cast(n,"int32"))))}function Eo(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function Fo(t,n,s,i={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let r,a,o,l,u,h,c;t.isTraining=!0;try{const p=null==i.batchSize?32:i.batchSize;Co(p);const d=!1,f=await t.standardizeUserData(n,s,i.sampleWeight,i.classWeight,d,p);r=f[0],a=f[1],c=f[2];let g,m=!1;if(null!=i.validationData&&i.validationData.length>0){if(m=!0,2!==i.validationData.length)throw 3===i.validationData.length?new yi("validationData including sample weights is not supported yet."):new mi(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${i.validationData} is invalid.`);o=i.validationData[0],l=i.validationData[1];const e=!0,n=await t.standardizeUserData(o,l,null,null,e,p);u=n[0],h=n[1],g=u.concat(h)}else if(null!=i.validationSplit&&i.validationSplit>0&&i.validationSplit<1){m=!0;const t=Math.floor(r[0].shape[0]*(1-i.validationSplit)),e=r[0].shape[0];u=To(r,t,e),r=To(r,0,t),h=To(a,t,e),a=To(a,0,t),g=u.concat(h)}else null!=i.validationSteps&&(m=!0);const y=r.concat(a).concat(c);t.checkTrainableWeightsConsistency();const b=t.makeTrainFunction(),w=t.getDedupedMetricsNames();let k,v;m?(t.makeTestFunction(),k=t.testFunction,v=w.slice().concat(w.map((t=>"val_"+t)))):(k=null,g=[],v=w.slice());const S=Ta(i.callbacks,i.yieldEvery);return await async function(t,n,s,i,r,a,o,l,u,h,c,p,d,f,g){null==r&&(r=32),null==a&&(a=1),null==c&&(c=!0),null==d&&(d=0);let m=!1;if(null!=u&&null!=h&&(m=!0),null!=g&&(m=!0,null==f))throw new mi("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const y=t.checkNumSamples(s,r,f,"steps_per_epoch");let b;null!=y&&(b=dr(0,y)),null==o&&(o=1);const{callbackList:w,history:k}=Ea(l,o,a,d,y,f,r,m,p);w.setModel(t),t.history=k,await w.onTrainBegin(),t.stopTraining_=!1;for(let o=d;o<a;++o){await w.onEpochBegin(o);const a={};if(null!=f)throw new yi("stepsPerEpoch mode is not implemented yet.");{if("batch"===c)throw new yi("batch shuffling is not implemneted yet");c&&e.util.shuffle(b);const o=e.tensor1d(b),l=Eo(y,r);for(let c=0;c<l.length;++c){const p={};if(await w.onBatchBegin(c,p),e.tidy((()=>{const d=l[c][0],f=l[c][1],g=mr(o,d,f-d);p.batch=c,p.size=f-d;const y=$o(s,g),b=n(y);for(let t=0;t<i.length;++t){const n=i[t],s=b[t];p[n]=s,e.keep(s)}if(c===l.length-1&&m){const n=t.testLoop(u,h,r);for(let t=0;t<i.length;++t){const s=i[t],r=n[t];e.keep(r),a["val_"+s]=r}}})),await w.onBatchEnd(c,p),Sa(p),t.stopTraining_)break}o.dispose()}if(await w.onEpochEnd(o,a),t.stopTraining_)break}return await w.onTrainEnd(),await t.history.syncData(),t.history}(t,b,y,w,p,i.epochs,i.verbose,S,k,g,i.shuffle,v,i.initialEpoch,null,null)}finally{t.isTraining=!1,Lo(r,n),Lo(a,s),Lo(u,o),Lo(h,l),null!=c&&e.dispose(c)}}function Do(t){const n=[];t instanceof e.Tensor&&(t=[t]);for(let e=0;e<t.length;++e){const s=t[e];if(1===s.rank)n.push(gr(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");n.push(s)}}return n}function Lo(t,n){if(null==t)return;const s=[];if(n instanceof e.Tensor)s.push(n.id);else if(Array.isArray(n))n.forEach((t=>s.push(t.id)));else if(null!=n)for(const t in n){const e=n[t];s.push(e.id)}const i=[];if(t instanceof e.Tensor)-1===s.indexOf(t.id)&&i.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===s.indexOf(t.id)&&i.push(t)}));else if(null!=t)for(const e in t){const n=t[e];-1===s.indexOf(n.id)&&i.push(n)}i.forEach((t=>{t.isDisposed||t.dispose()}))}function _o(t){return Array.isArray(t)}function Ro(t){return!function(t){return t instanceof e.Tensor}(t)&&!_o(t)}function Mo(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(_o(t)&&t.length>0)e=!0;else if(Ro(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new mi(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let r;if(Ro(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new mi(`No data provided for "${n}". Need data for each key in: ${e}`);r.push(t[n])}}else if(_o(t)){if((t=t).length!==e.length)throw new mi(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);r=t}else{if(t=t,e.length>1)throw new mi(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);r=[t]}if(r=Do(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new mi(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape ${a.shape}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new mi(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function Oo(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new mi(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new mi(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new mi(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape ${JSON.stringify(a.shape)}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new mi(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class Bo extends ko{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new mi("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");io(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const n={Adagrad:()=>e.train.adagrad(.01),Adadelta:()=>e.train.adadelta(1,.95,di()),Adam:()=>e.train.adam(.001,.9,.999,di()),Adamax:()=>e.train.adamax(.002,.9,.999,di(),0),RMSProp:()=>e.train.rmsprop(.001,.9,0,di()),SGD:()=>e.train.sgd(.01)};if(n.adagrad=n.Adagrad,n.adadelta=n.Adadelta,n.adam=n.Adam,n.adamax=n.Adamax,n.rmsprop=n.RMSProp,n.sgd=n.SGD,t in n)return n[t]();throw new mi(`Unknown Optimizer ${t}`)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof e.Optimizer))throw new mi("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let n=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new mi(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const e=t.loss;n=e.map((t=>Ua(t)))}else{const e=Ua(t.loss);this.outputs.forEach((t=>{n.push(e)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new mi(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const e of this.outputNames)null==t.loss[e]&&console.warn(`Output "${e}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${e} during training`),n.push(Ua(t.loss[e]))}this.lossFunctions=n,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],rr("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),r=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};rr("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;(e=>{let n,s,i;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let r;1===e[e.length-1]||this.lossFunctions[t]===Ba?-1!==["accuracy","acc"].indexOf(a)?s=ja:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ha):this.lossFunctions[t]===Oa?-1!==["accuracy","acc"].indexOf(a)?s=Ja:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ya):-1!==["accuracy","acc"].indexOf(a)?s=Va:-1!==["crossentropy","ce"].indexOf(a)&&(s=Za),-1!==["accuracy","acc"].indexOf(a)?r="acc":-1!==["crossentropy","ce"].indexOf(a)&&(r="ce"),i=s,n=""+r}else{const t=Qa(a);i=t,n=""+to(a)}let e;rr(n,(()=>{e=i})),r(t,n,e)}})(i[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;Co(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return Si(this.testLoop(a,r,s,n.verbose,n.steps))}finally{Lo(i[0],t),Lo(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,s){const i=null!=(s=s||{}).batches,r=t.testFunction;let a=[];if(s.verbose>0)throw new yi("Verbose mode is not implemented yet.");e.util.assert(!i||s.batches>0&&Number.isInteger(s.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(s.batches)}`));const o="function"==typeof n.next?n:await n.iterator();let l=0,u=0;for(;!i||u<s.batches;){const n=await o.next();if(a=e.tidy((()=>{if(n.value){const{xs:s,ys:i}=No(t,n.value),o=s.concat(i),h=e.tidy((()=>r(o)));if(e.dispose(o),0===u)for(let t=0;t<h.length;++t)a.push(e.scalar(0));const c=o[0].shape[0];for(let t=0;t<h.length;++t){const n=h[t],s=a[t];a[t]=e.tidy((()=>e.add(a[t],e.mul(c,n)))),u>0&&e.dispose(s)}e.dispose(h),l+=c,++u}return a})),n.done){i&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${s.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<a.length;++t){const n=a[t];a[t]=e.div(a[t],l),e.dispose(n)}return Si(a)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new mi(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new mi(`Either the input data should have a defined shape, or ${s} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,n){if(Array.isArray(n)&&0===n.length)throw new mi("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(n),i=s?n:[n],r=this.retrieveSymbolicTensors(i),a=new po;if(t instanceof e.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new mi(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)a.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new mi(`No value is provided for the model's input ${e.name}`);a.add(e,n)}const o=mo(r,a);return s?o:o[0]}retrieveSymbolicTensors(t){const e=wi(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map((t=>t.name));for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new mi(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return e}predictLoop(t,n=32,s=!1){return e.tidy((()=>{const i=this.checkNumSamples(t);if(s)throw new yi("Verbose predictLoop() is not implemented yet.");const r=Eo(i,n),a=this.outputs.map((t=>[]));for(let n=0;n<r.length;++n){e.tidy((()=>{const e=r[n][0],s=r[n][1],i=To(t,e,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new po(a);return mo(this.outputs,o)})).forEach(((t,e)=>a[e].push(t)))}return Si(a.map((t=>e.concat(t,0))))}))}predict(t,e={}){const n=Do(t);Oo(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return Co(s),this.predictLoop(n,s)}finally{Lo(n,t)}}predictOnBatch(t){Oo(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new gi("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===Oa?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=Ei(t.map((t=>t.shape[0])));i.sort();const r=Ei(n.map((t=>t.shape[0])));if(r.sort(),i.length>1)throw new mi(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(r.length>1)throw new mi(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(n.map((t=>t.shape)))}`);if(i.length>0&&r.length>0&&!e.util.arraysEqual(i,r))throw new mi(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=Mo(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=Mo(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[La,Ba,Ma];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===Ma&&1===r.shape[r.shape.length-1])throw new mi(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new mi(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new mi(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=vo(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await So(o[e],null,t[e]))}return[a,o,l]}testLoop(t,n,s,i=0,r){return e.tidy((()=>{const a=this.checkNumSamples(n,s,r,"steps"),o=[];if(i>0)throw new yi("Verbose mode is not implemented yet.");if(null!=r)throw new yi("steps mode in testLoop() is not implemented yet");{const i=Eo(a,s),r=e.tensor1d(dr(0,a));for(let s=0;s<i.length;++s){const a=i[s][0],l=i[s][1],u=mr(r,a,l-a),h=$o(n,u),c=t(h);if(0===s)for(let t=0;t<c.length;++t)o.push(e.scalar(0));for(let t=0;t<c.length;++t){const n=c[t];o[t]=e.add(o[t],e.mul(l-a,n))}}for(let t=0;t<o.length;++t)o[t]=e.div(o[t],a)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(vi(t,s)>1){i+=`_${vi(t.slice(0,n),s)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const n=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),a=[],o=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const o=new po(t),l=mo(this.outputs,o,{training:!0});let u;for(let t=0;t<this.lossFunctions.length;++t){let s=(0,this.lossFunctions[t])(i[t],l[t]);null!=r[t]&&(s=xo(s,r[t]));const a=e.mean(s);n.push(a),u=0===t?s:e.add(u,s)}for(let t=0;t<this.metricsTensors.length;++t){let s;if(this.outputs.length>1&&t<this.outputs.length)s=n[t];else{const n=this.metricsTensors[t][0],r=this.metricsTensors[t][1];s=e.mean(n(i[r],l[r]))}e.keep(s),a.push(s)}return u=e.mean(u),this.calculateLosses().forEach((t=>{u=e.add(u,t)})),u}),!0,o)].concat(a)}}makeTestFunction(){this.testFunction=t=>e.tidy((()=>{const n=[];let s;const i=t.slice(0,this.inputs.length),r=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let t=0;t<this.inputs.length;++t)a.push({key:this.inputs[t],value:i[t]});const o=new po(a),l=mo(this.outputs,o);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],a=e.mean(i(r[t],l[t]));s=0===t?a:e.add(s,a),n.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],a=e.mean(s(r[i],l[i]));n.push(a)}return n}))}async fit(t,e,n={}){return Fo(this,t,e,n)}async fitDataset(t,e){return Io(this,t,e)}async trainOnBatch(t,n){const s=await this.standardizeUserData(t,n),i=s[0],r=s[1],a=this.makeTrainFunction()(i.concat(r)),o=[];for(const t of a){const e=await t.data();o.push(e[0])}return e.dispose(a),Si(o)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const n=e.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=n-e.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=Ni(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>Ni(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=Ni(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[Ni(to(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>Ni(to(t))));{const t={};for(const e in this.metrics)t[e]=Ni(to(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=Fa(uo(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=zi(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>zi(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=zi(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>zi(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=zi(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,n){if("string"==typeof t){const n=e.io.getSaveHandlers(t);if(0===n.length)throw new mi(`Cannot find any save handlers for URL '${t}'`);if(n.length>1)throw new mi(`Found more than one (${n.length}) save handlers for URL '${t}'`);t=n[0]}if(null==t.save)throw new mi("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await e.io.encodeWeights(this.getNamedWeights(n)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.8.0",convertedBy:null};if(null!=n&&n.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:n,specs:r}=await e.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...r),s.data=e.io.concatenateArrayBuffers([s.data,n])}if(null!=this.userDefinedMetadata){const t=!0;no(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){no(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}Bo.className="Model",e.serialization.registerClass(Bo);class Po extends Bo{}async function Wo(t,n){if(null==n&&(n={}),"string"==typeof t){const s=e.io.getLoadHandlers(t,n);if(0===s.length)s.push(e.io.browserHTTPRequest(t,n));else if(s.length>1)throw new mi(`Found more than one (${s.length}) load handlers for URL '${t}'`);t=s[0]}return async function(t,n,s){null==s&&(s={});if(null==t.load)throw new mi("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const i=await t.load();let r=i.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==s.strict||s.strict,o=null!=i.weightData&&null!=i.weightSpecs&&a,l=Fa(uo(r),n,o),u=i.trainingConfig;null!=u&&l.loadTrainingConfig(u);null!=i.userDefinedMetadata&&l.setUserDefinedMetadata(i.userDefinedMetadata);if(null!=i.weightData){if(null==i.weightSpecs)throw new mi("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:n}=function(t,n){const s=e.io.decodeWeights(t,n),i={},r=[];return n.forEach((t=>{"optimizer"===t.group?r.push({name:t.name,tensor:s[t.name]}):i[t.name]=s[t.name]})),{modelWeights:i,optimizerWeights:r}}(i.weightData,i.weightSpecs);l.loadWeights(t,a),null!=l.optimizer&&n.length>0&&await l.optimizer.setWeights(n),e.dispose(t),e.dispose(n.map((t=>t.tensor)))}return l}(t,void 0,n)}Po.className="Functional",e.serialization.registerClass(Po);class Uo extends Bo{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:na("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new mi(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof Uo||t instanceof Bo;let n;if(e){if(n=t,1!==n.outputs.length)throw new mi("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new mi("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new mi("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=ka({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new mi(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new mi("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=ba(this.outputs[0])}this.inboundNodes=[],new ga({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:wi(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(aa(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new Bo({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new gi("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new gi("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new gi("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new gi("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new mi("Legacy serialization format not supported yet.");r=n}else e.util.assert(null!=n.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof Uo))throw new yi(`Sequential.fromConfig called on non-Sequential input: ${o}`);for(const t of r){const e=Fa(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new mi("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new mi("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function jo(t){return ka(t)}Uo.className="Sequential",e.serialization.registerClass(Uo);class Vo extends e.serialization.Serializable{getConfig(){return{}}}class Ko extends Vo{apply(t,n=1){return function(t,n=1){if(1!==n)throw new yi(`Support for alpha values other than 1 (${n}) is not implemented yet.`);return e.elu(t)}(t,n)}}Ko.className="elu",e.serialization.registerClass(Ko);class qo extends Vo{apply(t){return e.selu(t)}}qo.className="selu",e.serialization.registerClass(qo);class Go extends Vo{apply(t){return e.relu(t)}}Go.className="relu",e.serialization.registerClass(Go);class Ho extends Vo{apply(t){return e.tidy((()=>e.minimum(6,e.relu(t))))}}Ho.className="relu6",e.serialization.registerClass(Ho);class Jo extends Vo{apply(t){return t}}Jo.className="linear",e.serialization.registerClass(Jo);class Zo extends Vo{apply(t){return e.sigmoid(t)}}Zo.className="sigmoid",e.serialization.registerClass(Zo);class Yo extends Vo{apply(t){return function(t){return e.tidy((()=>{const n=e.add(.5,e.mul(.2,t));return e.clipByValue(n,0,1)}))}(t)}}Yo.className="hardSigmoid",e.serialization.registerClass(Yo);class Xo extends Vo{apply(t){return e.softplus(t)}}Xo.className="softplus",e.serialization.registerClass(Xo);class Qo extends Vo{apply(t){return function(t){return e.tidy((()=>e.div(t,e.add(e.abs(t),1))))}(t)}}Qo.className="softsign",e.serialization.registerClass(Qo);class tl extends Vo{apply(t){return e.tanh(t)}}tl.className="tanh",e.serialization.registerClass(tl);class el extends Vo{apply(t,n=-1){return e.softmax(t,n)}}el.className="softmax",e.serialization.registerClass(el);class nl extends Vo{apply(t,n=-1){return e.logSoftmax(t,n)}}nl.className="logSoftmax",e.serialization.registerClass(nl);class sl extends Vo{apply(t,n=1){return e.tidy((()=>e.mul(e.sigmoid(e.mul(t,n)),t)))}}sl.className="swish",e.serialization.registerClass(sl);class il extends Vo{apply(t){return e.tidy((()=>e.mul(t,e.tanh(e.softplus(t)))))}}function rl(t){return t.getClassName()}function al(t,n={}){return Ti(t,e.serialization.SerializationMap.getMap().classNameMap,n,"activation")}function ol(t){if(null==t){const t={className:"linear",config:{}};return al(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},al(e)}return t instanceof Vo?t:al(t)}function ll(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}il.className="mish",e.serialization.registerClass(il);class ul extends e.serialization.Serializable{}class hl extends ul{constructor(t){super(),ll(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return e.tidy((()=>{let n=e.zeros([1]);return this.hasL1&&(n=e.add(n,e.sum(e.mul(this.l1,e.abs(t))))),this.hasL2&&(n=e.add(n,e.sum(e.mul(this.l2,zr(t))))),e.reshape(n,[])}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}hl.className="L1L2",e.serialization.registerClass(hl);const cl={l1l2:"L1L2"};function pl(t){return Ai(t)}function dl(t,n={}){return Ti(t,e.serialization.SerializationMap.getMap().classNameMap,n,"regularizer")}function fl(t){if(null==t)return null;if("string"==typeof t){return dl({className:t in cl?cl[t]:t,config:{}})}return t instanceof ul?t:dl(t)}class gl extends ya{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,n){t=ra(t);let s=e.relu(t);return null!=this.maxValue&&(s=e.clipByValue(s,0,this.maxValue)),s}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}gl.className="ReLU",e.serialization.registerClass(gl);class ml extends ya{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=ra(t);return e.leakyRelu(s,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}ml.className="LeakyReLU",e.serialization.registerClass(ml);class yl extends ya{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=Yr(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=fl(t.alphaRegularizer),this.alphaConstraint=Gi(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new mi(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=aa(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new pa({ndim:t.length,axes:n})],this.built=!0}call(t,n){return t=ra(t),e.prelu(t,this.alpha.read())}getConfig(){const t={alphaInitializer:Zr(this.alphaInitializer),alphaRegularizer:pl(this.alphaRegularizer),alphaConstraint:Ki(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}yl.className="PReLU",e.serialization.registerClass(yl);class bl extends ya{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new yi(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=ra(t);return e.elu(s)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}bl.className="ELU",e.serialization.registerClass(bl);class wl extends ya{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,n){const s=ra(t);return e.mul(s,e.cast(e.greater(s,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}wl.className="ThresholdedReLU",e.serialization.registerClass(wl);class kl extends ya{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new el).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=ra(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function vl(t,e,n){if("number"==typeof t)return wi(t,e);if(t.length!==e)throw new mi(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new mi(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${r}`)}return t;var s}function Sl(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function xl(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+pr([n-e,0]);else{if("same"!==s)throw new mi(`Unsupport padding mode: ${s}.`);t*=e}return t}function Nl(t,n){return e.tidy((()=>(er(n),"channelsFirst"===n?e.transpose(t,[0,2,3,1]):t)))}function zl(t,n){return e.tidy((()=>(er(n),"channelsFirst"===n?e.transpose(t,[0,2,3,4,1]):t)))}function Il(t,n,s,i=[1,1],r="valid",a,o,l=null){return e.tidy((()=>{if(null==a&&(a="channelsLast"),er(a),3!==t.rank&&4!==t.rank)throw new mi(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==n.rank&&4!==n.rank)throw new mi(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let u=Nl(t,a);if("causal"===r)throw new yi("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return u=e.fused.conv2d({x:u,filter:n,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:s,activation:l}),"channelsFirst"===a&&(u=e.transpose(u,[0,3,1,2])),u}))}kl.className="Softmax",e.serialization.registerClass(kl);class Al extends ya{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",Al.verifyArgs(e),this.rank=t,_i(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new yi(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=vl(e.kernelSize,t,"kernelSize"),this.strides=vl(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,nr(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,er(this.dataFormat),this.activation=ol(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=Yr(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=Gi(e.biasConstraint),this.biasRegularizer=fl(e.biasRegularizer),this.activityRegularizer=fl(e.activityRegularizer),this.dilationRate=vl(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new mi(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new mi(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new mi(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(ki("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!Li(t.kernelSize,"number",1,3))throw new mi(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:rl(this.activation),useBias:this.useBias,biasInitializer:Zr(this.biasInitializer),biasRegularizer:pl(this.biasRegularizer),activityRegularizer:pl(this.activityRegularizer),biasConstraint:Ki(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class Cl extends Al{constructor(t,e){super(t,e),this.kernel=null,Cl.verifyArgs(e),this.filters=e.filters,_i(this.filters,"filters"),this.kernelInitializer=Yr(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=Gi(e.kernelConstraint),this.kernelRegularizer=fl(e.kernelRegularizer)}build(t){t=aa(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new mi(`The channel dimension of the input should be defined. Found ${t[e]}`);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,n){return e.tidy((()=>{let n;t=ra(t);const s=null==this.bias?null:this.bias.read(),i=Mi(this.activation.getClassName());if(null!=i&&2===this.rank)n=Il(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)n=function(t,n,s,i=1,r="valid",a,o=1){return e.tidy((()=>{if(null==a&&(a="channelsLast"),er(a),3!==t.shape.length)throw new mi(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==n.shape.length)throw new mi(`The kernel for a conv1dWithBias operation should be 3, but is ${n.shape.length} instead`);if(null!=s&&1!==s.shape.length)throw new mi(`The bias for a conv1dWithBias operation should be 1, but is ${n.shape.length} instead`);if("channelsFirst"===a&&(t=e.transpose(t,[0,2,1])),"causal"===r)throw new yi("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let l=e.conv1d(t,n,i,"same"===r?"same":"valid","NWC",o);return null!=s&&(l=Ar(l,s)),l}))}(t,this.kernel.read(),s,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)n=Il(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new yi("convolutions greater than 3D are not implemented yet.");n=function(t,n,s,i=[1,1,1],r="valid",a,o){return e.tidy((()=>{if(null==a&&(a="channelsLast"),er(a),4!==t.rank&&5!==t.rank)throw new mi(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==n.rank&&5!==n.rank)throw new mi(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let l=zl(t,a);if("causal"===r)throw new yi("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return l=e.conv3d(l,n,i,"same"===r?"same":"valid","NDHWC",o),null!=s&&(l=Ar(l,s)),"channelsFirst"===a&&(l=e.transpose(l,[0,4,1,2,3])),l}))}(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(n=this.activation.apply(n))}return n}))}computeOutputShape(t){t=aa(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=Sl(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:Zr(this.kernelInitializer),kernelRegularizer:pl(this.kernelRegularizer),kernelConstraint:Ki(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new mi(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class Tl extends Cl{constructor(t){super(2,t),Tl.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Li(t.kernelSize,"number",1,2))throw new mi(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}Tl.className="Conv2D",e.serialization.registerClass(Tl);class $l extends Cl{constructor(t){super(3,t),$l.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new mi(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}$l.className="Conv3D",e.serialization.registerClass($l);class El extends Tl{constructor(t){if(super(t),this.inputSpec=[new pa({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new mi(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=aa(t)).length)throw new mi("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new mi("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new pa({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n=ra(t);if(4!==n.shape.length)throw new mi(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const s=n.shape,i=s[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const o=s[r],l=s[a],u=this.kernelSize[0],h=this.kernelSize[1],c=this.strides[0],p=this.strides[1],d=[i,xl(o,c,u,this.padding),xl(l,p,h,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,1]));let f=e.conv2dTranspose(n,this.kernel.read(),d,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(f=e.transpose(f,[0,3,1,2])),null!=this.bias&&(f=Ar(f,this.bias.read(),this.dataFormat)),null!=this.activation&&(f=this.activation.apply(f)),f}))}computeOutputShape(t){const e=(t=aa(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=xl(e[s],o,r,this.padding),e[i]=xl(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}El.className="Conv2DTranspose",e.serialization.registerClass(El);class Fl extends $l{constructor(t){if(super(t),this.inputSpec=[new pa({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new mi(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=aa(t)).length)throw new mi("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new mi("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new pa({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n=ra(t);if(5!==n.shape.length)throw new mi(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const s=n.shape,i=s[0];let r,a,o;"channelsFirst"===this.dataFormat?(o=2,r=3,a=4):(o=1,r=2,a=3);const l=s[o],u=s[r],h=s[a],c=this.kernelSize[0],p=this.kernelSize[1],d=this.kernelSize[2],f=this.strides[0],g=this.strides[1],m=this.strides[2],y=[i,xl(l,f,c,this.padding),xl(u,g,p,this.padding),xl(h,m,d,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,4,1]));let b=e.conv3dTranspose(n,this.kernel.read(),y,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(b=e.transpose(b,[0,4,1,2,3])),null!==this.bias&&(b=Ar(b,this.bias.read(),this.dataFormat)),null!==this.activation&&(b=this.activation.apply(b)),b}))}computeOutputShape(t){const e=(t=aa(t)).slice();let n,s,i,r;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,r=4):(n=4,s=1,i=2,r=3);const a=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[n]=this.filters,e[s]=xl(e[s],u,a,this.padding),e[i]=xl(e[i],h,o,this.padding),e[r]=xl(e[r],c,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Fl.className="Conv3DTranspose",e.serialization.registerClass(Fl);class Dl extends Cl{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new mi("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new mi("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new mi(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=Yr(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=fl(e.depthwiseRegularizer),this.depthwiseConstraint=Gi(e.depthwiseConstraint),this.pointwiseInitializer=Yr(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=fl(e.pointwiseRegularizer),this.pointwiseConstraint=Gi(e.pointwiseConstraint)}build(t){if((t=aa(t)).length<this.rank+2)throw new mi(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new mi(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new pa({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n;if(t=ra(t),1===this.rank)throw new yi("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=e.transpose(t,[0,2,3,1])),n=e.separableConv2d(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(n=Ar(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),"channelsFirst"===this.dataFormat&&(n=e.transpose(n,[0,3,1,2])),n}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=Zr(this.depthwiseInitializer),t.pointwiseInitializer=Zr(this.pointwiseInitializer),t.depthwiseRegularizer=pl(this.depthwiseRegularizer),t.pointwiseRegularizer=pl(this.pointwiseRegularizer),t.depthwiseConstraint=Ki(this.depthwiseConstraint),t.pointwiseConstraint=Ki(this.pointwiseConstraint),t}}Dl.className="SeparableConv";class Ll extends Dl{constructor(t){super(2,t)}}Ll.className="SeparableConv2D",e.serialization.registerClass(Ll);class _l extends Cl{constructor(t){super(1,t),_l.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Li(t.kernelSize,"number",1,1))throw new mi(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}_l.className="Conv1D",e.serialization.registerClass(_l);class Rl extends ya{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,n){return e.tidy((()=>{if(t=ra(t),"channelsLast"===this.dataFormat){const e=br(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return br(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=br(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return br(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Rl.className="Cropping2D",e.serialization.registerClass(Rl);class Ml extends ya{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,er(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,Di(Zi,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,n){return e.tidy((()=>{let n=ra(t);const s=n.shape;if("channelsFirst"===this.dataFormat){n=e.transpose(n,[0,2,3,1]);const t=this.size[0]*s[2],i=this.size[1]*s[3],r="nearest"===this.interpolation?e.image.resizeNearestNeighbor(n,[t,i]):e.image.resizeBilinear(n,[t,i]);return e.transpose(r,[0,3,1,2])}{const t=this.size[0]*s[1],i=this.size[1]*s[2];return"nearest"===this.interpolation?e.image.resizeNearestNeighbor(n,[t,i]):e.image.resizeBilinear(n,[t,i])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Ml.className="UpSampling2D",e.serialization.registerClass(Ml);class Ol extends Al{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=Yr(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=Gi(t.depthwiseConstraint),this.depthwiseRegularizer=fl(t.depthwiseRegularizer)}build(t){if((t=aa(t)).length<4)throw new mi(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new mi(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{let n=function(t,n,s=[1,1],i="valid",r,a){return e.tidy((()=>{null==r&&(r="channelsLast"),er(r);let o=Nl(t,r);if(4!==t.rank)throw new mi(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==n.rank)throw new mi(`depthwiseKernel is required to be 4-D, but is instead ${n.rank}-D`);return o=e.depthwiseConv2d(o,n,s,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o}))}(t=ra(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(n=Ar(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),n}))}computeOutputShape(t){t=aa(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=Sl(e,this.kernelSize[0],this.padding,this.strides[0]),r=Sl(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=Zr(this.depthwiseInitializer),t.depthwiseRegularizer=pl(this.depthwiseRegularizer),t.depthwiseConstraint=Ki(this.depthwiseRegularizer),t}}function Bl(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new mi("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function Pl(t,n,s,i=!1,r,a,o=!1,l=!1){return e.tidy((()=>{const u=n.shape.length;if(u<3)throw new mi(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(dr(2,u));if(n=e.transpose(n,h),null!=a)throw new yi("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=e.cast(e.cast(r,"bool"),"float32")).rank===u-1&&(r=e.expandDims(r,-1)),r=e.transpose(r,h)),i&&(n=e.reverse(n,0),null!=r&&(r=e.reverse(r,0)));const c=[];let p,d=s;const f=n.shape[0],g=e.unstack(n);let m,y;null!=r&&(m=e.unstack(r));for(let n=0;n<f;++n){const s=g[n],i=e.tidy((()=>t(s,d)));if(null==r)p=i[0],d=i[1];else{const t=e.tidy((()=>{const t=m[n],s=e.sub(e.onesLike(t),t);return{output:e.add(e.mul(i[0],t),e.mul(d[0],s)),newStates:d.map(((n,r)=>e.add(e.mul(i[1][r],t),e.mul(n,s))))}}));p=t.output,d=t.newStates}l&&c.push(p)}if(l){const t=1;y=e.stack(c,t)}return[p,y,d]}))}Ol.className="DepthwiseConv2D",e.serialization.registerClass(Ol);class Wl extends ya{constructor(t){let e;if(super(t),null==t.cell)throw new mi("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new Jl({cells:t.cell}):t.cell,null==e.stateSize)throw new mi("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new pa({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return dr(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){sa(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,n){return e.tidy((()=>{Array.isArray(n)&&(n=n[0]);const t=this.returnSequences?n:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new yi("Constants support is not implemented in RNN yet.");sa(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new pa({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.util.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new mi(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new pa({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,n=!1){e.tidy((()=>{if(!this.stateful)throw new fi("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new mi("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>e.zeros([s,t]))):this.states_=[e.zeros([s,this.cell.stateSize])];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>e.zeros([s,t]))):this.states_[0]=e.zeros([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new mi(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.util.arraysEqual(i.shape,a))throw new mi(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map((t=>e.keep(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Bl(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new pa({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof da){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy((()=>{const e=null==n?null:n.mask,s=null==n?null:n.training;let i=null==n?null:n.initialState;t=ra(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new mi(`RNN Layer has ${r} state(s) but was passed ${i.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=Pl(((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]}),t,i,this.goBackwards,e,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c}))}getInitialState(t){return e.tidy((()=>{let n=e.zeros(t.shape);return n=e.sum(n,[1,2]),n=gr(n),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?vr(n,[1,t]):n)):this.cell.stateSize>1?[vr(n,[1,this.cell.stateSize])]:[n]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===Wl.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=Fa(e.cell,n);return new t(Object.assign(e,{cell:s}))}}Wl.className="RNN",e.serialization.registerClass(Wl);class Ul extends ya{}class jl extends Ul{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,_i(this.units,"units"),this.activation=ol(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Yr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Yr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Yr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=fl(t.kernelRegularizer),this.recurrentRegularizer=fl(t.recurrentRegularizer),this.biasRegularizer=fl(t.biasRegularizer),this.kernelConstraint=Gi(t.kernelConstraint),this.recurrentConstraint=Gi(t.recurrentConstraint),this.biasConstraint=Gi(t.biasConstraint),this.dropout=cr([1,pr([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=cr([1,pr([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=aa(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{if(2!==(t=t).length)throw new mi(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let s=t[1];t=t[0];const i=null!=n.training&&n.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Zl({ones:()=>e.onesLike(t),rate:this.dropout,training:i})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Zl({ones:()=>e.onesLike(s),rate:this.recurrentDropout,training:i}));const a=this.dropoutMask,o=this.recurrentDropoutMask;r=xr(null!=a?e.mul(t,a):t,this.kernel.read()),null!=this.bias&&(r=Ar(r,this.bias.read())),null!=o&&(s=e.mul(s,o));let l=e.add(r,xr(s,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:rl(this.activation),useBias:this.useBias,kernelInitializer:Zr(this.kernelInitializer),recurrentInitializer:Zr(this.recurrentInitializer),biasInitializer:Zr(this.biasInitializer),kernelRegularizer:pl(this.kernelRegularizer),recurrentRegularizer:pl(this.recurrentRegularizer),biasRegularizer:pl(this.biasRegularizer),activityRegularizer:pl(this.activityRegularizer),kernelConstraint:Ki(this.kernelConstraint),recurrentConstraint:Ki(this.recurrentConstraint),biasConstraint:Ki(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}jl.className="SimpleRNNCell",e.serialization.registerClass(jl);class Vl extends Wl{constructor(t){t.cell=new jl(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return new t(e)}}Vl.className="SimpleRNN",e.serialization.registerClass(Vl);class Kl extends Ul{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new mi("GRUCell does not support reset_after parameter set to true.");this.units=t.units,_i(this.units,"units"),this.activation=ol(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=ol(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Yr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Yr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Yr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=fl(t.kernelRegularizer),this.recurrentRegularizer=fl(t.recurrentRegularizer),this.biasRegularizer=fl(t.biasRegularizer),this.kernelConstraint=Gi(t.kernelConstraint),this.recurrentConstraint=Gi(t.recurrentConstraint),this.biasConstraint=Gi(t.biasConstraint),this.dropout=cr([1,pr([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=cr([1,pr([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=aa(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{if(2!==(t=t).length)throw new mi(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const s=null!=n.training&&n.training;let i=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Zl({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Zl({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:3}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let o,l,u;0<this.dropout&&this.dropout<1&&(t=e.mul(t,r[0]));let h=xr(t,this.kernel.read());this.useBias&&(h=Ar(h,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,a[0]));const c=this.recurrentKernel.read(),[p,d]=e.split(c,[2*this.units,this.units],c.rank-1),f=xr(i,p),[g,m,y]=e.split(h,3,h.rank-1),[b,w]=e.split(f,2,f.rank-1);o=this.recurrentActivation.apply(e.add(g,b)),l=this.recurrentActivation.apply(e.add(m,w));const k=xr(e.mul(l,i),d);u=this.activation.apply(e.add(y,k));const v=e.add(e.mul(o,i),e.mul(e.add(1,e.neg(o)),u));return[v,v]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:rl(this.activation),recurrentActivation:rl(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Zr(this.kernelInitializer),recurrentInitializer:Zr(this.recurrentInitializer),biasInitializer:Zr(this.biasInitializer),kernelRegularizer:pl(this.kernelRegularizer),recurrentRegularizer:pl(this.recurrentRegularizer),biasRegularizer:pl(this.biasRegularizer),activityRegularizer:pl(this.activityRegularizer),kernelConstraint:Ki(this.kernelConstraint),recurrentConstraint:Ki(this.recurrentConstraint),biasConstraint:Ki(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}Kl.className="GRUCell",e.serialization.registerClass(Kl);class ql extends Wl{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Kl(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}ql.className="GRU",e.serialization.registerClass(ql);class Gl extends Ul{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,_i(this.units,"units"),this.activation=ol(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=ol(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Yr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Yr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Yr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=fl(t.kernelRegularizer),this.recurrentRegularizer=fl(t.recurrentRegularizer),this.biasRegularizer=fl(t.biasRegularizer),this.kernelConstraint=Gi(t.kernelConstraint),this.recurrentConstraint=Gi(t.recurrentConstraint),this.biasConstraint=Gi(t.biasConstraint),this.dropout=cr([1,pr([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=cr([1,pr([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=aa(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends Fr{apply(e,s){const i=t.apply([n]),r=(new Lr).apply([n]),a=t.apply([2*n]);return kr(kr(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,n){return e.tidy((()=>{const s=null!=n.training&&n.training;if(3!==(t=t).length)throw new mi(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let i=t[1];const r=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Zl({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Zl({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:4}));const a=this.dropoutMask,o=this.recurrentDropoutMask;let l,u,h,c;0<this.dropout&&this.dropout<1&&(t=e.mul(t,a[0]));let p=xr(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,o[0])),p=e.add(p,xr(i,this.recurrentKernel.read())),this.useBias&&(p=Ar(p,this.bias.read()));const[d,f,g,m]=e.split(p,4,p.rank-1);l=this.recurrentActivation.apply(d),u=this.recurrentActivation.apply(f),h=e.add(e.mul(u,r),e.mul(l,this.activation.apply(g))),c=this.recurrentActivation.apply(m);const y=e.mul(c,this.activation.apply(h));return[y,y,h]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:rl(this.activation),recurrentActivation:rl(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Zr(this.kernelInitializer),recurrentInitializer:Zr(this.recurrentInitializer),biasInitializer:Zr(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:pl(this.kernelRegularizer),recurrentRegularizer:pl(this.recurrentRegularizer),biasRegularizer:pl(this.biasRegularizer),activityRegularizer:pl(this.activityRegularizer),kernelConstraint:Ki(this.kernelConstraint),recurrentConstraint:Ki(this.recurrentConstraint),biasConstraint:Ki(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}Gl.className="LSTMCell",e.serialization.registerClass(Gl);class Hl extends Wl{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Gl(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Hl.className="LSTM",e.serialization.registerClass(Hl);class Jl extends Ul{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,n){return e.tidy((()=>{let e=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(e.splice(0,t.stateSize.length)):s.push(e.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];e=s[a],r=0===a?[t[0]].concat(e):[r[0]].concat(e),r=o.call(r,n),i.push(r.slice(1))}e=[];for(const t of i.slice().reverse())e.push(...t);return[r[0]].concat(e)}))}build(t){let e;sa(t)&&(t=t[0]),t=t,this.cells.forEach(((n,s)=>{rr(`RNNCell_${s}`,(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(Fa(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return ha(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}ca(e)}}function Zl(t){const{ones:n,rate:s,training:i=!1,count:r=1}=t,a=()=>Cr(n(),s),o=()=>Tr(a,n,i);if(!r||r<=1)return e.keep(o().clone());return Array(r).fill(void 0).map(o).map((t=>e.keep(t.clone())))}Jl.className="StackedRNNCells",e.serialization.registerClass(Jl);class Yl extends Wl{constructor(t){if(t.unroll)throw new yi("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new yi("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new pa({ndim:5})]}call(t,n){return e.tidy((()=>{if(null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),n&&n.constants)throw new mi("ConvRNN2D cell does not support constants");const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return e.tidy((()=>{const{stateSize:n}=this.cell,s=t.shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)],a=e.zeros(r);return Array.isArray(n)?Array(n.length).fill(a):[a]}))}resetStates(t,n=!1){e.tidy((()=>{if(!this.stateful)throw new fi("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new mi("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>e.zeros(r))):this.states_=[e.zeros(r)];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>e.zeros(r))):this.states_[0]=e.zeros(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new mi(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.util.arraysEqual(s.shape,i))throw new mi(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map((t=>e.keep(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=Sl(l,s[0],i,r[0],a[0]),c=Sl(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Yl.className="ConvRNN2D";class Xl extends Gl{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,_i(this.filters,"filters"),this.kernelSize=vl(n,2,"kernelSize"),this.kernelSize.forEach((t=>_i(t,"kernelSize"))),this.strides=vl(s||1,2,"strides"),this.strides.forEach((t=>_i(t,"strides"))),this.padding=i||"valid",nr(this.padding),this.dataFormat=r||"channelsLast",er(this.dataFormat),this.dilationRate=vl(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>_i(t,"dilationRate")))}build(t){var n;t=aa(t);const s="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[s])throw new mi(`The channel dimension of the input should be defined. Found ${t[s]}`);const i=t[s],r=this.kernelSize.concat([i,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const a=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",a,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const s=this.biasInitializer,i=this.filters;t=new((n=class extends Fr{apply(t,n){return wr([s.apply([i]),e.ones([i]),s.apply([2*i])])}}).className="CustomInit",n)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,n){return e.tidy((()=>{if(3!==t.length)throw new mi(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const s=n.training||!1,i=t[0],r=t[1],a=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Zl({ones:()=>e.onesLike(i),rate:this.dropout,training:s,count:4}));const o=this.dropoutMask,l=(t,n,s)=>n&&n[s]?e.mul(n[s],t):t;let u=l(i,o,0),h=l(i,o,1),c=l(i,o,2),p=l(i,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Zl({ones:()=>e.onesLike(r),rate:this.recurrentDropout,training:s,count:4}));const d=this.recurrentDropoutMask;let f=l(r,d,0),g=l(r,d,1),m=l(r,d,2),y=l(r,d,3);const[b,w,k,v]=e.split(this.kernel.read(),4,3),[S,x,N,z]=this.useBias?e.split(this.bias.read(),4):[null,null,null,null];u=this.inputConv(u,b,S,this.padding),h=this.inputConv(h,w,x,this.padding),c=this.inputConv(c,k,N,this.padding),p=this.inputConv(p,v,z,this.padding);const[I,A,C,T]=e.split(this.recurrentKernel.read(),4,3);f=this.recurrentConv(f,I),g=this.recurrentConv(g,A),m=this.recurrentConv(m,C),y=this.recurrentConv(y,T);const $=this.recurrentActivation.apply(e.add(u,f)),E=this.recurrentActivation.apply(e.add(h,g)),F=e.add(e.mul(E,a),e.mul($,this.activation.apply(e.add(c,m)))),D=e.mul(this.recurrentActivation.apply(e.add(p,y)),this.activation.apply(F));return[D,D,F]}))}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,n,s,i){const r=e.conv2d(t,n,this.strides,i||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return s?Ar(r,s,this.dataFormat):r}recurrentConv(t,n){return e.conv2d(t,n,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Xl.className="ConvLSTM2DCell",e.serialization.registerClass(Xl);class Ql extends Yl{constructor(t){const e=new Xl(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}Ql.className="ConvLSTM2D",e.serialization.registerClass(Ql);class tu extends ya{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=ra(t);if(0<this.rate&&this.rate<1){const t=null!=n.training&&n.training,s=this.getNoiseShape(e);return Tr((()=>Cr(e,this.rate,s,this.seed)),(()=>e),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}tu.className="Dropout",e.serialization.registerClass(tu);class eu extends tu{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}eu.className="SpatialDropout1D",e.serialization.registerClass(eu);class nu extends ya{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,_i(this.units,"units"),this.activation=ol(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=Yr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=Yr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=Gi(t.kernelConstraint),this.biasConstraint=Gi(t.biasConstraint),this.kernelRegularizer=fl(t.kernelRegularizer),this.biasRegularizer=fl(t.biasRegularizer),this.activityRegularizer=fl(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=aa(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=aa(t)).slice();return e[e.length-1]=this.units,e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=ra(t),s=Mi(this.activation.getClassName());let i;return null!=s?i=xr(e,this.kernel.read(),s,this.bias?this.bias.read():null):(i=xr(e,this.kernel.read()),null!=this.bias&&(i=Ar(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:rl(this.activation),useBias:this.useBias,kernelInitializer:Zr(this.kernelInitializer),biasInitializer:Zr(this.biasInitializer),kernelRegularizer:pl(this.kernelRegularizer),biasRegularizer:pl(this.biasRegularizer),activityRegularizer:pl(this.activityRegularizer),kernelConstraint:Ki(this.kernelConstraint),biasConstraint:Ki(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}nu.className="Dense",e.serialization.registerClass(nu);class su extends ya{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=aa(t);for(const e of t.slice(1))if(null==e)throw new mi(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],hr(t,1)]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);let s=ra(t);if("channelsFirst"===this.dataFormat&&s.rank>1){const t=[0];for(let e=2;e<s.rank;++e)t.push(e);t.push(1),s=e.transpose(s,t)}return function(t){if(t.rank<=1)throw new mi(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const n=[t.shape[0],hr(t.shape,1)];return e.reshape(t,n)}(s)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}su.className="Flatten",e.serialization.registerClass(su);class iu extends ya{constructor(t){super(t),this.supportsMasking=!0,this.activation=ol(t.activation)}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=ra(t);return this.activation.apply(e)}))}getConfig(){const t={activation:rl(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}iu.className="Activation",e.serialization.registerClass(iu);class ru extends ya{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,n){return e.tidy((()=>{return t=ra(t),n=t,s=this.n,e.tidy((()=>{if(2!==n.shape.length)throw new mi(`repeat() expects a rank-2 tensor, but received a rank-${n.shape.length} tensor.`);return vr(gr(n,1),[1,s,1])}));var n,s}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}ru.className="RepeatVector",e.serialization.registerClass(ru);class au extends ya{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new mi("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=hr(t);if(null!==r){if(0===i||a%i!=0)throw new mi(n);s[r]=a/i}else if(a!==i)throw new mi(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=ra(t),i=s.shape,r=i.slice(0,1).concat(this.fixUnknownDimension(i.slice(1),this.targetShape));return e.reshape(s,r)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}au.className="Reshape",e.serialization.registerClass(au);class ou extends ya{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const n=dr(1,t.dims.length+1);if(!e.util.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new pa({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=aa(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,n){return e.transpose(ra(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}ou.className="Permute",e.serialization.registerClass(ou);class lu extends ya{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,n){const s=ra(t);return e.any(e.notEqual(s,this.maskValue),-1)}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=ra(t),i=e.any(e.notEqual(s,this.maskValue),-1,!0);return e.mul(s,e.cast(i,s.dtype))}))}}lu.className="Masking",e.serialization.registerClass(lu);class uu extends ya{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(xi(t.inputLength))}this.inputDim=t.inputDim,_i(this.inputDim,"inputDim"),this.outputDim=t.outputDim,_i(this.outputDim,"outputDim"),this.embeddingsInitializer=Yr(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=fl(t.embeddingsRegularizer),this.activityRegularizer=fl(t.activityRegularizer),this.embeddingsConstraint=Gi(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,n){return e.tidy((()=>this.maskZero?(t=ra(t),e.notEqual(t,e.zerosLike(t))):null))}computeOutputShape(t){if(t=aa(t),null==this.inputLength)return[...t,this.outputDim];const e=xi(this.inputLength);if(e.length!==t.length-1)throw new mi(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new mi(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);let s=ra(t);"int32"!==s.dtype&&(s=fr(s,"int32"));const i=Nr(this.embeddings.read(),e.reshape(s,[s.size]));return e.reshape(i,aa(this.computeOutputShape(s.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:Zr(this.embeddingsInitializer),embeddingsRegularizer:pl(this.embeddingsRegularizer),activityRegularizer:pl(this.activityRegularizer),embeddingsConstraint:Ki(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}uu.className="Embedding",e.serialization.registerClass(uu);class hu extends ya{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new yi}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new mi("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[aa(t)]),(t=t).length<2)throw new mi(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=Ei(e),e.length>1)throw new mi(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===Ei(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,n){return e.tidy((()=>{if(t=t,this.reshapeRequired){const n=[],s=t.map((t=>t.rank));if(-1===s.indexOf(null)){const e=pr(s);for(let s of t){const t=s.rank;for(let n=0;n<e-t;++n)s=gr(s,1);n.push(s)}return this.mergeFunction(n)}{let s=!1;for(const i of t){const t=i.rank;if(null==t){const t=i.shape,r=t[0],a=t.slice(1).concat([r]);let o=e.reshape(i,[r].concat(hr(t.slice(1))));o=e.transpose(o,[1,0]),o=e.reshape(o,a),n.push(o),s=!0}else if(t>1){const r=dr(1,t).concat([0]);n.push(e.transpose(i,r)),s=!0}else n.push(i)}let i=this.mergeFunction(n);const r=i.rank;if(s)if(null==r){const t=i.shape,n=t[t.length-1],s=[n].concat(t.slice(0,t.length-1));i=e.reshape(e.transpose(e.reshape(i,[-1,n]),[1,0]),s)}else if(r>1){const t=[r-1].concat(dr(0,r-1));i=e.transpose(i,t)}return i}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=Ei(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,n){return e.tidy((()=>{if(null==n)return null;if(!Array.isArray(n))throw new mi("`mask` should be an Array");if(!Array.isArray(t))throw new mi("`inputs` should be an Array");if(n.length!==t.length)throw new mi(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${n.length})`);if(n.every((t=>null==t)))return null;let s=(n=n.map((t=>null==t?t:e.expandDims(t,0))))[0];for(let t=1;t<n.length-1;++t)s=e.logicalAnd(s,n[t]);return s}))}}class cu extends hu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return n}))}}cu.className="Add",e.serialization.registerClass(cu);class pu extends hu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.mul(n,t[s]);return n}))}}pu.className="Multiply",e.serialization.registerClass(pu);class du extends hu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return e.mul(1/t.length,n)}))}}du.className="Average",e.serialization.registerClass(du);class fu extends hu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.maximum(n,t[s]);return n}))}}fu.className="Maximum",e.serialization.registerClass(fu);class gu extends hu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.minimum(n,t[s]);return n}))}}gu.className="Minimum",e.serialization.registerClass(gu);class mu extends hu{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new mi("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.util.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new mi("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return e.tidy((()=>wr(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new mi("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,n){if(null==n)return null;if(!Array.isArray(n))throw new mi("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new mi("`inputs` should be an array for Concatenate");if(n.length!==t.length)throw new mi(`Mismatch in the length of mask (${n.length}) and the legnth of inputs (${t.length})`);return e.tidy((()=>{let s=!0;if(n.forEach((t=>{null==t||(s=!1)})),s)return null;const i=[];for(let s=0;s<t.length;++s)null==n[s]?i.push(e.cast(e.onesLike(t[s]),"bool")):n[s].rank<t[s].rank?i.push(e.expandDims(n[s],-1)):i.push(n[s]);const r=e.concat(i,this.axis);return e.all(r,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function yu(t,e){for(;t<0;)t+=e;return t}mu.className="Concatenate",e.serialization.registerClass(mu);class bu extends hu{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new yi("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new mi(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new mi(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,s=t[0],i=t[1];return n=Array.isArray(this.axes)?this.axes.map(((e,n)=>yu(e,t[n].shape.length))):[yu(this.axes,s.shape.length),yu(this.axes,i.shape.length)],this.normalize&&(s=Da(s,n[0]),i=Da(i,n[1])),function(t,n,s){if(t.shape.length>3||n.shape.length>3)throw new yi("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.util.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),e.util.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`)),"number"==typeof s&&(s=[s,s]),"complex64"===t.dtype||"complex64"===n.dtype)throw new yi("batchDot is not implemented for complex64-type Tensors yet.");const i=t.shape.length,r=n.shape.length;null==s&&(s=[i-1,r-2]);const a=s;return e.tidy((()=>{let s,o;if(i>r){s=i-r;const t=[];for(let e=0;e<s;++e)t.push(1);n=e.reshape(n,n.shape.concat(t))}else if(r>i){s=r-i;const n=[];for(let t=0;t<s;++t)n.push(1);t=e.reshape(t,t.shape.concat(n))}else s=0;if(2===t.shape.length&&2===n.shape.length)o=a[0]===a[1]?e.sum(e.mul(t,n),a[0]):e.sum(e.mul(e.transpose(t,[1,0]),n),a[1]);else{const s=a[0]!==t.shape.length-1,i=a[1]===n.shape.length-1;o=e.matMul(t,n,s,i)}if(s>0){let t;t=i>r?i+r-3:i-1;const n=[];for(let e=t;e<t+s;++e)n.push(e);o=e.squeeze(o,n)}return 1===o.shape.length&&(o=e.expandDims(o,1)),o}))}(s,i,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[yu(this.axes,t.length),yu(this.axes,e.length)],n}computeOutputShape(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new yi("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}bu.className="Dot",e.serialization.registerClass(bu);class wu extends ya{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=ra(t);return Tr((()=>e.add(Sr(s.shape,0,this.stddev),s)),(()=>s),n.training||!1)}))}}wu.className="GaussianNoise",e.serialization.registerClass(wu);class ku extends ya{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=ra(t);if(this.rate>0&&this.rate<1){return Tr((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return e.mul(s,Sr(s.shape,1,t))}),(()=>s),n.training||!1)}return s}))}}ku.className="GaussianDropout",e.serialization.registerClass(ku);class vu extends ya{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||ra(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{if(this.rate<1&&this.rate>0){const s=this._getNoiseShape(t);return Tr((()=>{const n=ra(t),i=-1.7580993408473766;let r=e.greaterEqual(e.randomUniform(s),this.rate);r=fr(r,"float32");const a=((1-this.rate)*(1+this.rate*i**2))**-.5,o=-a*i*this.rate,l=e.add(e.mul(n,r),e.mul(e.add(r,-1),i));return e.add(e.mul(l,a),o)}),(()=>ra(t)),n.training||!1)}return t}))}}function Su(t,n,s,i,r,a=.001){let o;if(2===t.rank)o=e.batchNorm2d(t,n,s,i,r,a);else if(3===t.rank)o=e.batchNorm3d(t,n,s,i,r,a);else{if(4!==t.rank)throw new yi(`batchNormalization is not implemented for array of rank ${t.rank} yet`);o=e.batchNorm4d(t,n,s,i,r,a)}return o}function xu(t,n,s,i,r=.001){return e.util.arraysEqual(i.slice().sort(),dr(0,t.rank-1))?function(t,n,s,i,r=.001){return e.tidy((()=>{const a=e.moments(t,i),o=a.mean,l=a.variance;return[Su(t,o,l,s,n,r),o,l]}))}(t,n,s,i,r):function(t,n,s,i,r=.001){return e.tidy((()=>{const a=e.moments(t,i),o=a.mean,l=a.variance,u=[];for(const e of dr(0,t.rank))-1!==i.indexOf(e)?u.push(1):u.push(t.shape[e]);const h=e.reshape(o,u),c=e.reshape(l,u),p=null==n?null:e.reshape(n,u),d=null==s?null:e.reshape(s,u);return[Su(t,h,c,d,p,r),o,l]}))}(t,n,s,i,r)}vu.className="AlphaDropout",e.serialization.registerClass(vu);class Nu extends ya{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Yr(t.betaInitializer||"zeros"),this.gammaInitializer=Yr(t.gammaInitializer||"ones"),this.movingMeanInitializer=Yr(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=Yr(t.movingVarianceInitializer||"ones"),this.betaConstraint=Gi(t.betaConstraint),this.gammaConstraint=Gi(t.gammaConstraint),this.betaRegularizer=fl(t.betaRegularizer),this.gammaRegularizer=fl(t.gammaRegularizer)}build(t){t=aa(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new mi(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new pa({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return e.tidy((()=>{const s=null!=n.training&&n.training,i=ra(t),r=i.shape,a=r.length,o=dr(0,a),l=this.axis>=0?this.axis:this.axis+a;o.splice(l,1);const u=wi(1,a);u[l]=r[l];const h=o.slice();h.sort();const c=!e.util.arraysEqual(h,dr(0,a).slice(0,a-1));if(!s)return(()=>{if(c){const t=e.reshape(this.movingMean.read(),u),n=e.reshape(this.movingVariance.read(),u),s=this.center?e.reshape(this.beta.read(),u):null,r=this.scale?e.reshape(this.gamma.read(),u):null;return Su(i,t,n,s,r,this.epsilon)}return Su(i,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,d,f]=xu(i,this.gamma.read(),this.beta.read(),o,this.epsilon),g=(t,n,s)=>{e.tidy((()=>{const i=1-s,r=t.read(),a=e.mul(e.sub(r,n),i);t.write(e.sub(r,a))}))};return(()=>{g(this.movingMean,d,this.momentum),g(this.movingVariance,f,this.momentum)})(),p}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Zr(this.betaInitializer),gammaInitializer:Zr(this.gammaInitializer),movingMeanInitializer:Zr(this.movingMeanInitializer),movingVarianceInitializer:Zr(this.movingVarianceInitializer),betaRegularizer:pl(this.betaRegularizer),gammaRegularizer:pl(this.gammaRegularizer),betaConstraint:Ki(this.betaConstraint),gammaConstraint:Ki(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Nu.className="BatchNormalization",e.serialization.registerClass(Nu);class zu extends ya{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Yr(t.betaInitializer||"zeros"),this.gammaInitializer=Yr(t.gammaInitializer||"ones"),this.betaRegularizer=fl(t.betaRegularizer),this.gammaRegularizer=fl(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=aa(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==Ei(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(t,n){const s=ra(t),i=s.shape,r=i.length;return e.tidy((()=>{let{mean:t,variance:n}=e.moments(s,this.axis,!0);const a=wi(1,r);for(const t of this.axis)a[t]=i[t];const o=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?e.reshape(t,a):t;let l=o(this.gamma.read()),u=o(this.beta.read());const h=[],c=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(h.push(i[t]),c.push(1)):(h.push(1),c.push(i[t]));return t=e.tile(t,h),n=e.tile(n,h),l=e.tile(l,c),u=e.tile(u,c),Su(s,t,n,u,l,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Zr(this.betaInitializer),gammaInitializer:Zr(this.gammaInitializer),betaRegularizer:pl(this.betaRegularizer),gammaRegularizer:pl(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}zu.className="LayerNormalization",e.serialization.registerClass(zu);class Iu extends ya{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new mi(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new mi(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new mi(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new pa({ndim:4})]}computeOutputShape(t){let e,n;return t=aa(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,n){return e.tidy((()=>{return n=ra(t),s=this.padding,i=this.dataFormat,e.tidy((()=>{if(4!==n.rank)throw new mi(`temporalPadding expects input tensor to be 4-D, but received a ${n.rank}-D tensor.`);if(null==s&&(s=[[1,1],[1,1]]),2!==s.length||2!==s[0].length||2!==s[1].length)throw new mi("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new mi(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],s[0],s[1]]:[[0,0],s[0],s[1],[0,0]],e.pad(n,t)}));var n,s,i}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function Au(t,n,s,i,r,a){return e.tidy((()=>{let o;er(r),sr(a),nr(i),null==s&&(s=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=Nl(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool(t,n,s,l):e.avgPool(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o}))}function Cu(t,n,s,i,r,a){return e.tidy((()=>{let o;er(r),sr(a),nr(i),null==s&&(s=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=zl(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool3d(t,n,s,l):e.avgPool3d(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,4,1,2,3])),o}))}Iu.className="ZeroPadding2D",e.serialization.registerClass(Iu);class Tu extends ya{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new mi(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if(_i(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new mi(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}_i(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,nr(this.padding),this.inputSpec=[new pa({ndim:3})]}computeOutputShape(t){const e=Sl((t=aa(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n),t=gr(ra(t),2);const s=this.poolingFunction(ra(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return e.squeeze(s,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class $u extends Tu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return er(i),nr(s),Au(t,e,n,s,i,"max")}}$u.className="MaxPooling1D",e.serialization.registerClass($u);class Eu extends Tu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return er(i),nr(s),Au(t,e,n,s,i,"avg")}}Eu.className="AveragePooling1D",e.serialization.registerClass(Eu);class Fu extends ya{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new mi(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];_i(this.poolSize,"poolSize"),_i(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,er(this.dataFormat),nr(this.padding),this.inputSpec=[new pa({ndim:4})]}computeOutputShape(t){t=aa(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=Sl(e,this.poolSize[0],this.padding,this.strides[0]),n=Sl(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,n){return e.tidy((()=>(this.invokeCallHook(t,n),this.poolingFunction(ra(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Du extends Fu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return er(i),nr(s),Au(t,e,n,s,i,"max")}}Du.className="MaxPooling2D",e.serialization.registerClass(Du);class Lu extends Fu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return er(i),nr(s),Au(t,e,n,s,i,"avg")}}Lu.className="AveragePooling2D",e.serialization.registerClass(Lu);class _u extends ya{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new mi(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];_i(this.poolSize,"poolSize"),_i(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,er(this.dataFormat),nr(this.padding),this.inputSpec=[new pa({ndim:5})]}computeOutputShape(t){t=aa(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=Sl(e,this.poolSize[0],this.padding,this.strides[0]),n=Sl(n,this.poolSize[1],this.padding,this.strides[1]),s=Sl(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,n){return e.tidy((()=>(this.invokeCallHook(t,n),this.poolingFunction(ra(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Ru extends _u{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return er(i),nr(s),Cu(t,e,n,s,i,"max")}}Ru.className="MaxPooling3D",e.serialization.registerClass(Ru);class Mu extends _u{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return er(i),nr(s),Cu(t,e,n,s,i,"avg")}}Mu.className="AveragePooling3D",e.serialization.registerClass(Mu);class Ou extends ya{constructor(t){super(t),this.inputSpec=[new pa({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new yi}}class Bu extends Ou{constructor(t){super(t||{})}call(t,n){return e.tidy((()=>{const n=ra(t);return e.mean(n,1)}))}}Bu.className="GlobalAveragePooling1D",e.serialization.registerClass(Bu);class Pu extends Ou{constructor(t){super(t||{})}call(t,n){return e.tidy((()=>{const n=ra(t);return e.max(n,1)}))}}Pu.className="GlobalMaxPooling1D",e.serialization.registerClass(Pu);class Wu extends ya{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,er(this.dataFormat),this.inputSpec=[new pa({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new yi}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Uu extends Wu{call(t,n){return e.tidy((()=>{const n=ra(t);return"channelsLast"===this.dataFormat?e.mean(n,[1,2]):e.mean(n,[2,3])}))}}Uu.className="GlobalAveragePooling2D",e.serialization.registerClass(Uu);class ju extends Wu{call(t,n){return e.tidy((()=>{const n=ra(t);return"channelsLast"===this.dataFormat?e.max(n,[1,2]):e.max(n,[2,3])}))}}ju.className="GlobalMaxPooling2D",e.serialization.registerClass(ju);class Vu extends ya{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=Fa(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class Ku extends Vu{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=aa(t)).length<3)throw new mi(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=aa(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,n){return e.tidy((()=>Pl(((t,e)=>[ra(this.layer.call(t,n)),[]]),t=ra(t),[],!1,null,null,!1,!0)[1]))}}Ku.className="TimeDistributed",e.serialization.registerClass(Ku);class qu extends Vu{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=Fa(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=Fa(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,Di(Qi,"BidirectionalMergeMode",i),t.weights)throw new yi("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):Si(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Bl(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new mi("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map((t=>new pa({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new yi("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof da;for(const t of r)if(t instanceof da!==o)throw new mi("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy((()=>{const s=n.initialState;let i,r,a,o;if(null==s)i=this.forwardLayer.call(t,n),r=this.backwardLayer.call(t,n);else{const e=s.slice(0,s.length/2),a=s.slice(s.length/2);i=this.forwardLayer.call(t,Object.assign(n,{initialState:e})),r=this.backwardLayer.call(t,Object.assign(n,{initialState:a}))}return this.returnState&&(Array.isArray(i)&&(a=i.slice(1).concat(r.slice(1))),i=i[0],r=r[0]),this.returnSequences&&(r=e.reverse(r,1)),"concat"===this.mergeMode?o=wr([i,r]):"sum"===this.mergeMode?o=e.add(i,r):"ave"===this.mergeMode?o=e.mul(.5,e.add(i,r)):"mul"===this.mergeMode?o=e.mul(i,r):null==this.mergeMode&&(o=[i,r]),this.returnState?null==this.mergeMode?o.concat(a):[o].concat(a):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){rr(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),rr(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=Fa(e.layer);if(delete e.layer,null!=e.numConstants)throw new yi("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function Gu(t){return new Eu(t)}function Hu(t){return new Lu(t)}function Ju(t){return new Mu(t)}function Zu(t){return new Pu(t)}function Yu(t){return new ju(t)}function Xu(t){return new $u(t)}function Qu(t){return new Du(t)}qu.className="Bidirectional",e.serialization.registerClass(qu);const th=Zu,eh=Yu,nh=Xu,sh=Qu;var ih=Object.freeze({__proto__:null,inputLayer:function(t){return new wa(t)},elu:function(t){return new bl(t)},reLU:function(t){return new gl(t)},leakyReLU:function(t){return new ml(t)},prelu:function(t){return new yl(t)},softmax:function(t){return new kl(t)},thresholdedReLU:function(t){return new wl(t)},conv1d:function(t){return new _l(t)},conv2d:function(t){return new Tl(t)},conv2dTranspose:function(t){return new El(t)},conv3d:function(t){return new $l(t)},conv3dTranspose:function(t){return new Fl(t)},separableConv2d:function(t){return new Ll(t)},cropping2D:function(t){return new Rl(t)},upSampling2d:function(t){return new Ml(t)},depthwiseConv2d:function(t){return new Ol(t)},activation:function(t){return new iu(t)},dense:function(t){return new nu(t)},dropout:function(t){return new tu(t)},spatialDropout1d:function(t){return new eu(t)},flatten:function(t){return new su(t)},repeatVector:function(t){return new ru(t)},reshape:function(t){return new au(t)},permute:function(t){return new ou(t)},embedding:function(t){return new uu(t)},add:function(t){return new cu(t)},average:function(t){return new du(t)},concatenate:function(t){return new mu(t)},maximum:function(t){return new fu(t)},minimum:function(t){return new gu(t)},multiply:function(t){return new pu(t)},dot:function(t){return new bu(t)},batchNormalization:function(t){return new Nu(t)},layerNormalization:function(t){return new zu(t)},zeroPadding2d:function(t){return new Iu(t)},averagePooling1d:Gu,avgPool1d:function(t){return Gu(t)},avgPooling1d:function(t){return Gu(t)},averagePooling2d:Hu,avgPool2d:function(t){return Hu(t)},avgPooling2d:function(t){return Hu(t)},averagePooling3d:Ju,avgPool3d:function(t){return Ju(t)},avgPooling3d:function(t){return Ju(t)},globalAveragePooling1d:function(t){return new Bu(t)},globalAveragePooling2d:function(t){return new Uu(t)},globalMaxPooling1d:Zu,globalMaxPooling2d:Yu,maxPooling1d:Xu,maxPooling2d:Qu,maxPooling3d:function(t){return new Ru(t)},gru:function(t){return new ql(t)},gruCell:function(t){return new Kl(t)},lstm:function(t){return new Hl(t)},lstmCell:function(t){return new Gl(t)},simpleRNN:function(t){return new Vl(t)},simpleRNNCell:function(t){return new jl(t)},convLstm2d:function(t){return new Ql(t)},convLstm2dCell:function(t){return new Xl(t)},rnn:function(t){return new Wl(t)},stackedRNNCells:function(t){return new Jl(t)},bidirectional:function(t){return new qu(t)},timeDistributed:function(t){return new Ku(t)},globalMaxPool1d:th,globalMaxPool2d:eh,maxPool1d:nh,maxPool2d:sh,Layer:ya,RNN:Wl,RNNCell:Ul,input:jo,gaussianNoise:function(t){return new wu(t)},gaussianDropout:function(t){return new ku(t)},alphaDropout:function(t){return new vu(t)},masking:function(t){return new lu(t)}});var rh=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return ja(t,e)},binaryCrossentropy:function(t,e){return Ha(t,e)},sparseCategoricalAccuracy:function(t,e){return Ja(t,e)},categoricalAccuracy:function(t,e){return Va(t,e)},categoricalCrossentropy:function(t,e){return Za(t,e)},precision:function(t,e){return qa(t,e)},recall:function(t,e){return Ga(t,e)},cosineProximity:function(t,e){return Pa(t,e)},meanAbsoluteError:function(t,e){return _a(t,e)},meanAbsolutePercentageError:function(t,e){return Ra(t,e)},MAPE:function(t,e){return Ra(t,e)},mape:function(t,e){return Ra(t,e)},meanSquaredError:function(t,e){return La(t,e)},MSE:function(t,e){return La(t,e)},mse:function(t,e){return La(t,e)}}),ah=Object.freeze({__proto__:null,modelFromJSON:async function(t,n){"modelTopology"in t||(t={modelTopology:t});let s=(t=t).modelTopology;null!=s.model_config&&(s=s.model_config);const i=Fa(uo(s),n);if(null!=t.weightsManifest){const n=await e.io.loadWeights(t.weightsManifest,t.pathPrefix,i.weights.map((t=>t.originalName))),s={};for(const t of i.weights)s[t.originalName]=n[t.originalName];i.loadWeights(s),e.dispose(n)}return i}});var oh=Object.freeze({__proto__:null,l1l2:function(t){return new hl(t)},l1:function(t){return ll(e=t),new hl({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return ll(e=t),new hl({l2:null!=e?e.l2:null,l1:0});var e}});class lh extends Na{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof Bo))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function uh(t,e){return t<e}function hh(t,e){return t>e}class ch extends lh{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new yi("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=uh:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=hh:this.monitorFunc=uh,this.monitorFunc===uh&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===uh?1/0:-1/0}async onEpochEnd(t,e){await va(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(t)}`),e}}const ph={earlyStopping:function(t){return new ch(t)}};t.Callback=lh,t.CallbackList=za,t.CustomCallback=Ca,t.EarlyStopping=ch,t.History=Aa,t.InputSpec=pa,t.LayerVariable=ua,t.LayersModel=Bo,t.RNN=Wl,t.Sequential=Uo,t.SymbolicTensor=da,t.callbacks=ph,t.constraints=Hi,t.initializers=Xr,t.input=jo,t.layers=ih,t.loadLayersModel=function(t,e){return null==e&&(e={}),Wo(t,e)},t.metrics=rh,t.model=function(t){return new Bo(t)},t.models=ah,t.registerCallbackConstructor=function(t,e){$a.registerCallbackConstructor(t,e)},t.regularizers=oh,t.sequential=function(t){return new Uo(t)},t.version_layers=co,Object.defineProperty(t,"__esModule",{value:!0})}));
//# sourceMappingURL=tf-layers.es2017.min.js.map
